[
  { "_comment": "gnmt_fp32_latency",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=language_translation --model-name=mlperf_gnmt --precision=fp32 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --num-cores=-1 --batch-size=1 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --benchmark-only --data-location=/dataset --in-graph=workspace/mlperf_gnmt_fp32_pretrained_model.pb",
    "output": "numactl --cpunodebind=0 --membind=0 python /workspace/intelai_models/fp32/run_inference.py --in_graph=workspace/mlperf_gnmt_fp32_pretrained_model.pb --batch_size=1 --num_inter_threads=1 --num_intra_threads=28 --src_vocab_file=/dataset/vocab.bpe.32000.en --tgt_vocab_file=/dataset/vocab.bpe.32000.de --inference_input_file=/dataset/newstest2014.tok.bpe.32000.en --inference_ref_file=/dataset/newstest2014.tok.bpe.32000.de",
    "cpuset": "0-111"},

  { "_comment": "gnmt_fp32_throughput",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=language_translation --model-name=mlperf_gnmt --precision=fp32 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --num-cores=-1 --batch-size=32 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --benchmark-only --data-location=/dataset --in-graph=workspace/mlperf_gnmt_fp32_pretrained_model.pb",
    "output": "numactl --cpunodebind=0 --membind=0 python /workspace/intelai_models/fp32/run_inference.py --in_graph=workspace/mlperf_gnmt_fp32_pretrained_model.pb --batch_size=32 --num_inter_threads=1 --num_intra_threads=28 --src_vocab_file=/dataset/vocab.bpe.32000.en --tgt_vocab_file=/dataset/vocab.bpe.32000.de --inference_input_file=/dataset/newstest2014.tok.bpe.32000.en --inference_ref_file=/dataset/newstest2014.tok.bpe.32000.de",
    "cpuset": "0-111"},

  { "_comment": "gnmt_fp32_accuracy",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=language_translation --model-name=mlperf_gnmt --precision=fp32 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --num-cores=-1 --batch-size=32 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --accuracy-only --data-location=/dataset --in-graph=workspace/mlperf_gnmt_fp32_pretrained_model.pb",
    "output": "numactl --cpunodebind=0 --membind=0 python /workspace/intelai_models/fp32/run_inference.py --in_graph=workspace/mlperf_gnmt_fp32_pretrained_model.pb --batch_size=32 --num_inter_threads=1 --num_intra_threads=28 --src_vocab_file=/dataset/vocab.bpe.32000.en --tgt_vocab_file=/dataset/vocab.bpe.32000.de --inference_input_file=/dataset/newstest2014.tok.bpe.32000.en --inference_ref_file=/dataset/newstest2014.tok.bpe.32000.de",
    "cpuset": "0-111"}
]
