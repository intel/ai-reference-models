releases:
  versioned:
    tag_specs:
    - '{tensorflow-itex-max-series-base-public}{tf-max-series-bert-large-inference}'
slice_sets:
  tf-max-series-bert-large-inference:
  - add_to_name: -tf-max-series-bert-large-inference
    dockerfile_exclusive_name: -bert-large-inference
    args:
    - PACKAGE_NAME=tf-max-series-bert-large-inference
    dockerfile_subdirectory: gpu_model_containers
    downloads:
    - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/v2_7_0/fp32_bert_squad.pb
      destination: frozen_graph/fp32_bert_squad.pb
    files:
    - destination: benchmarks/common
      source: benchmarks/common
    - destination: benchmarks/launch_benchmark.py
      source: benchmarks/launch_benchmark.py
    - destination: benchmarks/__init__.py
      source: benchmarks/__init__.py
    - destination: models/common
      source: models/common
    - destination: benchmarks/language_modeling/tensorflow/bert_large/inference
      source: benchmarks/language_modeling/tensorflow/bert_large/inference
    - destination: benchmarks/language_modeling/tensorflow/bert_large/__init__.py
      source: benchmarks/language_modeling/tensorflow/bert_large/__init__.py
    - destination: benchmarks/language_modeling/tensorflow/__init__.py
      source: benchmarks/language_modeling/tensorflow/__init__.py
    - destination: benchmarks/language_modeling/__init__.py
      source: benchmarks/language_modeling/__init__.py
    - destination: models/language_modeling/tensorflow/bert_large/inference
      source: models/language_modeling/tensorflow/bert_large/inference
    - destination: quickstart/common
      source: quickstart/common
    - destination: quickstart
      source: quickstart/language_modeling/tensorflow/bert_large/inference/gpu
    - source: quickstart/language_modeling/tensorflow/bert_large/inference/gpu/README.md
      destination: README.md
    wrapper_package_files:
    - source: quickstart/language_modeling/tensorflow/bert_large/inference/gpu/benchmark.sh
      destination: benchmark.sh
    - source: quickstart/language_modeling/tensorflow/bert_large/inference/gpu/build.sh
      destination: build.sh
    - source: dockerfiles/gpu_model_containers/tf-max-series-bert-large-inference.Dockerfile
      destination: tf-max-series-bert-large-inference.Dockerfile
    - source: output/tf-max-series-bert-large-inference.tar.gz
      destination: model_packages/tf-max-series-bert-large-inference.tar.gz
    - source: ""
      destination: info.txt
    - source: third_party
      destination: licenses/third_party
    - source: LICENSE
      destination: licenses/LICENSE
    partials:
    - model_package
    - entrypoint
