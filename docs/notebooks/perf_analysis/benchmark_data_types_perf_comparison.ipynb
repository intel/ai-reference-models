{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Compare Model Zoo Benchmark performance among different data types on Intel optimized  Tensorflow\n",
    "\n",
    "This jupyter notebook will help you evaluate performance benefits among different data types like int8/bf16 on Intel-optimized Tensorflow via several models from Intel Model Zoo. \n",
    "The notebook will show users some bar charts like below for performance comparison among different data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\perf_comparison_types.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Display Platform Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTION: Users should change the value of os.environ['ModelZooRoot'] according to their environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# If default path does not work, change ModelZooRoot path according to your environment\n",
    "## USER INPUT\n",
    "accuracy_only=False\n",
    "current_path = os.getcwd()\n",
    "os.environ['ModelZooRoot'] = current_path + \"/../../../\"\n",
    "os.environ['ProfileUtilsRoot'] = os.environ['ModelZooRoot'] + \"docs/notebooks/perf_analysis/profiling/\"\n",
    "print(os.environ['ModelZooRoot'])\n",
    "print(os.environ['ProfileUtilsRoot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Check for mandatory python scripts after ModelZooRoot and ProfileUtilsRoot are assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()\n",
    "benchmark_path = os.environ['ModelZooRoot'] + \"benchmarks/launch_benchmark.py\"\n",
    "if os.path.exists(benchmark_path) == True:\n",
    "    print(benchmark_path)\n",
    "else:\n",
    "    print(\"ERROR! Can't find benchmark script!\")\n",
    "    \n",
    "profile_utils_path = os.environ['ProfileUtilsRoot'] + \"profile_utils.py\"\n",
    "if os.path.exists(profile_utils_path) == True:\n",
    "    print(profile_utils_path)\n",
    "else:\n",
    "    print(\"ERROR! Can't find profile_utils script!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import PlatformUtils\n",
    "plat_utils = PlatformUtils()\n",
    "plat_utils.dump_platform_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Run the benchmark on the Intel TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check TensorFlow version and MKL enablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (\"We are using Tensorflow version\", tf.__version__)\n",
    "major_version = int(tf.__version__.split(\".\")[0])\n",
    "if major_version >= 2:\n",
    "   from tensorflow.python import _pywrap_util_port\n",
    "   print(\"MKL enabled:\", _pywrap_util_port.IsMklEnabled())\n",
    "else:\n",
    "   print(\"MKL enabled:\", tf.pywrap_tensorflow.IsMklEnabled())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_2'></a>\n",
    "## Step 2 : Select a supported Topology/Benchmark with different data types support\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_2_1'></a>\n",
    "### Step 2.1 : List out the Topologies/Benchmarks with multiple data types support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from profiling.profile_utils import ConfigFile\n",
    "\n",
    "accuracy_only=False\n",
    "\n",
    "config = ConfigFile()\n",
    "df, df_types, df_types_obj = config.convert_configs_to_pd_dataframe()\n",
    "\n",
    "name_list = []\n",
    "\n",
    "for name,group in df_types_obj:\n",
    "    name_list.append(name)\n",
    "print(\" Benchmark that supports multiple data types :\")\n",
    "index=0\n",
    "for name in name_list:\n",
    "    print(' Index %d : %s' %(index, name))\n",
    "    index +=1\n",
    "# Initial CSV files list\n",
    "csv_fname_list = []\n",
    "precision_list = []\n",
    "selected_precision_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_2_2'></a>\n",
    "### Step 2.2 : Pick a Topology/Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User picks a benchmark type\n",
    "## USER INPUT\n",
    "selected_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_index > index:\n",
    "    print(\"Please select a valid index number.\")\n",
    "else:\n",
    "    selected_df = df_types_obj.get_group(name_list[selected_index])\n",
    "    #print(name_list[selected_index])\n",
    "    selected_topology = name_list[selected_index][0] +'-'+name_list[selected_index][1]\n",
    "    print(selected_topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_3'></a>\n",
    "## Step 3 : Select a supported data types for the Topology/Benchmark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_3_1'></a>\n",
    "### Step 3.1 : List out all data types supported by the selected Topology/Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = df_types_obj.get_group(name_list[selected_index])\n",
    "print(name_list[selected_index])\n",
    "print(\" supported data types :\")\n",
    "precision_df = selected_df[\"precision\"]\n",
    "\n",
    "for i in range(len(precision_df)):\n",
    "    print(' Index %d : %s' %(i, selected_df[\"precision\"].iloc[i]))\n",
    "    precision_list.append(selected_df[\"precision\"].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_3_2'></a>\n",
    "### Step 3.2 : Pick a Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTION : Please select one supported data type and change data_type_index accordingly\n",
    "By default, users can start with 0 as data_type_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User picks a data type for selected benchmark type\n",
    "## USER INPUT\n",
    "data_type_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_3_3'></a>\n",
    "### Step 3.3 : List out the selected topology/benchmark name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type_index > len(precision_df):\n",
    "    print(\"Please select a valid index number.\")\n",
    "else:\n",
    "    topology_name = selected_df.iloc[data_type_index]['benchmark']\n",
    "    print(\"selected topology/benchmark for this run : \", topology_name)\n",
    "\n",
    "selected_precision_list.append(precision_list[data_type_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the selected topology and data type as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SELECTED_TOPO'] = selected_topology\n",
    "os.environ['SELECTED_TYPE'] = selected_df[\"precision\"].iloc[data_type_index]\n",
    "print(os.environ['SELECTED_TOPO'])\n",
    "print(os.environ['SELECTED_TYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_4'></a>\n",
    "## Step 4: Configure parameters for launch_benchmark.py according to the selected Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Import Model Zoo CPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.environ['ModelZooRoot']+os.sep+'benchmarks/common/')\n",
    "from platform_util import CPUInfo \n",
    "cpu_info = CPUInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: User can also manually set batch size and number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "cpu_count = cpu_info.cores_per_socket\n",
    "cpu_socket_count =  cpu_info.sockets\n",
    "print(\"CPU count per socket:\" ,  cpu_count ,\" \\nSocket count:\",cpu_socket_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTION: Users can change the value of thread_number and batch_size to see different performance\n",
    "1. thread_umber: the value will apply to num_cores parameters in launch_benchmark.py  \n",
    "2. utilized_socket_number:  the value will apply to the socket-id parameter in launch_benchmark.py \n",
    "3. num_inter_threads: the value will  apply to the num-inter-threads parameter in launch_benchmark.py \n",
    "4. num_intra_threads: the value will  apply to the num-intra-threads parameter in launch_benchmark.py \n",
    "5. batch_size: the value will apply to the batch_size parameter in launch_benchmark.py \n",
    "6. log_folder: the folder where the logs are stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USER INPUT\n",
    "thread_number=cpu_count \n",
    "utilized_socket_number=1 #cpu_socket_count\n",
    "num_inter_threads = utilized_socket_number\n",
    "num_intra_threads = thread_number\n",
    "batch_size=32\n",
    "log_folder=os.getcwd() + os.sep + \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: get required data and files if needed.\n",
    "No action if there is no output from this below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters from config\n",
    "config = ConfigFile()\n",
    "config.read_config(topology_name)\n",
    "if config.data_download != '' and config.data_location == '':\n",
    "    print(\"\\nPlease follow below command to get the data : \")\n",
    "    val = config.read_value_from_section(topology_name, 'data-download')\n",
    "    print(val)\n",
    "if config.preprocessing != '':\n",
    "    print(\"\\nPlease follow below command to get required files and installation : \")\n",
    "    val = config.read_value_from_section(topology_name, 'preprocessing')\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">NOTE: If no action required from above cell, please skip below cell and go to [Step 4.4](#step_4_4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (OPTIONAL) ACTION: Users set the configurations in topo.ini for those required data and files\n",
    "1. data_download_path: the value will be set as the data-location parameter in topo.ini for the related topology\n",
    "2. model_source_dir: the value will be set as the model-source-dir parameter in topo.ini for the related topology such as Wide and Deep. Those models use tensorflow-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USER INPUT\n",
    "data_download_path = ''\n",
    "model_source_dir = ''\n",
    "\n",
    "# Overwrite configurations in topo.ini\n",
    "config = ConfigFile()\n",
    "config.read_config(topology_name)\n",
    "\n",
    "# data-location\n",
    "if config.data_download != '':\n",
    "    config.write_value_from_section(topology_name, 'data-location', data_download_path)\n",
    "    config.data_location = data_download_path\n",
    "\n",
    "# need data to get accuracy number\n",
    "if accuracy_only == True and config.data_location == '':\n",
    "    print(\"ERROR! STOP! need data for accuacy evaluatoin!\")  \n",
    "   \n",
    "# model-source-dir    \n",
    "if config.preprocessing != '':\n",
    "    config.write_value_from_section(topology_name, 'model-source-dir', model_source_dir)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_4_4'></a>\n",
    "### Step 4.4: Prepare the pre-trained model and model parameters for running the benchmark\n",
    "1. Get related parameters according to the selected topology\n",
    "2. Get the pretrained model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = ConfigFile()\n",
    "configvals = []\n",
    "# Get common parameters according to users' inputs  \n",
    "params = config.get_parameters(topology_name, configvals,\n",
    "                   batch_size=batch_size, thread_number=thread_number, socket_number=utilized_socket_number,\n",
    "                   num_inter_threads=num_inter_threads, num_intra_threads=num_intra_threads, accuracy_only=accuracy_only)\n",
    "\n",
    "# Get the parameters from config\n",
    "configvals=config.read_config(topology_name)\n",
    "\n",
    "# Get the pre-trained model file\n",
    "if config.wget != '' and ( config.in_graph == '' or config.checkpoint == ''  ):\n",
    "    pretrain_model_path = config.download_pretrained_model(current_path=current_path)\n",
    "    pretrain_model_path = config.uncompress_file(pretrain_model_path, current_path=current_path)\n",
    "    if config.in_graph == 'NA':\n",
    "        config.checkpoint = pretrain_model_path        \n",
    "    if config.checkpoint == 'NA':\n",
    "        config.in_graph = pretrain_model_path \n",
    "# set pre-trained model path        \n",
    "if config.checkpoint == 'NA':\n",
    "    configvals.append(\"--in-graph\")\n",
    "    configvals.append(config.in_graph)\n",
    "if config.in_graph == 'NA':\n",
    "    configvals.append(\"--checkpoint\")\n",
    "    configvals.append(config.checkpoint)\n",
    "    \n",
    "#Set output-dir folder\n",
    "if log_folder !='':\n",
    "    configvals.append(\"--output-dir\")\n",
    "    configvals.append(log_folder)\n",
    "\n",
    "# Combine common parameters and config parameters\n",
    "params = params + configvals    \n",
    "    \n",
    "sys.argv=[benchmark_path]+params\n",
    "print(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.5: Create a CSV file to log the performance numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import PerfPresenter\n",
    "job_type = topology_name.split(' ')[1]\n",
    "csv_fname=job_type+'_'+topology_name.replace(' ', '')+'.csv'\n",
    "print(csv_fname)\n",
    "perfp=PerfPresenter()\n",
    "perfp.create_csv_logfile(job_type, csv_fname)\n",
    "\n",
    "found = False\n",
    "for csv in csv_fname_list:\n",
    "    if csv == csv_fname:\n",
    "        found = True\n",
    "        break\n",
    "if found == False:\n",
    "    csv_fname_list.append(csv_fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.6: Retrieve MKLDNN Runtime Information\n",
    "> NOTE : performance may be impacted if users enable those MKLDNN debug features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can enable MKLDNN VERBOSE Mode to have more information from MKLDNN library.  \n",
    "Users can export the MKLDNN_VERBOSE environment variable to turn verbose mode on and control the level of verbosity.\n",
    "\n",
    "|Environment variable|Value|Description|\n",
    "|:-----|:----|:-----|\n",
    "|MKLDNN_VERBOSE| 0 |no verbose output (default)|\n",
    "||1|primitive information at execution|\n",
    "||2|primitive information at creation and execution|  \n",
    "\n",
    "Refer to the [link](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) for detailed verbose mode information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MKLDNN_VERBOSE'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can use the DNNL_JIT_DUMP environment variable to inspect MKLDNN JIT code,  \n",
    "and then check instructions usage by dissassembling the JIT kernel.\n",
    "\n",
    "|Environment variable|Value|Description|\n",
    "|:-----|:----|:-----|\n",
    "|DNNL_JIT_DUMP | 0 |JIT dump is disabled (default)|\n",
    "||any other value|JIT dump is enabled|\n",
    "\n",
    "Refer to the [link](https://oneapi-src.github.io/oneDNN/dev_guide_inspecting_jit.html) for detailed JIT Dump information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: recomend only enable JIT DUMP for inference. For training, number of JIT DUMP files would be huge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MKLDNN_JIT_DUMP'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_5'></a>\n",
    "## Step 5:  Run the benchmark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Below section will enable Tensorflow timeline for the model by patching it, and then unpatch it after the model completes its training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set enable_tf_timeline to False if users don't want to get TF timeline information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_tf_timeline = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the benchmark\n",
    "> NOTE: Users don't need to finish training if whole training takes a long time.  \n",
    "Users can stop below cell in the middle of the training, and still get the related performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_tf_timeline == True:\n",
    "    # patch related model script\n",
    "    repo_path = os.environ['ModelZooRoot'] #current_path + os.sep + \"../../\"\n",
    "    config.patch_model_to_enable_timeline(repopath=repo_path)\n",
    "\n",
    "# run the benchmark with the patch\n",
    "import sys\n",
    "benchmark_path = os.environ['ModelZooRoot']+os.sep+\"benchmarks/\"\n",
    "sys.path.append(benchmark_path)\n",
    "from launch_benchmark import LaunchBenchmark\n",
    "\n",
    "util = LaunchBenchmark()\n",
    "util.main()\n",
    "\n",
    "if enable_tf_timeline == True:\n",
    "    # unpatch related model script\n",
    "    config.unpatch_model_to_enable_timeline(model_path=repo_path+'/models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_6'></a>\n",
    "## Step 6: Parse output for performance number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Found the file path of the related runtime log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the path of the latest log file\n",
    "configvals=config.read_config(topology_name)\n",
    "import os\n",
    "for file in os.listdir(log_folder):\n",
    "    if file.endswith(\".log\"):\n",
    "        logpath = os.path.join(log_folder, file)\n",
    "        used_logpath = logpath + \".old\"\n",
    "        os.rename(logpath, used_logpath)\n",
    "        print(used_logpath)\n",
    "        break\n",
    "os.environ[\"TF_LOGPATH\"] = used_logpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the logfile for performance number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accuracy_only == False:\n",
    "    print(\"get throughput\")\n",
    "    val = config.throughput_keyword\n",
    "    index = int(config.throughput_index)\n",
    "    line = perfp.read_throughput(used_logpath, keyword=val, index=index)\n",
    "    if line!=None:\n",
    "        throughput=line\n",
    "        print(throughput)\n",
    "        # log the perf number\n",
    "        perfp.log_infer_perfcsv(0, throughput, 0, csv_fname)\n",
    "    else:\n",
    "        print(\"ERROR! can't find correct performance number from log. please check log for runtime issues\")\n",
    "else:\n",
    "    # get accuracy number and caculate throughput\n",
    "    print(\"get accuracy and throughput\")\n",
    "    #val = config.throughput_keyword\n",
    "    #index = int(config.throughput_index)\n",
    "    accuracy = perfp.read_accuracy(used_logpath)\n",
    "    iternation = perfp.read_iteration_time(used_logpath)\n",
    "    if accuracy != [] and iternation != []:\n",
    "        final_accuracy=accuracy[-1]\n",
    "        iternation_time = iternation[-1]\n",
    "        throughput = float(batch_size)/iternation_time\n",
    "        print(final_accuracy,throughput)\n",
    "        # log the perf number\n",
    "        perfp.log_infer_perfcsv(0, throughput, final_accuracy, csv_fname)\n",
    "    else:\n",
    "        print(\"ERROR! can't find correct performance number from log. please check log for runtime issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional : print out the log file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = open(used_logpath)\n",
    "logout = logfile.read()\n",
    "print(logout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users should be able to see a new Timeline json file after running the benchmark\n",
    "If users don't see a new timeline json file, they need to make sure that they patch the model script correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import CommonUtils\n",
    "utils = CommonUtils()\n",
    "paths = []\n",
    "paths.append(os.environ['ModelZooRoot']+os.sep + \"benchmarks\")\n",
    "paths.append(os.environ['ModelZooRoot']+os.sep + \"docs/notebooks/perf_analysis\")\n",
    "pattern = \"*.json\"\n",
    "timeline_files, timeline_paths = utils.found_files_in_folders(pattern, paths)\n",
    "if timeline_paths == []:\n",
    "    print(\"No %s files found\" %(pattern))\n",
    "else:\n",
    "    print(timeline_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users should be able to see new JIT DUMP files after running the benchmark if they enable the MKLDNN JIT DUMP feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import CommonUtils\n",
    "utils = CommonUtils()\n",
    "paths = []\n",
    "paths.append(os.environ['ModelZooRoot']+os.sep + \"benchmarks\")\n",
    "paths.append(os.environ['ModelZooRoot']+os.sep + \"docs/notebooks/perf_analysis\")\n",
    "pattern = \"*.bin\"\n",
    "jitdump_files, jitdump_paths = utils.found_files_in_folders(pattern, paths)\n",
    "if jitdump_paths == []:\n",
    "    print(\"No %s files found\" %(pattern) ,\"Need to set MKLDNN_JIT_DUMP as 1 first\")\n",
    "else:\n",
    "    print(jitdump_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Gather all generated JIT DUMP files\n",
    "Copy the jit dump files from benchmark folder to the JITDUMP folder.  \n",
    "Those jit dump files will be analyzed in another Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "jitdump_dir_path = os.environ['ModelZooRoot']+os.sep + \"docs/notebooks/perf_analysis\" + os.sep + \"JITDUMP\"\n",
    "if os.path.isfile(jitdump_dir_path) == False:\n",
    "    os.mkdir(jitdump_dir_path)\n",
    "shutil.move(os.environ['TF_LOGPATH'],jitdump_dir_path)\n",
    "if jitdump_paths != []:\n",
    "    for path in jitdump_paths:\n",
    "        shutil.move(path,jitdump_dir_path)\n",
    "target_path = jitdump_dir_path+'_'+os.environ['SELECTED_TYPE']\n",
    "shutil.move(jitdump_dir_path, target_path)\n",
    "print(target_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat Step 3 to Step 6 among different data types for selected topology/benchmark\n",
    "> NOTE : Please iterate over different data types before you start Step 7 for performance comparison.\n",
    "Users can pick one of below options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 1: Automatically pick next data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_index +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Go back Step 3.3 by clicking the link : [Step 3.3](#step_3_3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 2: manully pick next data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Go back Step 3.2 by clicking the link : [Step 3.2](#step_3_2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_7'></a>\n",
    "## Step 7: Draw the performance comparison diagram\n",
    "> NOTE: Please iterate over different data types before the Step 7 and the Step 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in csv_fname_list:\n",
    "    print(csv)\n",
    "for precision in selected_precision_list:\n",
    "    print(precision)\n",
    "print(selected_topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from profiling.profile_utils import PerfPresenter\n",
    "\n",
    "perfp=PerfPresenter(True)\n",
    "# inference  throughput\n",
    "title = 'Perf comparison among data types'\n",
    "perfp.draw_perf_diag_from_csvs(csv_fname_list, selected_precision_list, 'throughput','throughput (image/sec)', selected_topology, title)\n",
    "perfp.draw_perf_ratio_diag_from_csvs(csv_fname_list, selected_precision_list, 'throughput','speedup', selected_topology, title)\n",
    "if accuracy_only == True:\n",
    "    perfp.draw_perf_diag_from_csvs(csv_fname_list, selected_precision_list,'accuracy','accuracy', selected_topology, title)\n",
    "    perfp.draw_perf_ratio_diag_from_csvs(csv_fname_list, selected_precision_list,'accuracy','accuracy loss', selected_topology, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_8'></a>\n",
    "## Step 8: Gather all generated Tensorflow Timeline Json files and JITDUMP files\n",
    "Copy the timeline json files from benchmark folder to the Timeline folder.\n",
    "Those Timeline files will be analyzed in another Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import datetime\n",
    "from profiling.profile_utils import CommonUtils\n",
    "utils = CommonUtils()\n",
    "\n",
    "if timeline_paths != []:\n",
    "    timeline_dir_path = os.environ['ModelZooRoot']+os.sep + \"docs/notebooks/perf_analysis\" + os.sep + \"Timeline\"\n",
    "    if os.path.isfile(timeline_dir_path) == False:\n",
    "        os.mkdir(timeline_dir_path)\n",
    "    for path in timeline_paths:\n",
    "        shutil.move(path,timeline_dir_path)\n",
    "    # move JITDUMP results into Timeline folder     \n",
    "    pattern = \"JITDUMP_*\"\n",
    "    jitdump_fds, jitdump_fd_paths = utils.found_files_in_folder(pattern, os.environ['ModelZooRoot']+os.sep + \"docs/notebooks/perf_analysis\")\n",
    "    for fd_path in jitdump_fd_paths:\n",
    "        print(fd_path)\n",
    "        shutil.move(fd_path,timeline_dir_path)\n",
    "    # rename Timeline folder with topo and time info   \n",
    "    timeinfo = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "    target_path = timeline_dir_path+'_'+os.environ['SELECTED_TOPO']+'_'+timeinfo\n",
    "    shutil.move(timeline_dir_path, target_path)\n",
    "    print(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intel-tensorflow",
   "language": "python",
   "name": "intel-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
