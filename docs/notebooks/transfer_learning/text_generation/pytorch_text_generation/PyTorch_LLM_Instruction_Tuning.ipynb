{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f720dcc9",
   "metadata": {},
   "source": [
    "## Instruction Tuning for Text Generation using PyTorch and Hugging Face LLMs\n",
    "\n",
    "This notebook demonstrates how to instruction-tune pretrained large language models (LLMs) from [Hugging Face](https://huggingface.co) using datasets from the [Hugging Face Datasets catalog](https://huggingface.co/datasets) or a custom dataset.\n",
    "\n",
    "Please install the dependencies from [setup.md](/notebooks/setup.md) before executing this notebook.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "1. [Import dependencies and setup parameters](#1.-Import-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "    1. [Option A: Use a Hugging Face dataset](#Option-A:-Use-a-Hugging-Face-dataset)\n",
    "    2. [Option B: Use a custom dataset](#Option-B:-Use-a-custom-dataset)\n",
    "    3. [Map and tokenize the dataset](#Map-and-tokenize-the-dataset)\n",
    "    \n",
    "3. [Prepare the model and test domain knowledge](#3.-Prepare-the-model-and-test-domain-knowledge)\n",
    "4. [Transfer learning](#4.-Transfer-learning)\n",
    "5. [Retest domain knowledge](#5.-Retest-domain-knowledge)\n",
    "6. [Quantize the model](#6.-Quantize-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32c377",
   "metadata": {},
   "source": [
    "## 1. Import dependencies and setup parameters\n",
    "\n",
    "This notebook assumes that you have already followed the instructions in the [setup.md](/notebooks/setup.md) to setup a PyTorch environment with all the dependencies required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdba859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import urllib\n",
    "import warnings\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import logging as datasets_logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    TrainingArguments,\n",
    "    GenerationConfig,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "datasets_logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115e4d9",
   "metadata": {},
   "source": [
    "There is an additional [PEFT module](https://github.com/huggingface/peft) required to train models with low-rank adaptation (LoRA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462dac6d",
   "metadata": {},
   "source": [
    "Specify the name of the pretrained model from Hugging Face to use (https://huggingface.co/docs/transformers/tasks/language_modeling)\n",
    "\n",
    "Example: \n",
    "* distilgpt2\n",
    "* EleutherAI/gpt-j-6b\n",
    "* bigscience/bloom-560m\n",
    "* bigscience/bloomz-560m\n",
    "* bigscience/bloomz-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56da576",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-j-6b\"\n",
    "\n",
    "# Define an output directory\n",
    "output_dir = os.environ[\"OUTPUT_DIR\"] if \"OUTPUT_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"output\")\n",
    "\n",
    "# Define a dataset directory\n",
    "dataset_dir = os.environ[\"DATASET_DIR\"] if \"DATASET_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"dataset\")\n",
    "\n",
    "print(\"Model name:\", model_name)\n",
    "print(\"Output directory:\", output_dir)\n",
    "print(\"Dataset directory:\", dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45144538",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "The notebook has two options for getting a dataset:\n",
    "* Option A: Use a dataset from the [Hugging Face Datasets catalog](https://huggingface.co/datasets)\n",
    "* Option B: Use a custom dataset (downloaded from another source or from your local system)\n",
    "\n",
    "In both cases, we define objects for the train and (optional) validation splits and tokenize them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177f852",
   "metadata": {},
   "source": [
    "### Option A: Use a Hugging Face dataset\n",
    "\n",
    "[Hugging Face Datasets](https://huggingface.co/datasets) has a catalog of datasets that can be specified by name. Information about the dataset is available in the catalog (including information on the size of the dataset and the splits). For instruction-tuning, choose a dataset with fields for \"task\"/\"context\"/\"output\" or \"instruction\"/\"context\"/\"response\" or similar.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"instruction\": \"Convert this sentence into a question.\",\n",
    "    \"context\": \"He read the book.\",\n",
    "    \"response\": \"Did he read the book?\"\n",
    "}\n",
    "```\n",
    "\n",
    "For example: \n",
    "* databricks/databricks-dolly-15k\n",
    "* togethercomputer/RedPajama-Data-Instruct \n",
    "\n",
    "The next cell gets a dataset from the Hugging Face datasets API. If the notebook is executed multiple times, the dataset will be used from the dataset directory, to speed up the time that it takes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name =  'databricks/databricks-dolly-15k'\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the dataset does not have a validation split, create one\n",
    "if 'validation' not in dataset.keys():\n",
    "    dataset[\"validation\"] = load_dataset(dataset_name, split=f\"train[:25%]\")\n",
    "    dataset[\"train\"] = load_dataset(dataset_name, split=f\"train[25%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b457fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a random sample\n",
    "dataset['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7473c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this dictionary for the keys used in your dataset\n",
    "dataset_schema = {\n",
    "    \"instruction_key\": \"instruction\", \n",
    "    \"context_key\": \"context\",\n",
    "    \"response_key\": \"response\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43511d14",
   "metadata": {},
   "source": [
    "Skip ahead to [mapping and tokenizing](#Map-and-tokenize-the-dataset) the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321d967",
   "metadata": {},
   "source": [
    "### Option B: Use a custom dataset\n",
    "\n",
    "Instead of using a dataset from the Hugging Face dataset catalog, a custom JSON file from your local system or a download can be used.\n",
    "\n",
    "In this example, we download an instruction text dataset example, where each record of the dataset contains text fields for \"instruction\", \"input\", and \"output\" like the following:\n",
    "```\n",
    "{\n",
    "    \"instruction\": \"Convert this sentence into a question.\",\n",
    "    \"input\": \"He read the book.\",\n",
    "    \"output\": \"Did he read the book?\"\n",
    "}\n",
    "```\n",
    "If you are using a custom dataset or downloaded dataset that has similarly formatted json, you can use the same code as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ae4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a URL to download or skip this cell and provide a local path in the next cell\n",
    "url = \"https://raw.githubusercontent.com/sahil280114/codealpaca/master/data/code_alpaca_2k.json\"\n",
    "\n",
    "filename = os.path.basename(url)\n",
    "destination = os.path.join(dataset_dir, filename)\n",
    "\n",
    "# If we don't already have the json file, download it\n",
    "if not os.path.exists(destination):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    with open(destination, \"w\") as file:\n",
    "        file.write(data)\n",
    "    print('Downloaded file to {}'.format(destination))\n",
    "else:\n",
    "    print('Using existing file found at {}'.format(destination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89996638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize these variables if you want to load data from pre-existing local files\n",
    "train_file = destination\n",
    "validation_file = None\n",
    "\n",
    "data_files = {}\n",
    "dataset_args = {}\n",
    "data_files[\"train\"] = train_file\n",
    "if validation_file is not None:\n",
    "    data_files[\"validation\"] = validation_file\n",
    "extension = (\n",
    "    train_file.split(\".\")[-1]\n",
    "    if train_file is not None\n",
    "    else validation_file.split(\".\")[-1]\n",
    ")\n",
    "if extension == \"txt\":\n",
    "    extension = \"text\"\n",
    "\n",
    "dataset = load_dataset(extension, data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095feadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'validation' not in dataset.keys():\n",
    "    dataset[\"validation\"] = load_dataset(extension, data_files=data_files, split=f\"train[:25%]\")\n",
    "    dataset[\"train\"] = load_dataset(extension, data_files=data_files, split=f\"train[25%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a random sample\n",
    "dataset['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38758ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this dictionary for the keys used in your dataset\n",
    "dataset_schema = {\n",
    "    \"instruction_key\": \"instruction\", \n",
    "    \"context_key\": \"input\",\n",
    "    \"response_key\": \"output\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b741459",
   "metadata": {},
   "source": [
    "### Map and tokenize the dataset\n",
    "\n",
    "After describing the schema of your dataset, create formatted prompts out of each example for instruction-tuning. Then tokenize the prompts with the model's tokenizer and concatenate them together into longer sequences to speed up fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_with_context\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{{{instruction_key}}}\\n\\n### Context:\\n{{{context_key}}}\\n\\n### Response:\\n{{{response_key}}}\".format(\n",
    "        **dataset_schema)\n",
    "    ),\n",
    "    \"prompt_without_context\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{{{instruction_key}}}\\n\\n### Response:\\n{{{response_key}}}\".format(**dataset_schema)\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704af2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(examples):\n",
    "    prompts = []\n",
    "    for example in examples:\n",
    "        prompt_template = PROMPT_DICT[\"prompt_without_context\"] \\\n",
    "                if (dataset_schema['context_key'] not in example.keys() or \n",
    "                    example[dataset_schema['context_key']] is None) else PROMPT_DICT[\"prompt_with_context\"]\n",
    "        prompt = prompt_template.format_map(example)\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset:\n",
    "    prompts = create_prompts(dataset[key])\n",
    "    columns_to_be_removed = list(dataset[key].features.keys())\n",
    "    dataset[key] = dataset[key].add_column(\"prompts\", prompts)\n",
    "    dataset[key] = dataset[key].remove_columns(columns_to_be_removed)\n",
    "    \n",
    "dataset['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = (0) \n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c804c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    results = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    for i in range(len(results[\"input_ids\"])):\n",
    "        if (\n",
    "            results[\"input_ids\"][i][-1] != tokenizer.eos_token_id\n",
    "            and len(results[\"input_ids\"][i]) < max_seq_length\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            results[\"input_ids\"][i].append(tokenizer.eos_token_id)\n",
    "            results[\"attention_mask\"][i].append(1)\n",
    "\n",
    "    results[\"labels\"] = results[\"input_ids\"].copy()\n",
    "\n",
    "    return results\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenize(examples[\"prompts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac026c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_data(dataset, max_seq_length):\n",
    "    concatenated_dataset = {}\n",
    "    for column in dataset.features:\n",
    "        concatenated_data = [item for sample in dataset[column] for item in sample]\n",
    "        reshaped_data = [concatenated_data[i*max_seq_length:(i+1)*max_seq_length] \\\n",
    "            for i in range(len(concatenated_data) // max_seq_length)]\n",
    "        concatenated_dataset[column] = reshaped_data\n",
    "    return datasets.Dataset.from_dict(concatenated_dataset)\n",
    "\n",
    "tokenized_dataset_ = tokenized_dataset[\"train\"].remove_columns(\"prompts\")\n",
    "tokenized_dataset[\"train\"] = concatenate_data(tokenized_dataset_, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811fab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "validation_dataset = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a2709",
   "metadata": {},
   "source": [
    "## 3. Prepare the model and test domain knowledge\n",
    "\n",
    "This notebook uses the Hugging Face Trainer API to download a model for Causal Language Modeling and its associated tokenizer. Get the model and look at some output for a sample prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f42425",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint = False  # User adjust as needed\n",
    "experiment_identifier = 'bf16'  # User adjust as needed\n",
    "\n",
    "model_output_dir = os.path.join(output_dir, model_name, experiment_identifier)\n",
    "print(\"Model will be saved to:\", model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7089731",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_from_checkpoint:\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_output_dir)\n",
    "    except OSError:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        model = PeftModelForCausalLM.from_pretrained(model, model_output_dir)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print('Check the model class: {}'.format(type(model)))\n",
    "print('Check the model data type: {}'.format(model.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf6823",
   "metadata": {},
   "source": [
    "Use this sample prompt or write your own. Tokenize it, send it to the model for text generation, and then decode and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2288a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For code generation custom dataset\n",
    "prompt_template = PROMPT_DICT[\"prompt_with_context\"]\n",
    "test_example = {dataset_schema['instruction_key']: 'Write a Python function that sorts the following list.',\n",
    "               dataset_schema['context_key']: '[3, 2, 1]',\n",
    "               dataset_schema['response_key']: ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = prompt_template.format_map(test_example)\n",
    "\n",
    "encoded_input = tokenizer(test_prompt, padding=True, return_tensors='pt')\n",
    "num_tokens = len(encoded_input['input_ids'])\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=1.0,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.0,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "max_new_tokens=128\n",
    "\n",
    "output = model.generate(input_ids=encoded_input['input_ids'], \n",
    "                        generation_config=generation_config, \n",
    "                        max_new_tokens=max_new_tokens)\n",
    "\n",
    "test_output = tokenizer.batch_decode(output)\n",
    "print(test_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f1925",
   "metadata": {},
   "source": [
    "## 4. Transfer learning\n",
    "\n",
    "Set up the LoRA parameters and get the PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b33f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly mask the tokens\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "from llm_utils import hf_model_map\n",
    "model_info = hf_model_map[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8  # Rank parameter \n",
    "lora_alpha = 32  # Alpha parameter\n",
    "lora_dropout = 0.05  # Dropout parameter \n",
    "\n",
    "# PEFT settings\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c433db",
   "metadata": {},
   "source": [
    "Set up Hugging Face training arguments. For improved training time on Intel® fourth generation Xeon processors, you can experiment with `bf16=True` and `use_ipex=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94431240",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "do_eval = False  # Use the validation dataset to evaluate perplexity\n",
    "bf16 = True  # Train with bfloat16 precision\n",
    "use_ipex = False  # Use Intel® Optimization for PyTorch (IPEX)\n",
    "max_train_samples = None  # Option to truncate training samples for faster sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f15c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir, \n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    bf16=bf16,\n",
    "    use_ipex=use_ipex,\n",
    "    no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: \n",
    "if max_train_samples is not None:\n",
    "    train_dataset = train_dataset.select(range(max_train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a32718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset if do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42dc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_eval:\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to the model_output_dir\n",
    "model.save_pretrained(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b079f",
   "metadata": {},
   "source": [
    "## 5. Retest domain knowledge\n",
    "\n",
    "Inference with the test prompt to see if the fine-tuned model gives a better response. You may want to train for at least 3 epochs to see improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a42b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "if model.dtype == torch.bfloat16:\n",
    "    with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):\n",
    "        output = model.generate(input_ids=torch.tensor(encoded_input['input_ids']), \n",
    "                                generation_config=generation_config, \n",
    "                                max_new_tokens=max_new_tokens)\n",
    "else:\n",
    "    output = model.generate(input_ids=torch.tensor(encoded_input['input_ids']), \n",
    "                            generation_config=generation_config, \n",
    "                            max_new_tokens=max_new_tokens)\n",
    "    \n",
    "retest_output = tokenizer.batch_decode(output)\n",
    "print(retest_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79ca2e",
   "metadata": {},
   "source": [
    "## 6. Quantize the model\n",
    "\n",
    "If your model is not converted to bfloat16 precision and optimized by IPEX, quantizing it with Intel® Neural Compressor can result in faster inference times with little or no loss in accuracy. The purpose of this experimental section is to help you explore quantization recipes such as [SmoothQuant](https://github.com/intel/neural-compressor/blob/v2.1.1/docs/source/smooth_quant.md) for large language models. As Intel Neural Compressor adds more support for peft models with low-rank adaptation, we will expand this section.\n",
    "\n",
    "Import a data loader class that has a format compatible with Intel Neural Compressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils import INCDataloader\n",
    "\n",
    "calib_dataloader = INCDataloader(train_dataset, tokenizer, batch_size=8, max_seq_length=max_seq_length,\n",
    "                                 for_calib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc58e2",
   "metadata": {},
   "source": [
    "Next use the Intel Neural Compressor to benchmark the full precision model to see how it performs, as a baseline.\n",
    "\n",
    "> Note that there is a known issue when running Intel Neural Compressor from a notebook that you may sometimes see the error \n",
    "> `zmq.error.ZMQError: Address already in use`. If you see this error, rerun the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd57798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_compressor.config import BenchmarkConfig\n",
    "from neural_compressor.benchmark import fit\n",
    "\n",
    "backend = 'ipex' if use_ipex else 'default'\n",
    "config = BenchmarkConfig(backend=backend, warmup=50, iteration=500)\n",
    "\n",
    "results = fit(model=model, config=config, b_dataloader=calib_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d07a2",
   "metadata": {},
   "source": [
    "Use the Intel Neural Compressor to quantize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f915701",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = False # Whether to use kl divergence for calibration\n",
    "fallback_add = True # Whether to add fp32 fallback option, recommended for gpt-j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33bec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_compressor import quantization\n",
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "\n",
    "recipes = {\n",
    "    \"smooth_quant\": True,\n",
    "    \"smooth_quant_args\": {\n",
    "        \"alpha\": 0.5,\n",
    "        \"folding\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "op_type_dict = {}\n",
    "if kl:\n",
    "    op_type_dict = {'linear': {'activation': {'algorithm': ['kl']}}}\n",
    "if fallback_add:\n",
    "    op_type_dict['add'] = {\"weight\": {\"dtype\": [\"fp32\"]}, \"activation\": {\"dtype\": [\"fp32\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d96549",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PostTrainingQuantConfig(quant_level=1, backend=backend, excluded_precisions=[\"bf16\"],\n",
    "                                 recipes=recipes, op_type_dict=op_type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantization.fit(model=model, conf=config, calib_dataloader=calib_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e545a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "inc_output_dir = os.path.join(output_dir, model_name, '{}_quantized'.format(experiment_identifier))\n",
    "quantized_model.save(inc_output_dir)\n",
    "print(\"Quantized model directory:\", inc_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f0c4ab",
   "metadata": {},
   "source": [
    "Next we use the Intel Neural Compressor to benchmark the quantized model. These metrics can be compared with\n",
    "the benchmarking metrics from the full precision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_compressor.utils.pytorch import load\n",
    "\n",
    "# Load the quantized model using the INC utils method\n",
    "model_path = os.path.join(inc_output_dir, 'best_model.pt')\n",
    "quantized_model = load(model_path, model, dataloader=calib_dataloader)\n",
    "\n",
    "# Benchmark the quantized model\n",
    "config = BenchmarkConfig(backend=backend, warmup=50, iteration=500)\n",
    "results = fit(model=quantized_model, config=config, b_dataloader=calib_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229270d",
   "metadata": {},
   "source": [
    "Lastly, we can inspect the disk size of the pre- and post-quantization model files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The size of the un-compressed model:')\n",
    "!du -h {model_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The size of the compressed model:')\n",
    "!du -h {inc_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b60bdac",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "<b>[databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)</b> - Copyright (2023) Databricks, Inc. This dataset was developed at Databricks (https://www.databricks.com) and its use is subject to the CC BY-SA 3.0 license. Certain categories of material in the dataset include materials from the following sources, licensed under the CC BY-SA 3.0 license: Wikipedia (various pages) - https://www.wikipedia.org/ Copyright © Wikipedia editors and contributors.\n",
    "\n",
    "\n",
    "```\n",
    "@software{together2023redpajama,\n",
    "  author = {Together Computer},\n",
    "  title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},\n",
    "  month = April,\n",
    "  year = 2023,\n",
    "  url = {https://github.com/togethercomputer/RedPajama-Data}\n",
    "}\n",
    "```\n",
    "\n",
    "```\n",
    "@misc{codealpaca,\n",
    "  author = {Sahil Chaudhary},\n",
    "  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},\n",
    "  year = {2023},\n",
    "  publisher = {GitHub},\n",
    "  journal = {GitHub repository},\n",
    "  howpublished = {\\url{https://github.com/sahil280114/codealpaca}},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fbedc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
