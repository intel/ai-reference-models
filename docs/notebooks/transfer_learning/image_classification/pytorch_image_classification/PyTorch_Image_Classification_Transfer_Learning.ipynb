{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67b1e912",
   "metadata": {},
   "source": [
    "# Transfer Learning for Image Classification\n",
    "\n",
    "This notebook uses image classification models from [Torchvision](https://pytorch.org/vision/stable/index.html) that were originally trained using [ImageNet](https://image-net.org/) and does transfer learning with a Torchvision dataset or your own raw images.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "1. [Import dependencies and setup parameters](#1.-Import-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Predict using the original model](#3.-Predict-using-the-original-model)\n",
    "4. [Transfer learning](#4.-Transfer-learning)\n",
    "5. [Visualize the model output](#5.-Visualize-the-model-output)\n",
    "6. [Export the saved model](#6.-Export-the-saved-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a7f98",
   "metadata": {},
   "source": [
    "## 1. Import dependencies and setup parameters\n",
    "\n",
    "This notebook assumes that you have already followed the instructions in the [README.md](/notebooks/README.md) to setup a PyTorch environment with all the dependencies required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f98c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "from pydoc import locate\n",
    "import warnings\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model_utils import torchvision_model_map, get_retrainable_model\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Supported models:')\n",
    "print('\\n'.join(torchvision_model_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a model from the list above\n",
    "model_name = \"efficientnet_b0\"\n",
    "\n",
    "# Specify the the parent directory for the custom or Torchvision dataset\n",
    "dataset_directory = os.environ[\"DATASET_DIR\"] if \"DATASET_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"dataset\")\n",
    "    \n",
    "# Specify a directory for output\n",
    "output_directory = os.environ[\"OUTPUT_DIR\"] if \"OUTPUT_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"output\")\n",
    "\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Dataset directory:\", dataset_directory)\n",
    "print(\"Output directory:\", output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f85b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name not in torchvision_model_map.keys():\n",
    "    raise ValueError(\"The specified model_name ({}) is invalid. Please select from: {}\".\n",
    "                     format(model_name, torchvision_model_map.keys()))\n",
    "    \n",
    "print(\"Pretrained Image Classification Model:\", model_name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaecdff",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db4b34",
   "metadata": {},
   "source": [
    "Define transforms for data resizing and augmentation. The normalization means and standard deviations `[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]` are specific to torchvision image classification models and are explained in the [documentation](https://pytorch.org/vision/stable/models.html#classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing transforms\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.Resize([256, 256]))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip())\n",
    "    transforms.append(T.ToTensor())\n",
    "    transforms.append(T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cd4df40",
   "metadata": {},
   "source": [
    "### Option A: Use a Torchvision dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0b814cb",
   "metadata": {},
   "source": [
    "To use a Torchvision dataset, load from the Torchvision.datasets library, applying transforms for image augmentation, normalization, and resizing. This example uses the Food101 dataset from the [Torchvision datasets for image classification](https://pytorch.org/vision/stable/datasets.html#image-classification), but you can choose from a variety of options. If the dataset is not found in the dataset directory it is downloaded. Subsequent runs will reuse the already downloaded dataset.\n",
    "\n",
    "Note: Some Torchvision datasets use a `train=True/False` argument and others have a `split=\"train\"/\"test\"` convention. See the Torchvision documentation to see how to specify the subset you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.Food101(dataset_directory, split='train',\n",
    "                                       transform=get_transform(True), download=True)\n",
    "dataset_test = torchvision.datasets.Food101(dataset_directory, split='test',\n",
    "                                            transform=get_transform(False), download=True)   \n",
    "class_names = dataset.classes\n",
    "\n",
    "print('Training data size: {}'.format(len(dataset)))\n",
    "print('Validation data size: {}'.format(len(dataset_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb716b",
   "metadata": {},
   "source": [
    "Now skip ahead to the [Predict using the original model](#3.-Predict-using-the-original-model) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9656f",
   "metadata": {},
   "source": [
    "### Option B: Use a downloaded or custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235fa0f",
   "metadata": {},
   "source": [
    "To use your own image dataset for transfer learning with the rest of this notebook, format your images as `.jpg` files and save them in folders named after the classes that you want the model to predict. To provide a working example using the correct layout, we will download and extract a flower species dataset. After downloading and extracting, you will have the following  subdirectories in your dataset directory. Each species subfolder will contain numerous `.jpg` files:\n",
    "\n",
    "```\n",
    "dataset_directory\n",
    "└── flower_photos\n",
    "    └── daisy\n",
    "    └── dandelion\n",
    "    └── roses\n",
    "    └── sunflowers\n",
    "    └── tulips\n",
    "```\n",
    "\n",
    "Use this as an example to organize your own image files accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29105a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you have your own properly organized subdirectory of images, adjust this variable\n",
    "dataset_subdir = os.path.join(dataset_directory, \"flower_photos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e96b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if you want to use the example flowers dataset\n",
    "if not os.path.exists(dataset_subdir):\n",
    "    os.makedirs(dataset_subdir)\n",
    "    dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "    \n",
    "    # Download and extract the tar\n",
    "    zip_file = wget.download(dataset_url, dataset_directory)\n",
    "    print(\"Extracting {} to {}\".format(zip_file, dataset_directory))\n",
    "    with ZipFile(zip_file, \"r\") as zipfile:\n",
    "        zipfile.extractall(path=dataset_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5482b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(dataset_subdir, get_transform(True))\n",
    "dataset_test = datasets.ImageFolder(dataset_subdir, get_transform(False))\n",
    "class_names = dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 25% for validation and 75% for training\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "num_training_samples = math.floor(len(dataset)*.75)\n",
    "\n",
    "dataset_test = torch.utils.data.Subset(dataset, indices[-num_training_samples:])\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:num_training_samples])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2c043",
   "metadata": {},
   "source": [
    "## 3. Predict using the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader just for visualization\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=30,\n",
    "                                          shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ImageNet labels for displaying with the predictions\n",
    "imagenet_classes = []\n",
    "labels_file_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
    "labels_file_path = os.path.join(dataset_directory, os.path.basename(labels_file_url))\n",
    "if not os.path.exists(labels_file_url):\n",
    "    wget.download(labels_file_url, dataset_directory)\n",
    "\n",
    "with open(labels_file_path) as f:\n",
    "    imagenet_labels = f.readlines()\n",
    "    imagenet_classes = [l.strip() for l in imagenet_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pretrained torchvision model\n",
    "pretrained_model_class = locate('torchvision.models.{}'.format(model_name))\n",
    "model = pretrained_model_class(pretrained=True)\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(data_loader))\n",
    "\n",
    "# Get predictions from the pretrained model\n",
    "model.eval()\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e893dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the actual labels for this batch\n",
    "actual_label_batch = [class_names[int(id)] for id in classes]\n",
    "\n",
    "# List of the predicted labels for this batch\n",
    "_, predicted_id = torch.max(outputs, 1)\n",
    "predicted_label_batch = [imagenet_classes[id] for id in predicted_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb66dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results table to list out the ImageNet class prediction vs the actual dataset label\n",
    "results_table = []\n",
    "for prediction, actual in zip(predicted_label_batch, actual_label_batch):\n",
    "    results_table.append([prediction, actual])\n",
    "\n",
    "pd.DataFrame(results_table, columns=[\"ImageNet Prediction\", \"Actual Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9409af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,9))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "for n in range(30):\n",
    "    plt.subplot(6,5,n+1)\n",
    "    inp = inputs[n]\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.title(predicted_label_batch[n].title(), fontsize=9)\n",
    "    plt.axis('off')\n",
    "_ = plt.suptitle(\"ImageNet predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7d039",
   "metadata": {},
   "source": [
    "## 4. Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc029ef3",
   "metadata": {},
   "source": [
    "Replace the pretrained head of the network with a new layer based on the number of classes in our dataset. Train the model using the new dataset for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f0eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# To reduce training time, the feature extractor layer can remain frozen (do_fine_tuning=False).\n",
    "# Fine-tuning can be enabled to potentially get better accuracy. Note that enabling fine-tuning\n",
    "# will increase training time.\n",
    "do_fine_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, criterion, optimizer, dataset, dataset_test, num_epochs=10):\n",
    "    since = time.time()\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Create data loaders for training and validation\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "    data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataset)\n",
    "\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "            \n",
    "        # Iterate over data.\n",
    "        for inputs, labels in data_loader_test:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                    \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss = running_loss / len(dataset_test)\n",
    "        epoch_acc = running_corrects.double() / len(dataset_test)\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "                \n",
    "        print(f'Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        print()\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best Validation Accuracy: {best_acc:4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b754b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_retrainable_model(model_name, len(class_names), do_fine_tuning)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "print('Trainable parameters: {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
    "model = main(model, criterion, optimizer, dataset, dataset_test, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b2493",
   "metadata": {},
   "source": [
    "## 5. Visualize the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc700939",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(inputs)\n",
    "_, predicted_id = torch.max(outputs, 1)\n",
    "predicted_label_batch = [class_names[id] for id in predicted_id]\n",
    "\n",
    "# Display the results\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "for n in range(30):\n",
    "    plt.subplot(6,5,n+1)\n",
    "    inp = inputs[n]\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    correct_prediction = actual_label_batch[n] == predicted_label_batch[n]\n",
    "    color = \"darkgreen\" if correct_prediction else \"crimson\"\n",
    "    title = predicted_label_batch[n].title() if correct_prediction else \"{}\\n({})\".format(predicted_label_batch[n], actual_label_batch[n]) \n",
    "    plt.title(title, fontsize=9, color=color)\n",
    "    plt.axis('off')\n",
    "_ = plt.suptitle(\"Model predictions\")\n",
    "plt.show()\n",
    "print(\"Correct predictions are shown in green\")\n",
    "print(\"Incorrect predictions are shown in red with the actual label in parenthesis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d3602",
   "metadata": {},
   "source": [
    "## 6. Export the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "file_path = \"{}/image_classification.pt\".format(output_directory)\n",
    "torch.save(model.state_dict(), file_path)\n",
    "print(\"Saved to {}\".format(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0250c",
   "metadata": {},
   "source": [
    "## Dataset citations\n",
    "```\n",
    "@inproceedings{bossard14,\n",
    "  title = {Food-101 -- Mining Discriminative Components with Random Forests},\n",
    "  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},\n",
    "  booktitle = {European Conference on Computer Vision},\n",
    "  year = {2014}\n",
    "}\n",
    "\n",
    "@ONLINE {tfflowers,\n",
    "author = \"The TensorFlow Team\",\n",
    "title = \"Flowers\",\n",
    "month = \"jan\",\n",
    "year = \"2019\",\n",
    "url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
