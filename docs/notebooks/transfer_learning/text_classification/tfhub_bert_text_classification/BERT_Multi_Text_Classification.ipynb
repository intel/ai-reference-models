{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass text classification using BERT models from TF Hub\n",
    "\n",
    "This notebook demonstrates fine tuning BERT models from [TF Hub](https://tfhub.dev) with multiclass text classification datasets.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "1. [Import dependencies and setup parameters](#1.-Import-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Build the model](#3.-Build-the-model)\n",
    "4. [Fine tuning and evaluation](#4.-Fine-tuning-and-evaluation)\n",
    "5. [Export the model](#5.-Export-the-model)\n",
    "6. [Reload the model and make predictions](#6.-Reload-the-model-and-make-predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies and setup parameters\n",
    "\n",
    "This notebook assumes that you have already followed the instructions in the [README.md](/notebooks/README.md) to setup a TensorFlow environment with all the dependencies required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from bert_utils import get_model_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that tensorflow_text isn't used directly but the import is required to register ops used by the\n",
    "# BERT text preprocessor\n",
    "! pip3 install tensorflow-text==2.12.0 --no-deps\n",
    "import tensorflow_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will run one of the supported [BERT models from TF Hub](https://tfhub.dev/google/collections/bert/1). The table below has a list of the available models and links to their URLs in TF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TF Hub model map from json and print a list of the supported models\n",
    "tfhub_model_map, models_df = get_model_map(\"tfhub_bert_model_map_classifier.json\", return_data_frame=True)\n",
    "models_df.style.hide(axis=\"index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name of the BERT model to use. This string must match one of the models listed in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"small_bert/bert_en_uncased_L-2_H-128_A-2\"\n",
    "if model_name not in tfhub_model_map.keys():\n",
    "    raise ValueError(\"The specified model name ({}) is not supported\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a directory to download the dataset\n",
    "dataset_directory = os.environ[\"DATASET_DIR\"] if \"DATASET_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"dataset\")\n",
    "\n",
    "# Define an output directory for the saved model to be exported\n",
    "output_directory = os.environ[\"OUTPUT_DIR\"] if \"OUTPUT_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"output\")\n",
    "\n",
    "# Output directory for logs and checkpoints generated during training\n",
    "if not os.path.isdir(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "tfhub_preprocess = tfhub_model_map[model_name][\"preprocess\"]\n",
    "tfhub_bert_encoder = tfhub_model_map[model_name][\"bert_encoder\"]\n",
    "\n",
    "print(\"Using TF Hub model:\", model_name)\n",
    "print(\"BERT encoder URL:\", tfhub_bert_encoder)\n",
    "print(\"Preprocessor URL:\", tfhub_preprocess)\n",
    "print(\"Dataset directory:\", dataset_directory)\n",
    "print(\"Output directory:\", output_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "This notebook gets the dataset from a text file or from the [TensorFlow Datasets catalog](https://www.tensorflow.org/datasets/catalog/overview).\n",
    "\n",
    "The code ends up defining [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects for each split (train, validation, and test) and a map for the translating the numerical to string label.\n",
    "\n",
    "Execute the following cell to set the batch size and declare the base class used for the dataset setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Base class used for defining the multi text classification dataset being used\n",
    "class MultiTextClassificationData():\n",
    "    def __init__(self, batch_size, label_map):\n",
    "        self.batch_size = batch_size\n",
    "        self.label_map = label_map\n",
    "        self.reverse_label_map = {}\n",
    "        self.train_ds = None\n",
    "        self.val_ds = None\n",
    "        self.test_ds = None\n",
    "        self.dataset_name = \"\"\n",
    "        \n",
    "        for k, v in self.label_map.items():\n",
    "            self.reverse_label_map[v] = k\n",
    "        \n",
    "    def get_str_label(self, numerical_value):\n",
    "        if not isinstance(numerical_value, int):\n",
    "            numerical_value = int(tf.math.round(numerical_value))\n",
    "        \n",
    "        if numerical_value in self.label_map.keys():\n",
    "            return self.label_map[numerical_value]\n",
    "        else:\n",
    "            raise ValueError(\"The key {} was not found in the label map\".format(numerical_value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Use a TensorFlow dataset\n",
    "\n",
    "[TensorFlow Datasets](https://www.tensorflow.org/datasets) has a [catalog of datasets](https://www.tensorflow.org/datasets/catalog/overview) that can be specified by name. Information about the dataset is available in the catalog (including information on the size of the dataset and the splits).\n",
    "\n",
    "The next cell demonstrates using the [`ag_news_subset`](https://www.tensorflow.org/datasets/catalog/ag_news_subset) dataset from the TensorFlow datasets catalog to get splits for training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFDSMultiTextClassificationData(MultiTextClassificationData):\n",
    "    def __init__(self, dataset_dir, tfds_name, train_split, val_split, test_split, label_map, batch_size):\n",
    "        \"\"\"\n",
    "        Intialize the TFDSMultiTextClassificationData class for a dataset multi text classification dataset\n",
    "        from the TensorFlow dataset catalog.\n",
    "        \n",
    "        :param dataset_dir: Path to a dataset directory to read/write data\n",
    "        :param tfds_name: String name of the TensorFlow dataset to load\n",
    "        :param train_split: String specifying which split to load for training (e.g. \"train[:80%]\"). See the\n",
    "                            https://www.tensorflow.org/datasets/splits documentation for more information on\n",
    "                            defining splits.\n",
    "        :param val_split: String specifying the split to load for validation.\n",
    "        :param test_split: String specifying the split to load for test.\n",
    "        :param label_map: Dictionary where the key is a numerical value and the value is the string label\n",
    "        :param batch_size: Batch size\n",
    "        \"\"\"\n",
    "        # Init base class\n",
    "        MultiTextClassificationData.__init__(self, batch_size, label_map) \n",
    "        \n",
    "        [self.train_ds, self.val_ds, self.test_ds], info = tfds.load(tfds_name,\n",
    "                     data_dir=dataset_dir,\n",
    "                     split=[train_split, val_split, test_split],\n",
    "                     batch_size=batch_size,\n",
    "                     as_supervised=True,\n",
    "                     shuffle_files=True,\n",
    "                     with_info=True)\n",
    "        self.dataset_name = tfds_name\n",
    "        print(info)\n",
    "\n",
    "\n",
    "# Name of the TFDS to use\n",
    "tfds_name=\"ag_news_subset\"\n",
    "\n",
    "# Location where the dataset will be downloaded\n",
    "dataset_directory = os.path.join(dataset_directory, tfds_name)\n",
    "if not os.path.isdir(dataset_directory):\n",
    "    os.makedirs(dataset_directory)\n",
    "\n",
    "# Label map for sentiment analysis\n",
    "label_map = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "    \n",
    "# Initialize the dataset splits using a dataset from the TensorFlow datasets catalog\n",
    "dataset = TFDSMultiTextClassificationData(dataset_dir=dataset_directory,\n",
    "                                           tfds_name=tfds_name,\n",
    "                                           train_split=\"train[:50%]\",\n",
    "                                           val_split=\"train[:20%]\",\n",
    "                                           test_split=\"test[:20%]\",\n",
    "                                           label_map=label_map,\n",
    "                                           batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip to the next step [3. Build the model](#3.-Build-the-model) to continue using the TF dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Use your own dataset\n",
    "Instead of using a dataset from TensorFlow datasets, another dataset from your local system or a download can be used.\n",
    "\n",
    "In this example, we download the Conference Title dataset. This is a single tab-separated value file with two columns. The first column is the conference title and the second column is the label (VLDB, ISCAS, SIGGRAPH, INFOCOM, WWW):\n",
    "\n",
    "```\n",
    "<conference title>\t<label>\n",
    "<conference title>\t<label>\n",
    "<conference title>\t<label>\n",
    "...\n",
    "```\n",
    "\n",
    "If you are using a custom dataset that has a similarly formatted csv or tsv file, you can still use the class defined below. Just create your object passing in custom values for delimiter, header (whether the file has a header row), the label map, mapping function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCsvMultiTextClassificationData(MultiTextClassificationData):\n",
    "    def __init__(self, csv_file, delimiter, header, train_percent, val_percent,\n",
    "                 test_percent, label_map, batch_size, dataset_name, map_function=None):\n",
    "        \"\"\"\n",
    "        Intialize the CustomCsvMultiTextClassificationData class for a dataset multi text\n",
    "        classification dataset that uses a single csv file.\n",
    "        \n",
    "        :param csv_file: Path to the csv file\n",
    "        :param delimiter: String character that separates the fields in each row\n",
    "        :param header: Boolean indicating whether or not the csv file has a header line that should be skipped\n",
    "        :param train_percent: Decimal value for the percentage of the dataset that should be used for training\n",
    "                              (e.g. 0.8 for 80%)\n",
    "        :param val_percent: Decimal value for the percentage of the dataset that should be used for validation\n",
    "                            (e.g. 0.1 for 10%)\n",
    "        :param test_percent: Decimal value for the percentage of the dataset that should be used for test\n",
    "                             (e.g. 0.1 for 10%)\n",
    "        :param label_map: Dictionary where the key is a numerical value and the value is the string label\n",
    "        :param batch_size: Batch size\n",
    "        :param dataset_name: Name of the dataset. This is used later in this notebook for naming the saved model\n",
    "                             export folder and determining which input strings to use when testing the reloaded model\n",
    "        :param map_function: (Optional) If the csv file has string labels instead of the numerical values, provide a\n",
    "                             map function to apply on the dataset\n",
    "        \"\"\"\n",
    "        # Init base class\n",
    "        MultiTextClassificationData.__init__(self, batch_size, label_map)\n",
    "        \n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        if (train_percent + val_percent + test_percent) > 1:\n",
    "            raise ValueError(\"The combined value of the train percentage, validation percentage, and \" \\\n",
    "                             \"test percentage cannot be greater than 1\")\n",
    "        \n",
    "        if not os.path.exists(csv_file):\n",
    "            raise FileNotFoundError(\"Unable to find the csv file at\", csv_file)      \n",
    "\n",
    "        custom_dataset = tf.data.experimental.CsvDataset(filenames=csv_file,\n",
    "                                                         record_defaults=[tf.string, tf.string],\n",
    "                                                         field_delim=delimiter,\n",
    "                                                         use_quote_delim=True,\n",
    "                                                         header=header)\n",
    "\n",
    "        # Count the number of lines in the csv file to get the dataset length\n",
    "        custom_dataset_len = sum(1 for line in open(csv_file))\n",
    "\n",
    "        if header:\n",
    "            custom_dataset_len -= 1\n",
    "\n",
    "        # Optionally map the dataset labels using the map_function\n",
    "        if map_function:\n",
    "            custom_dataset = custom_dataset.map(map_function)\n",
    "        \n",
    "        # Create batches based on the specified batch size\n",
    "        custom_dataset = custom_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "        # Calculate sizes for the splits\n",
    "        total_num_batches = int(custom_dataset_len / batch_size)\n",
    "        train_size = int(train_percent * total_num_batches)\n",
    "        val_size = int(val_percent * total_num_batches)\n",
    "        test_size = int(test_percent * total_num_batches)\n",
    "\n",
    "        # Create the train, validation, and test splits\n",
    "        self.train_ds = custom_dataset.take(train_size)    \n",
    "        self.val_ds = custom_dataset.skip(train_size).take(val_size)\n",
    "        self.test_ds = custom_dataset.skip(train_size).skip(val_size)\n",
    "\n",
    "        # Set the cardinality so that progress bars will work properly\n",
    "        self.train_ds = self.train_ds.apply(tf.data.experimental.assert_cardinality(train_size))\n",
    "        self.val_ds = self.val_ds.apply(tf.data.experimental.assert_cardinality(val_size))\n",
    "        self.test_ds = self.test_ds.apply(tf.data.experimental.assert_cardinality(test_size))\n",
    "\n",
    "# Modify the variables below to use a different dataset or a csv file on your local system.\n",
    "# The csv_path variable should be pointing to a csv file with 2 columns (the label and the text)\n",
    "dataset_url = \"https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv\"\n",
    "dataset_directory = os.path.join(dataset_directory, \"titleconference\")\n",
    "csv_name = \"title_conference.csv\"\n",
    "delimiter = \",\"\n",
    "header = True  # Set to true if the csv file has a header row\n",
    "csv_path = os.path.join(dataset_directory, csv_name)\n",
    "\n",
    "if not os.path.exists(dataset_directory):\n",
    "    os.makedirs(dataset_directory)\n",
    "\n",
    "# If we don't already have the csv file, download and extract the zip file to get it.\n",
    "if not os.path.exists(csv_path):\n",
    "    df = pd.read_csv(dataset_url, header=0)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "label_map = {\n",
    "    \"VLDB\": 0,\n",
    "    \"ISCAS\": 1,\n",
    "    \"SIGGRAPH\": 2,\n",
    "    \"INFOCOM\": 3, \n",
    "    \"WWW\": 4\n",
    "}\n",
    "\n",
    "int_to_label_map ={}\n",
    "for k, v in label_map.items():\n",
    "    int_to_label_map[v] = k\n",
    "\n",
    "# Map function to translate labels in the csv file to numerical values when loading the dataset\n",
    "def map_title(features, label):\n",
    "    label = tf.py_function(lambda x: label_map[x.numpy().decode('utf-8')], [label], tf.int64)\n",
    "    return features, label\n",
    "\n",
    "# Initialize the dataset splits using the custom dataset\n",
    "dataset = CustomCsvMultiTextClassificationData(csv_file=csv_path,\n",
    "                                                delimiter=delimiter,\n",
    "                                                header=header,\n",
    "                                                train_percent=0.8,\n",
    "                                                val_percent=0.1,\n",
    "                                                test_percent=0.1,\n",
    "                                                batch_size=batch_size,\n",
    "                                                label_map=int_to_label_map,\n",
    "                                                dataset_name=csv_name,\n",
    "                                                map_function=map_title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model\n",
    "\n",
    "Create the BERT model to fine tune using a input layer, the preprocessing layer (from TF Hub), the BERT encoder layer (from TF Hub), one dense layer, and a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input_layer')\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(input_layer)\n",
    "encoder_layer = hub.KerasLayer(tfhub_bert_encoder, trainable=True, name='encoder')\n",
    "outputs = encoder_layer(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "net = tf.keras.layers.Dense(16, activation='relu', name='fully_connected_layer')(net)\n",
    "# Add dropout layer for regularization\n",
    "net = tf.keras.layers.Dropout(0.2)(net)\n",
    "net = tf.keras.layers.Dense(len(label_map), activation='softmax', name='classifier')(net)\n",
    "classifier_model = tf.keras.Model(input_layer, net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine tuning and evaluation\n",
    "\n",
    "Train the model for the specified number of epochs, then evaluate the model using the test dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that there is a known error during custom dataset training: `train_function (Empty logs). Please use Model.compile(..., run_eagerly=True), or tf.config.run_functions_eagerly(True) for more information of where went wrong, or file a issue/bug to tf.keras.`\n",
    "> If you see this error, try using the first dataset for at least partially training (it doesn't have to finish). Then re-run with the custom dataset and training should work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# The number of training epochs to run\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 3e-5\n",
    "\n",
    "# Maximum total input sequence length after WordPiece tokenization (longer sequences will be truncated)\n",
    "max_seq_length = 128\n",
    "\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5), \n",
    "                        loss='sparse_categorical_crossentropy', \n",
    "                        metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "history = classifier_model.fit(dataset.train_ds,\n",
    "                               validation_data=dataset.val_ds,\n",
    "                               epochs=num_train_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the accuracy using the test dataset. If the accuracy does not meet your expectations, try to increasing the size of the training dataset split or the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(dataset.test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using a single batch from the test dataset, and then display the results along with the input text and the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1\n",
    "predictions = classifier_model.predict(dataset.test_ds, batch_size=batch_size, steps=num_steps)\n",
    "\n",
    "prediction_list = []\n",
    "step_count = 0\n",
    "\n",
    "for batch in dataset.test_ds:\n",
    "    label_list = list(batch[1].numpy())\n",
    "    text_list = list(batch[0].numpy())\n",
    "    \n",
    "    for i, (text, actual_label) in enumerate(zip(text_list, label_list)):\n",
    "        score = tf.nn.softmax(predictions[i])\n",
    "        score = tf.reduce_max(score)\n",
    "        prediction = tf.math.argmax(predictions[i]).numpy()\n",
    "        prediction = dataset.get_str_label(prediction)\n",
    "        prediction_list.append([text.decode('utf-8'),\n",
    "                                tf.get_static_value(score),\n",
    "                                prediction,\n",
    "                                dataset.get_str_label(actual_label)])\n",
    "    \n",
    "    step_count += 1\n",
    "    if num_steps <= step_count:\n",
    "        break\n",
    "    \n",
    "result_df = pd.DataFrame(prediction_list, columns=[\"Input Text\", \"Score\", \"Predicted Label\", \"Actual Label\"])\n",
    "result_df.style.hide(axis=\"index\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export the model\n",
    "\n",
    "Since training has completed, export the `saved_model.pb` to the output directory in a folder with the model and dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"{}_{}\".format(model_name, dataset.dataset_name)\n",
    "model_dir = os.path.join(output_directory, model_dir)\n",
    "classifier_model.save(model_dir, include_optimizer=False)\n",
    "\n",
    "saved_model_path = os.path.join(model_dir, \"saved_model.pb\")\n",
    "if os.path.exists(saved_model_path):\n",
    "    print(\"Saved model location:\", saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reload the model and make predictions\n",
    "\n",
    "Reload from the `saved_model.pb` in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section defines a list of strings to send as input to the reloaded model. If you are using a dataset other than the [AG News dataset](https://www.tensorflow.org/datasets/catalog/ag_news_subset), you can update the snippet below with your own list of input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text = [\"WASHINGTON - Employers stepped up hiring in August, expanding payrolls by 144,000 and lowering the unemployment rate to 5.4 percent.\",\n",
    "              \"PRESENTACION, Philippines (Reuters) - Philippine communist rebels freed Wednesday two soldiers they had held as 'prisoners of war' for'\\\n",
    "              'more than five months, saying they wanted to rebuild confidence in peace talks with the government.\", \n",
    "              \"Geneva - Worldwide sales of industrial robots surged to record levels in the first half of 2004 after equipment prices fell while labour' \\\n",
    "              'costs grew, the United Nations Economic Commission for Europe said in a report to be released today.\"]\n",
    "    \n",
    "if not input_text:\n",
    "    raise ValueError(\"Please define the list of input_text strings.\")\n",
    "\n",
    "# Send the input text to the reloaded model\n",
    "predict_results = tf.nn.softmax(reloaded_model(tf.constant(input_text)))\n",
    "\n",
    "# Get the results into a data frame to display\n",
    "result_list = [[input_text[i],\n",
    "                tf.get_static_value(tf.reduce_max(predict_results[i])),\n",
    "                dataset.get_str_label(tf.math.argmax(predict_results[i]))] for i in range(len(input_text))]\n",
    "result_df = pd.DataFrame(result_list, columns=[\"Input Text\", \"Score\", \"Predicted Label\"])\n",
    "result_df.style.hide(axis=\"index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```\n",
    "@misc{zhang2015characterlevel,\n",
    "    title={Character-level Convolutional Networks for Text Classification},\n",
    "    author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n",
    "    year={2015},\n",
    "    eprint={1509.01626},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.LG}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tlt_notebook_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0aa4059be174cbf31be2e5e13e301e374cc50c7151267adca3135ebe59561bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
