diff --git a/official/legacy/image_classification/callbacks.py b/official/legacy/image_classification/callbacks.py
index 9360a72be..79263dbaa 100644
--- a/official/legacy/image_classification/callbacks.py
+++ b/official/legacy/image_classification/callbacks.py
@@ -77,6 +77,7 @@ def get_callbacks(
             save_weights_only=True,
             verbose=1))
     callbacks.append(MovingAverageCallback())
+  #callbacks.append(ThresholdStopping(monitor = "val_accuracy", threshold = 0.759))
   return callbacks
 
 
@@ -148,8 +149,8 @@ class CustomTensorBoard(tf.keras.callbacks.TensorBoard):
   def _calculate_metrics(self) -> MutableMapping[str, Any]:
     logs = {}
     # TODO(b/149030439): disable LR reporting.
-    # if self._track_lr:
-    #   logs['learning_rate'] = self._calculate_lr()
+    if self._track_lr:
+      logs['learning_rate'] = self._calculate_lr()
     return logs
 
   def _calculate_lr(self) -> int:
@@ -253,3 +254,37 @@ class AverageModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):
       result = super()._save_model(epoch, logs)  # pytype: disable=attribute-error  # typed-keras
       self.model.set_weights(non_avg_weights)
       return result
+
+class ThresholdStopping(tf.keras.callbacks.Callback):
+  def __init__(self,
+               monitor="val_accuracy",
+               threshold=1
+              ):
+    super().__init__()
+
+    self.monitor = monitor
+    self.threshold = threshold
+    self.stopped_epoch = 0
+
+  def on_train_begin(self, logs=None):
+    self.stopped_epoch = 0
+
+  def on_epoch_end(self, epoch, logs=None):
+    current = self.get_monitor_value(logs)
+    if current is None:
+      return
+    if current >= self.threshold:
+      self.stopped_epoch = epoch
+      self.model.stop_training = True
+
+  def get_monitor_value(self, logs):
+    logs = logs or {}
+    monitor_value = logs.get(self.monitor)
+    if monitor_value is None:
+      logging.warning(
+        "Early stopping conditioned on metric `%s` "
+        "which is not available. Available metrics are: %s",
+        self.monitor,
+        ",".join(list(logs.keys())),
+        )
+    return monitor_value
diff --git a/official/legacy/image_classification/classifier_trainer.py b/official/legacy/image_classification/classifier_trainer.py
index 8f1d2e6b6..ebd65f84d 100644
--- a/official/legacy/image_classification/classifier_trainer.py
+++ b/official/legacy/image_classification/classifier_trainer.py
@@ -36,7 +36,16 @@ from official.modeling import hyperparams
 from official.modeling import performance
 from official.utils import hyperparams_flags
 from official.utils.misc import keras_utils
+import time
 
+global is_mpi
+try:
+    import horovod.tensorflow.keras as hvd
+    hvd.init()
+    is_mpi = hvd.size()
+except ImportError:
+    is_mpi = 0
+    print("No MPI horovod support, this is running in no-MPI mode!")
 
 def get_models() -> Mapping[str, tf.keras.Model]:
   """Returns the mapping from model type name to Keras model."""
@@ -272,7 +281,7 @@ def define_classifier_flags():
       help='The name of the dataset, e.g. ImageNet, etc.')
   flags.DEFINE_integer(
       'log_steps',
-      default=100,
+      default=200,
       help='The interval of steps between logging of batch level stats.')
 
 
@@ -290,6 +299,12 @@ def train_and_eval(
   """Runs the train and eval path using compile/fit."""
   logging.info('Running train and eval.')
 
+  if is_mpi:
+    gpus = tf.config.experimental.list_physical_devices('XPU')
+    for gpu in gpus:
+      tf.config.experimental.set_memory_growth(gpu, True)
+    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'XPU')
+
   distribute_utils.configure_cluster(params.runtime.worker_hosts,
                                      params.runtime.task_index)
 
@@ -300,7 +315,7 @@ def train_and_eval(
       num_gpus=params.runtime.num_gpus,
       tpu_address=params.runtime.tpu)
 
-  strategy_scope = distribute_utils.get_strategy_scope(strategy)
+  #strategy_scope = distribute_utils.get_strategy_scope(strategy)
 
   logging.info('Detected %d devices.',
                strategy.num_replicas_in_sync if strategy else 1)
@@ -325,56 +340,74 @@ def train_and_eval(
 
   logging.info('Global batch size: %d', train_builder.global_batch_size)
 
-  with strategy_scope:
-    model_params = params.model.model_params.as_dict()
-    model = get_models()[params.model.name](**model_params)
-    learning_rate = optimizer_factory.build_learning_rate(
-        params=params.model.learning_rate,
-        batch_size=train_builder.global_batch_size,
-        train_epochs=train_epochs,
-        train_steps=train_steps)
-    optimizer = optimizer_factory.build_optimizer(
-        optimizer_name=params.model.optimizer.name,
-        base_learning_rate=learning_rate,
-        params=params.model.optimizer.as_dict(),
-        model=model)
-    optimizer = performance.configure_optimizer(
-        optimizer,
-        use_float16=train_builder.dtype == 'float16',
-        loss_scale=get_loss_scale(params))
-
-    metrics_map = _get_metrics(one_hot)
-    metrics = [metrics_map[metric] for metric in params.train.metrics]
-    steps_per_loop = train_steps if params.train.set_epoch_loop else 1
-
-    if one_hot:
-      loss_obj = tf.keras.losses.CategoricalCrossentropy(
-          label_smoothing=params.model.loss.label_smoothing)
-    else:
-      loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()
-    model.compile(
-        optimizer=optimizer,
-        loss=loss_obj,
-        metrics=metrics,
-        steps_per_execution=steps_per_loop)
-
-    initial_epoch = 0
-    if params.train.resume_checkpoint:
-      initial_epoch = resume_from_checkpoint(
-          model=model, model_dir=params.model_dir, train_steps=train_steps)
+  model_params = params.model.model_params.as_dict()
+  model = get_models()[params.model.name](**model_params)
+  learning_rate = optimizer_factory.build_learning_rate(
+    params=params.model.learning_rate,
+    batch_size=train_builder.global_batch_size * hvd.size(),
+    train_epochs=train_epochs,
+    train_steps=train_steps)
+  optimizer = optimizer_factory.build_optimizer(
+    optimizer_name=params.model.optimizer.name,
+    base_learning_rate=learning_rate,
+    params=params.model.optimizer.as_dict(),
+    model=model)
+  optimizer = performance.configure_optimizer(
+    optimizer,
+    use_float16=train_builder.dtype == 'float16',
+    loss_scale=get_loss_scale(params))
+
+  metrics_map = _get_metrics(one_hot)
+  metrics = [metrics_map[metric] for metric in params.train.metrics]
+  steps_per_loop = train_steps if params.train.set_epoch_loop else 1
 
-    callbacks = custom_callbacks.get_callbacks(
-        model_checkpoint=params.train.callbacks.enable_checkpoint_and_export,
-        include_tensorboard=params.train.callbacks.enable_tensorboard,
-        time_history=params.train.callbacks.enable_time_history,
-        track_lr=params.train.tensorboard.track_lr,
-        write_model_weights=params.train.tensorboard.write_model_weights,
-        initial_step=initial_epoch * train_steps,
-        batch_size=train_builder.global_batch_size,
-        log_steps=params.train.time_history.log_steps,
-        model_dir=params.model_dir,
-        backup_and_restore=params.train.callbacks.enable_backup_and_restore)
+  if one_hot:
+    loss_obj = tf.keras.losses.CategoricalCrossentropy(
+      label_smoothing=params.model.loss.label_smoothing)
+  else:
+    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()
+
+  hvd_optimizer = hvd.DistributedOptimizer(optimizer, num_groups=1)
+  model.compile(
+    optimizer=hvd_optimizer,
+    loss=loss_obj,
+    metrics=metrics,
+    steps_per_execution=steps_per_loop)
 
+  initial_epoch = 0
+  if params.train.resume_checkpoint:
+    initial_epoch = resume_from_checkpoint(
+      model=model, model_dir=params.model_dir, train_steps=train_steps)
+
+  # Add broadcast callback for rank0
+  callbacks = []
+
+  if hvd.local_rank() == 0:
+    callbacks = custom_callbacks.get_callbacks(
+      model_checkpoint=params.train.callbacks.enable_checkpoint_and_export,
+      include_tensorboard=params.train.callbacks.enable_tensorboard,
+      time_history=params.train.callbacks.enable_time_history,
+      track_lr=params.train.tensorboard.track_lr,
+      write_model_weights=params.train.tensorboard.write_model_weights,
+      initial_step=initial_epoch * train_steps,
+      batch_size=train_builder.global_batch_size,
+      log_steps=params.train.time_history.log_steps,
+      model_dir=params.model_dir,
+      backup_and_restore=params.train.callbacks.enable_backup_and_restore)
+  else:
+    callbacks = custom_callbacks.get_callbacks(
+      model_checkpoint=False,
+      include_tensorboard=params.train.callbacks.enable_tensorboard,
+      time_history=params.train.callbacks.enable_time_history,
+      track_lr=params.train.tensorboard.track_lr,
+      write_model_weights=params.train.tensorboard.write_model_weights,
+      initial_step=initial_epoch * train_steps,
+      batch_size=train_builder.global_batch_size,
+      log_steps=params.train.time_history.log_steps,
+      model_dir=params.model_dir,
+      backup_and_restore=params.train.callbacks.enable_backup_and_restore)
+
+  callbacks.append(hvd.callbacks.BroadcastGlobalVariablesCallback(0))
   serialize_config(params=params, model_dir=params.model_dir)
 
   if params.evaluation.skip_eval:
@@ -386,6 +419,10 @@ def train_and_eval(
         'validation_freq': params.evaluation.epochs_between_evals,
     }
 
+  print('[info] Training steps = ', train_steps)
+  print('[info] Validation steps = ', validation_steps)
+  global_start_time = time.time()
+
   history = model.fit(
       train_dataset,
       epochs=train_epochs,
@@ -395,6 +432,11 @@ def train_and_eval(
       verbose=2,
       **validation_kwargs)
 
+  global_end_time = time.time()
+  print('[info] Global start time = ', time.asctime(time.localtime(global_start_time)))
+  print('[info] Global end time = ', time.asctime(time.localtime(global_end_time)))
+  print('[info] Global consume time = ', ((global_end_time - global_start_time) / (60.0)), ' mins')
+
   validation_output = None
   if not params.evaluation.skip_eval:
     validation_output = model.evaluate(
diff --git a/official/legacy/image_classification/configs/base_configs.py b/official/legacy/image_classification/configs/base_configs.py
index 975a1be9d..c7ad5c30c 100644
--- a/official/legacy/image_classification/configs/base_configs.py
+++ b/official/legacy/image_classification/configs/base_configs.py
@@ -185,6 +185,7 @@ class OptimizerConfig(hyperparams.Config):
   beta_1: float = None
   beta_2: float = None
   epsilon: float = None
+  weight_decay: float = None
 
 
 @dataclasses.dataclass
diff --git a/official/legacy/image_classification/dataset_factory.py b/official/legacy/image_classification/dataset_factory.py
index b0bd931e6..8eb254588 100644
--- a/official/legacy/image_classification/dataset_factory.py
+++ b/official/legacy/image_classification/dataset_factory.py
@@ -26,6 +26,7 @@ import tensorflow_datasets as tfds
 from official.legacy.image_classification import augment
 from official.legacy.image_classification import preprocessing
 from official.modeling.hyperparams import base_config
+import horovod.tensorflow as hvd
 
 AUGMENTERS = {
     'autoaugment': augment.AutoAugment,
@@ -204,7 +205,16 @@ class DatasetBuilder:
   def num_steps(self) -> int:
     """The number of steps (batches) to exhaust this dataset."""
     # Always divide by the global batch size to get the correct # of steps
-    return self.num_examples // self.global_batch_size
+    distributed_size = 1
+    if self.config.split == 'train':
+      distributed_size = hvd.size()
+    divide_steps = self.num_examples // (self.global_batch_size * distributed_size)
+    remain_steps = self.num_examples % (self.global_batch_size * distributed_size)
+    if remain_steps == 0:
+      return divide_steps
+    else:
+      return divide_steps + 1
+    #return self.num_examples // (self.global_batch_size * hvd.size())
 
   @property
   def dtype(self) -> tf.dtypes.DType:
@@ -399,14 +409,10 @@ class DatasetBuilder:
     Returns:
       A TensorFlow dataset outputting batched images and labels.
     """
-    if (self.config.builder != 'tfds' and self.input_context and
-        self.input_context.num_input_pipelines > 1):
-      dataset = dataset.shard(self.input_context.num_input_pipelines,
-                              self.input_context.input_pipeline_id)
+    if self.is_training:
+      dataset = dataset.shard(hvd.size(), hvd.rank())
       logging.info(
-          'Sharding the dataset: input_pipeline_id=%d '
-          'num_input_pipelines=%d', self.input_context.num_input_pipelines,
-          self.input_context.input_pipeline_id)
+          'Sharding the dataset: total size: {}, local rank: {}.'.format(hvd.size(), hvd.rank()))
 
     if self.is_training and self.config.builder == 'records':
       # Shuffle the input files.
@@ -451,10 +457,10 @@ class DatasetBuilder:
       # replicas automatically when strategy.distribute_datasets_from_function
       # is called, so we use local batch size here.
       dataset = dataset.batch(
-          self.local_batch_size, drop_remainder=self.is_training)
+          self.local_batch_size, drop_remainder=False)
     else:
       dataset = dataset.batch(
-          self.global_batch_size, drop_remainder=self.is_training)
+          self.global_batch_size, drop_remainder=False)
 
     # Prefetch overlaps in-feed with training
     dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
diff --git a/official/legacy/image_classification/lars_optimizer.py b/official/legacy/image_classification/lars_optimizer.py
new file mode 100644
index 000000000..029ae654e
--- /dev/null
+++ b/official/legacy/image_classification/lars_optimizer.py
@@ -0,0 +1,248 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Layer-wise Adaptive Rate Scaling optimizer for large-batch training."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+# from tf2_common.training import optimizer_v2modified
+from tensorflow.python.framework import ops
+
+from tensorflow.python.ops import array_ops
+from tensorflow.python.ops import linalg_ops
+from tensorflow.python.ops import math_ops
+from tensorflow.python.training import training_ops
+from tensorflow.python.ops import state_ops
+
+# from tensorflow.python.keras import backend_config
+# from tensorflow.python.keras.optimizer_v2 import optimizer_v2
+tf_minor = int(tf.__version__.split('.')[1])
+if tf_minor >=13:
+  from keras.src.optimizers.legacy import optimizer_v2
+  from keras.src import backend_config
+elif tf_minor >= 12:
+  from keras.optimizers.legacy import optimizer_v2
+  from keras import backend_config
+elif tf_minor >= 9:
+  from keras.optimizers.optimizer_v2 import optimizer_v2
+  from keras import backend_config
+elif tf_minor >= 6:
+  from keras.optimizer_v2 import optimizer_v2
+  from keras import backend_config
+else:
+  from tensorflow.python.keras.optimizer_v2 import optimizer_v2
+  from tensorflow.python.keras import backend_config
+
+# class LARSOptimizer(optimizer_v2modified.OptimizerV2Modified):
+class LARSOptimizer(optimizer_v2.OptimizerV2):
+  """Layer-wise Adaptive Rate Scaling for large batch training.
+
+  Introduced by "Large Batch Training of Convolutional Networks" by Y. You,
+  I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)
+
+  Implements the LARS learning rate scheme presented in the paper above. This
+  optimizer is useful when scaling the batch size to up to 32K without
+  significant performance degradation. It is recommended to use the optimizer
+  in conjunction with:
+      - Gradual learning rate warm-up
+      - Linear learning rate scaling
+      - Poly rule learning rate decay
+
+  Note, LARS scaling is currently only enabled for dense tensors. Sparse tensors
+  use the default momentum optimizer.
+  """
+
+  def __init__(
+      self,
+      learning_rate,
+      momentum=0.9,
+      weight_decay=0.0001,
+      # The LARS coefficient is a hyperparameter
+      eeta=0.001,
+      epsilon=0.0,
+      name="LARSOptimizer",
+      # Enable skipping variables from LARS scaling.
+      # TODO(sameerkm): Enable a direct mechanism to pass a
+      # subset of variables to the optimizer.
+      skip_list=None,
+      use_nesterov=False,
+      **kwargs):
+    """Construct a new LARS Optimizer.
+
+    Args:
+      learning_rate: A `Tensor`, floating point value, or a schedule that is a
+        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable
+        that takes no arguments and returns the actual value to use. The
+        learning rate.
+      momentum: A floating point value. Momentum hyperparameter.
+      weight_decay: A floating point value. Weight decay hyperparameter.
+      eeta: LARS coefficient as used in the paper. Dfault set to LARS
+        coefficient from the paper. (eeta / weight_decay) determines the highest
+        scaling factor in LARS.
+      epsilon: Optional epsilon parameter to be set in models that have very
+        small gradients. Default set to 0.0.
+      name: Optional name prefix for variables and ops created by LARSOptimizer.
+      skip_list: List of strings to enable skipping variables from LARS scaling.
+        If any of the strings in skip_list is a subset of var.name, variable
+        'var' is skipped from LARS scaling. For a typical classification model
+        with batch normalization, the skip_list is ['batch_normalization',
+        'bias']
+      use_nesterov: when set to True, nesterov momentum will be enabled
+      **kwargs: keyword arguments.
+
+    Raises:
+      ValueError: If a hyperparameter is set to a non-sensical value.
+    """
+    if momentum < 0.0:
+      raise ValueError("momentum should be positive: %s" % momentum)
+    if weight_decay < 0.0:
+      raise ValueError("weight_decay should be positive: %s" % weight_decay)
+    super(LARSOptimizer, self).__init__(name=name, **kwargs)
+
+    self._set_hyper("learning_rate", learning_rate)
+
+    # When directly using class members, instead of
+    # _set_hyper and _get_hyper (such as learning_rate above),
+    # the values are fixed after __init(), and not being
+    # updated during the training process.
+    # This provides better performance but less flexibility.
+    self.momentum = momentum
+    self.weight_decay = weight_decay
+    self.eeta = eeta
+    self.epsilon = epsilon or backend_config.epsilon()
+    self._skip_list = skip_list
+    self.use_nesterov = use_nesterov
+
+  def _prepare_local(self, var_device, var_dtype, apply_state):
+    lr_t = self._get_hyper("learning_rate", var_dtype)
+    local_step = math_ops.cast(self.iterations, var_dtype)
+    lr_t = math_ops.cast(lr_t(local_step), var_dtype)
+    learning_rate_t = array_ops.identity(lr_t)
+
+    apply_state[(var_device, var_dtype)].update(
+        dict(
+            learning_rate=learning_rate_t,
+            ))
+
+  def _create_slots(self, var_list):
+    for v in var_list:
+      self.add_slot(v, "momentum")
+
+  def compute_lr(self, grad, var, coefficients):
+    scaled_lr = coefficients["learning_rate"]
+    if self._skip_list is None or not any(v in var.name
+                                          for v in self._skip_list):
+      w_norm = linalg_ops.norm(var, ord=2)
+      g_norm = linalg_ops.norm(grad, ord=2)
+      trust_ratio = array_ops.where(
+          math_ops.greater(w_norm, 0),
+          array_ops.where(
+              math_ops.greater(g_norm, 0),
+              (self.eeta * w_norm /
+               (g_norm + self.weight_decay * w_norm + self.epsilon)), 1.0), 1.0)
+
+      scaled_lr = coefficients["learning_rate"] * trust_ratio
+      # Add the weight regularization gradient
+      grad = grad + self.weight_decay * var
+    return scaled_lr, grad
+
+  def _apply_dense(self, grad, var, apply_state=None):
+    var_device, var_dtype = var.device, var.dtype.base_dtype
+    coefficients = ((apply_state or {}).get((var_device, var_dtype))
+                    or self._fallback_apply_state(var_device, var_dtype))
+
+    scaled_lr, grad = self.compute_lr(grad, var, coefficients)
+    mom = self.get_slot(var, "momentum")
+    return training_ops.apply_momentum(
+        var,
+        mom,
+        math_ops.cast(1.0, var.dtype.base_dtype),
+        grad * scaled_lr,
+        self.momentum,
+        use_locking=False,
+        use_nesterov=self.use_nesterov)
+
+  def _resource_apply_dense(self, grad, var, apply_state=None):
+    var_device, var_dtype = var.device, var.dtype.base_dtype
+    coefficients = ((apply_state or {}).get((var_device, var_dtype))
+                    or self._fallback_apply_state(var_device, var_dtype))
+
+    scaled_lr, grad = self.compute_lr(grad, var, coefficients)
+    mom = self.get_slot(var, "momentum")
+    # Use ApplyKerasMomentum instead of ApplyMomentum
+    # training_ops.resource_apply_keras_momentum(
+    #     var.handle,
+    #     mom.handle,
+    #     scaled_lr,
+    #     grad,
+    #     coefficients["momentum"],
+    #     use_locking=False,
+    #     use_nesterov=self.use_nesterov)
+
+    mom_t = mom * self.momentum - grad * scaled_lr
+    mom_t = state_ops.assign(mom, mom_t, use_locking=False)
+    if self.use_nesterov:
+      var_t = var + mom_t * self.momentum - grad * scaled_lr
+    else:
+      var_t = var + mom_t
+    return state_ops.assign(var, var_t, use_locking=False).op
+
+  # Fallback to momentum optimizer for sparse tensors
+  def _apply_sparse(self, grad, var, apply_state=None):
+    var_device, var_dtype = var.device, var.dtype.base_dtype
+    coefficients = ((apply_state or {}).get((var_device, var_dtype))
+                    or self._fallback_apply_state(var_device, var_dtype))
+
+    mom = self.get_slot(var, "momentum")
+    return training_ops.sparse_apply_momentum(
+        var,
+        mom,
+        coefficients["learning_rate"],
+        grad.values,
+        grad.indices,
+        self.momentum,
+        use_locking=False,
+        use_nesterov=self.use_nesterov)
+
+  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
+    var_device, var_dtype = var.device, var.dtype.base_dtype
+    coefficients = ((apply_state or {}).get((var_device, var_dtype))
+                    or self._fallback_apply_state(var_device, var_dtype))
+
+    mom = self.get_slot(var, "momentum")
+    return training_ops.resource_sparse_apply_keras_momentum(
+        var.handle,
+        mom.handle,
+        coefficients["learning_rate"],
+        grad,
+        indices,
+        self.momentum,
+        use_locking=False,
+        use_nesterov=self.use_nesterov)
+
+  def get_config(self):
+    config = super(LARSOptimizer, self).get_config()
+    config.update({
+        "learning_rate": self._serialize_hyperparameter("learning_rate"),
+        "momentum": self.momentum,
+        "weight_decay": self.weight_decay,
+        "eeta": self.eeta,
+        "epsilon": self.epsilon,
+        "use_nesterov": self.use_nesterov,
+        "skip_list": self._skip_list,
+    })
+    return config
diff --git a/official/legacy/image_classification/learning_rate.py b/official/legacy/image_classification/learning_rate.py
index 94b3a9143..e4851d40b 100644
--- a/official/legacy/image_classification/learning_rate.py
+++ b/official/legacy/image_classification/learning_rate.py
@@ -21,10 +21,13 @@ from typing import Any, Mapping, Optional
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.python.util.tf_export import keras_export
+from tensorflow.python.framework import ops
+from tensorflow.python.ops import math_ops
 
 BASE_LEARNING_RATE = 0.1
 
-
+@tf.keras.utils.register_keras_serializable(package='Custom', name='WarmupDeacySchedule')
 class WarmupDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
   """A wrapper for LearningRateSchedule that includes warmup steps."""
 
@@ -65,10 +68,11 @@ class WarmupDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
     return lr
 
   def get_config(self) -> Mapping[str, Any]:
-    config = self._lr_schedule.get_config()
+    config = {}
     config.update({
         "warmup_steps": self._warmup_steps,
         "warmup_lr": self._warmup_lr,
+        "lr_schedule": self._lr_schedule,
     })
     return config
 
@@ -114,3 +118,103 @@ class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):
         "warmup_steps": self._warmup_steps,
         "init_learning_rate": self._init_learning_rate,
     }
+
+@tf.keras.utils.register_keras_serializable(package='Custom', name='PolynomialDeacyWithWarmup')
+class PolynomialDecayWithWarmup(
+tf.keras.optimizers.schedules.LearningRateSchedule):
+  """A LearningRateSchedule that uses a polynomial decay with warmup."""
+  def __init__(
+        self,
+        batch_size,
+        steps_per_epoch,
+        train_steps,
+        initial_learning_rate=None,
+        end_learning_rate=None,
+        warmup_epochs=None,
+        compute_lr_on_cpu=False,
+        name=None):
+    """Applies a polynomial decay to the learning rate with warmup."""
+    super(PolynomialDecayWithWarmup, self).__init__()
+
+    self.batch_size = batch_size
+    self.steps_per_epoch = steps_per_epoch
+    self.train_steps = train_steps
+    self.name = name
+    self.learning_rate_ops_cache = {}
+    self.compute_lr_on_cpu = compute_lr_on_cpu
+
+    if batch_size < 16384:
+        self.initial_learning_rate = 10.0
+        warmup_epochs_ = 5
+    elif batch_size < 32768:
+        self.initial_learning_rate = 25.0
+        warmup_epochs_ = 5
+    else:
+        self.initial_learning_rate = 31.2
+        warmup_epochs_ = 25
+
+    # Override default poly learning rate and warmup epochs
+    if initial_learning_rate:
+        self.initial_learning_rate = initial_learning_rate
+
+    if end_learning_rate:
+        self.end_learning_rate = end_learning_rate
+    else:
+        self.end_learning_rate = 0.0001
+
+    if warmup_epochs is not None:
+        warmup_epochs_ = warmup_epochs
+    self.warmup_epochs = warmup_epochs_
+
+    warmup_steps = warmup_epochs_ * steps_per_epoch
+    self.warmup_steps = tf.cast(warmup_steps, tf.float32)
+    self.decay_steps = train_steps - warmup_steps + 1
+    self.poly_rate_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(
+        initial_learning_rate=self.initial_learning_rate,
+        decay_steps=self.decay_steps,
+        end_learning_rate=self.end_learning_rate,
+        power=2.0)
+
+  def __call__(self, step):
+    if tf.executing_eagerly():
+      return self._get_learning_rate(step)
+
+    # In an eager function or graph, the current implementation of optimizer
+    # repeatedly call and thus create ops for the learning rate schedule. To
+    # avoid this, we cache the ops if not executing eagerly.
+    graph = tf.compat.v1.get_default_graph()
+    if graph not in self.learning_rate_ops_cache:
+      if self.compute_lr_on_cpu:
+        with tf.device('/device:CPU:0'):
+          self.learning_rate_ops_cache[graph] = self._get_learning_rate(step)
+      else:
+        self.learning_rate_ops_cache[graph] = self._get_learning_rate(step)
+    return self.learning_rate_ops_cache[graph]
+
+  def _get_learning_rate(self, step):
+    with ops.name_scope_v2(self.name or 'PolynomialDecayWithWarmup') as name:
+      initial_learning_rate = ops.convert_to_tensor(
+        self.initial_learning_rate, name='initial_learning_rate')
+      warmup_steps = ops.convert_to_tensor(
+        self.warmup_steps, name='warmup_steps')
+      step = tf.cast(step, tf.float32)
+      warmup_rate = (
+            initial_learning_rate * step / warmup_steps)
+
+      poly_steps = math_ops.subtract(step, warmup_steps)
+      poly_rate = self.poly_rate_scheduler(poly_steps)
+
+      decay_rate = tf.where(step <= warmup_steps,
+                          warmup_rate, poly_rate, name=name)
+      return decay_rate
+
+  def get_config(self):
+    return {
+      'batch_size': self.batch_size,
+      'steps_per_epoch': self.steps_per_epoch,
+      'train_steps': self.train_steps,
+      'initial_learning_rate': self.initial_learning_rate,
+      'end_learning_rate': self.end_learning_rate,
+      'warmup_epochs': self.warmup_epochs,
+      'name': self.name,
+    }
diff --git a/official/legacy/image_classification/optimizer_factory.py b/official/legacy/image_classification/optimizer_factory.py
index 2ba0ed050..e5cb134e8 100644
--- a/official/legacy/image_classification/optimizer_factory.py
+++ b/official/legacy/image_classification/optimizer_factory.py
@@ -27,6 +27,7 @@ from official.legacy.image_classification import learning_rate
 from official.legacy.image_classification.configs import base_configs
 from official.modeling import optimization
 from official.modeling.optimization import legacy_adamw
+import official.legacy.image_classification.lars_optimizer as lars_optimizer
 
 # pylint: disable=protected-access
 
@@ -256,6 +257,16 @@ def build_optimizer(
         beta_2=beta_2,
         epsilon=epsilon,
     )
+  elif optimizer_name == 'lars':
+    logging.info('Using Lars')
+    weight_decay = params.get('weight_decay', 0.0002)
+    epsilon = params.get('epsilon', 0)
+    optimizer = lars_optimizer.LARSOptimizer(
+          learning_rate=base_learning_rate,
+          momentum=params.get('momentum', 0.9),
+          weight_decay=weight_decay,
+          skip_list=['batch_normalization', 'bias', 'bn'],
+          epsilon=epsilon)
   else:
     raise ValueError('Unknown optimizer %s' % optimizer_name)
 
@@ -293,7 +304,7 @@ def build_learning_rate(params: base_configs.LearningRateConfig,
   else:
     warmup_steps = 0
 
-  lr_multiplier = params.scale_by_batch_size
+  lr_multiplier = 0 #params.scale_by_batch_size
 
   if lr_multiplier and lr_multiplier > 0:
     # Scale the learning rate based on the batch size and a multiplier
@@ -326,6 +337,14 @@ def build_learning_rate(params: base_configs.LearningRateConfig,
         batch_size=batch_size,
         total_steps=train_epochs * train_steps,
         warmup_steps=warmup_steps)
+  elif decay_type == 'polynomial':
+    lr = learning_rate.PolynomialDecayWithWarmup(
+        batch_size=batch_size,
+        steps_per_epoch=train_steps,
+        train_steps=train_epochs * train_steps,
+        initial_learning_rate=base_lr,
+        warmup_epochs=params.warmup_epochs)
+    return lr
   if warmup_steps > 0:
     if decay_type not in ['cosine_with_warmup']:
       logging.info('Applying %d warmup steps to the learning rate',
diff --git a/official/legacy/image_classification/resnet/imagenet_preprocessing.py b/official/legacy/image_classification/resnet/imagenet_preprocessing.py
index 5eab92596..8bed8900b 100644
--- a/official/legacy/image_classification/resnet/imagenet_preprocessing.py
+++ b/official/legacy/image_classification/resnet/imagenet_preprocessing.py
@@ -113,7 +113,7 @@ def process_record_dataset(dataset,
   dataset = dataset.map(
       lambda value: parse_record_fn(value, is_training, dtype),
       num_parallel_calls=tf.data.experimental.AUTOTUNE)
-  dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)
+  dataset = dataset.batch(batch_size, drop_remainder=False)
 
   # Operations between the final prefetch and the get_next call to the iterator
   # will happen synchronously during run time. We prefetch here again to
diff --git a/official/legacy/image_classification/resnet/resnet_runnable.py b/official/legacy/image_classification/resnet/resnet_runnable.py
index e37851564..302696c40 100644
--- a/official/legacy/image_classification/resnet/resnet_runnable.py
+++ b/official/legacy/image_classification/resnet/resnet_runnable.py
@@ -99,7 +99,7 @@ class ResnetRunnable(orbit.StandardTrainer, orbit.StandardEvaluator):
         datasets_num_private_threads=self.flags_obj
         .datasets_num_private_threads,
         dtype=self.dtype,
-        drop_remainder=True,
+        drop_remainder=False,
         training_dataset_cache=self.flags_obj.training_dataset_cache)
     orbit.StandardTrainer.__init__(
         self,
