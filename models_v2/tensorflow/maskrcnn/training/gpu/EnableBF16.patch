diff --git a/TensorFlow2/Segmentation/MaskRCNN/main.py b/TensorFlow2/Segmentation/MaskRCNN/main.py
index 9edc89cc..03029cae 100644
--- a/TensorFlow2/Segmentation/MaskRCNN/main.py
+++ b/TensorFlow2/Segmentation/MaskRCNN/main.py
@@ -57,6 +57,18 @@ def main():
     # setup dataset
     dataset = Dataset(params)
 
+    if params.horovod:
+      import tensorflow as tf
+      import horovod.tensorflow.keras as hvd
+      hvd.init()
+      if hvd.size() <= 1:
+        raise RuntimeError("Found hvd.size() <= 1, please ensure you are running multiple processes in horovod mode.")
+      xpus = tf.config.experimental.list_physical_devices('XPU')
+      tf.config.experimental.set_visible_devices(xpus[hvd.local_rank()], 'XPU')
+      logging.info("Horovod is enabled, this is running in MPI mode! local_rank = {}".format(hvd.local_rank()))
+    else:
+      logging.info("No Horovod support, this is running in no-MPI mode!")
+
     if params.mode == 'train':
         run_training(dataset, params)
     if params.mode == 'eval':
diff --git a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/arguments.py b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/arguments.py
index 63368c09..fa1a0e11 100644
--- a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/arguments.py
+++ b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/arguments.py
@@ -53,6 +53,12 @@ RUNTIME_GROUP.add_argument(
     ]
 )
 
+RUNTIME_GROUP.add_argument(
+    '--horovod',
+    action='store_true',
+    help='Run distributed multiple processes with horovod'
+)
+
 RUNTIME_GROUP.add_argument(
     '--data_dir',
     type=str,
@@ -72,7 +78,7 @@ RUNTIME_GROUP.add_argument(
 RUNTIME_GROUP.add_argument(
     '--backbone_checkpoint',
     type=str,
-    default='/weights/rn50_tf_amp_ckpt_v20.06.0/nvidia_rn50_tf_amp',
+    default=None,
     metavar='FILE',
     help='Pretrained checkpoint for resnet'
 )
diff --git a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/dataset/dataset.py b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/dataset/dataset.py
index 29d2373a..b8c874e0 100644
--- a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/dataset/dataset.py
+++ b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/dataset/dataset.py
@@ -48,8 +48,38 @@ class Dataset:
 
     def train_fn(self, batch_size):
         """ Input function for training. """
+
+        # The original code taking first batch and repeat, so it still depends on real data.
+        # We directly synthetic data here to avoid loading real data.
+        if self._params.use_synthetic_data:
+            self._logger.info("Using synthetic data")
+            name_to_features = {
+                "source_ids": tf.zeros((batch_size, ), dtype=tf.float32),
+                "images": tf.zeros((batch_size, 1024, 1024, 3), dtype=tf.float32),
+                "image_info": tf.zeros((batch_size, 5,) , dtype=tf.float32),
+                "cropped_gt_masks": tf.zeros((batch_size, 100, 116, 116), dtype=tf.float32),
+                "gt_boxes": tf.zeros((batch_size, 100, 4) , dtype=tf.float32),
+                "gt_classes": tf.zeros((batch_size, 100, 1) , dtype=tf.float32),
+                "score_targets_2": tf.zeros((batch_size, 256, 256, 3), dtype=tf.float32),
+                "box_targets_2": tf.zeros((batch_size, 256, 256, 12), dtype=tf.float32),
+                "score_targets_3": tf.zeros((batch_size, 128, 128, 3), dtype=tf.float32),
+                "box_targets_3": tf.zeros((batch_size, 128, 128, 12), dtype=tf.float32),
+                "score_targets_4": tf.zeros((batch_size, 64, 64, 3), dtype=tf.float32),
+                "box_targets_4": tf.zeros((batch_size, 64, 64, 12), dtype=tf.float32),
+                "score_targets_5": tf.zeros((batch_size, 32, 32, 3), dtype=tf.float32),
+                "box_targets_5": tf.zeros((batch_size, 32, 32, 12), dtype=tf.float32),
+                "score_targets_6": tf.zeros((batch_size, 16, 16, 3), dtype=tf.float32),
+                "box_targets_6": tf.zeros((batch_size, 16, 16, 12), dtype=tf.float32),
+            }
+
+            return tf.data.Dataset.from_tensors((name_to_features, {})).repeat()
+
         data = tf.data.TFRecordDataset(self._train_files)
 
+        if self._params.horovod:
+            import horovod.tensorflow.keras as hvd
+            data = data.shard(hvd.size(), hvd.rank())
+
         data = data.cache()
         data = data.shuffle(buffer_size=4096, reshuffle_each_iteration=True, seed=self._params.seed)
         data = data.repeat()
@@ -128,9 +158,9 @@ class Dataset:
         data_options.experimental_threading.max_intra_op_parallelism = 1
         data_options.experimental_optimization.map_parallelization = True
 
-        map_vectorization_options = tf.data.experimental.MapVectorizationOptions()
-        map_vectorization_options.enabled = True
-        map_vectorization_options.use_choose_fastest = True
-        data_options.experimental_optimization.map_vectorization = map_vectorization_options
+        # map_vectorization_options = tf.data.experimental.MapVectorizationOptions()
+        # map_vectorization_options.enabled = True
+        # map_vectorization_options.use_choose_fastest = True
+        # data_options.experimental_optimization.map_vectorization = map_vectorization_options
 
         return data_options
diff --git a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/dataset/dataset_parser.py b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/dataset/dataset_parser.py
index 96da0e36..3c071b10 100644
--- a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/dataset/dataset_parser.py
+++ b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/dataset/dataset_parser.py
@@ -99,7 +99,7 @@ def dataset_parser(value, mode, params, use_instance_mask, seed=None, regenerate
 
     example_decoder = create_example_decoder()
 
-    with tf.xla.experimental.jit_scope(compile_ops=True):
+    with tf.xla.experimental.jit_scope(compile_ops=False):
 
         with tf.name_scope('parser'):
 
diff --git a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/model/anchors.py b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/model/anchors.py
index ba922500..e6455b3f 100644
--- a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/model/anchors.py
+++ b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/model/anchors.py
@@ -240,7 +240,7 @@ class AnchorLabeler:
             tf.constant(0, dtype=tf.int32, shape=match_results.shape))
         ignore_labels = tf.fill(match_results.shape, -1)
 
-        return (ignore_labels + positive_labels + negative_labels,
+        return (tf.cast(ignore_labels + positive_labels + negative_labels, tf.float32),
                 positive_labels, negative_labels)
 
     def label_anchors(self, gt_boxes, gt_labels):
diff --git a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/model/losses.py b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/model/losses.py
index f4e22985..b681d4c0 100644
--- a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/model/losses.py
+++ b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/model/losses.py
@@ -28,8 +28,8 @@ class MaskRCNNLoss(tf.keras.layers.Layer):
     so that it doesn't expand `mask_targets`.
     """
 
-    def __init__(self):
-        super().__init__(trainable=False, dtype=tf.float32)
+    def __init__(self, name="MaskRCNNLoss"):
+        super().__init__(name=name, trainable=False, dtype=tf.float32)
 
     def call(self, inputs, **kwargs):
         """
@@ -82,8 +82,8 @@ class FastRCNNLoss(tf.keras.layers.Layer):
     Reference: https://github.com/facebookresearch/Detectron/blob/master/detectron/modeling/fast_rcnn_heads.py
     """
 
-    def __init__(self, num_classes):
-        super().__init__(trainable=False, dtype=tf.float32)
+    def __init__(self, num_classes, name="FastRCNNLoss"):
+        super().__init__(name=name, trainable=False, dtype=tf.float32)
         self._num_classes = num_classes
 
     def call(self, inputs, **kwargs):
@@ -155,8 +155,8 @@ class RPNLoss(tf.keras.layers.Layer):
     Computes total RPN detection loss including box and score from all levels.
     """
 
-    def __init__(self, batch_size, rpn_batch_size_per_im, min_level, max_level):
-        super().__init__(trainable=False, dtype=tf.float32)
+    def __init__(self, batch_size, rpn_batch_size_per_im, min_level, max_level, name="RPNLoss"):
+        super().__init__(name=name, trainable=False, dtype=tf.float32)
         self._batch_size = batch_size
         self._rpn_batch_size_per_im = rpn_batch_size_per_im
         self._min_level = min_level
diff --git a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/ops/roi_ops.py b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/ops/roi_ops.py
index 8670753e..deaf434a 100644
--- a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/ops/roi_ops.py
+++ b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/ops/roi_ops.py
@@ -96,16 +96,16 @@ def _propose_rois_gpu(scores,
         pre_nms_boxes = tf.cast(pre_nms_boxes, dtype=tf.float32)
         pre_nms_scores = tf.cast(pre_nms_scores, dtype=tf.float32)
 
-        with tf.device('CPU:0'):
-            boxes, scores, _, _ = tf.image.combined_non_max_suppression(
-                pre_nms_boxes,
-                pre_nms_scores,
-                max_output_size_per_class=topk_limit,
-                max_total_size=post_nms_topk_limit,
-                iou_threshold=rpn_nms_threshold,
-                score_threshold=0.0,
-                pad_per_class=False
-            )
+        # with tf.device('CPU:0'):
+        boxes, scores, _, _ = tf.image.combined_non_max_suppression(
+            pre_nms_boxes,
+            pre_nms_scores,
+            max_output_size_per_class=topk_limit,
+            max_total_size=post_nms_topk_limit,
+            iou_threshold=rpn_nms_threshold,
+            score_threshold=0.0,
+            pad_per_class=False
+        )
 
         boxes = box_utils.to_absolute_coordinates(boxes, height, width)
 
diff --git a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/runtime/learning_rate.py b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/runtime/learning_rate.py
index ee96214e..a3566a1a 100644
--- a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/runtime/learning_rate.py
+++ b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/runtime/learning_rate.py
@@ -1,6 +1,6 @@
 import tensorflow as tf
 
-
+@tf.keras.utils.register_keras_serializable('PiecewiseConstantWithWarmupSchedule')
 class PiecewiseConstantWithWarmupSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
     """
     Schedule that starts with learning rate at `init_value` and monotonically increases
diff --git a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/runtime/run.py b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/runtime/run.py
index d7b001b3..6c44f429 100644
--- a/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/runtime/run.py
+++ b/TensorFlow2/Segmentation/MaskRCNN/mrcnn_tf2/runtime/run.py
@@ -14,38 +14,43 @@ from mrcnn_tf2.runtime.weights_mapping import WEIGHTS_MAPPING
 def run_training(dataset, params):
     setup(params)
 
-    strategy = tf.distribute.MirroredStrategy()
-    params.replicas = strategy.num_replicas_in_sync
-    params.global_train_batch_size = params.train_batch_size * params.replicas
-    logging.info(f'Distributed Strategy is activated for {params.replicas} device(s)')
-
-    with strategy.scope():
-
-        learning_rate = PiecewiseConstantWithWarmupSchedule(
-            init_value=params.init_learning_rate,
-            # scale boundaries from epochs to steps
-            boundaries=[
-                int(b * dataset.train_size / params.global_train_batch_size)
-                for b in params.learning_rate_boundaries
-            ],
-            values=params.learning_rate_values,
-            # scale only by local BS as distributed strategy later scales it by number of replicas
-            scale=params.train_batch_size
-        )
+    params.replicas = 1
+    if params.horovod:
+        import horovod.tensorflow as hvd
+        num_processes = hvd.size()
+    else:
+        num_processes = 1
+    params.global_train_batch_size = params.train_batch_size * num_processes
+
+    learning_rate = PiecewiseConstantWithWarmupSchedule(
+        init_value=params.init_learning_rate,
+        # scale boundaries from epochs to steps
+        boundaries=[
+            int(b * dataset.train_size / params.global_train_batch_size)
+            for b in params.learning_rate_boundaries
+        ],
+        values=params.learning_rate_values,
+        # scale only by local BS as distributed strategy later scales it by number of replicas
+        scale=params.train_batch_size
+    )
 
-        optimizer = tf.keras.optimizers.SGD(
-            learning_rate=learning_rate,
-            momentum=params.momentum
-        )
+    optimizer = tf.keras.optimizers.legacy.SGD(
+        learning_rate=learning_rate,
+        momentum=params.momentum
+    )
 
-        mask_rcnn_model = create_model(params)
+    if params.horovod:
+        import horovod.tensorflow.keras as hvd
+        logging.info("MPI horovod is enabled, using hvd.DistributedOptimizer")
+        optimizer = hvd.DistributedOptimizer(optimizer, groups=1)
 
-        mask_rcnn_model.compile(
-            optimizer=optimizer
-        )
+    mask_rcnn_model = create_model(params)
 
-    # distributed strategy splits data between instances so we need global BS
-    train_data = dataset.train_fn(batch_size=params.global_train_batch_size)
+    mask_rcnn_model.compile(
+        optimizer=optimizer
+    )
+
+    train_data = dataset.train_fn(batch_size=params.train_batch_size)
 
     if params.eagerly:
         mask_rcnn_model.run_eagerly = True
@@ -112,8 +117,9 @@ def setup(params):
         logging.info('XLA is activated')
 
     if params.amp:
-        policy = tf.keras.mixed_precision.experimental.Policy("mixed_float16", loss_scale="dynamic")
-        tf.keras.mixed_precision.experimental.set_policy(policy)
+        # change precision from mixed_float16 to mixed_bfloat16
+        policy = tf.keras.mixed_precision.Policy("mixed_bfloat16")
+        tf.keras.mixed_precision.set_global_policy(policy)
         logging.info('AMP is activated')
 
 
@@ -164,13 +170,26 @@ def create_callbacks(params):
             mapping=lambda name: WEIGHTS_MAPPING.get(name.replace(':0', ''), name)
         )
 
-    yield tf.keras.callbacks.ModelCheckpoint(
-        filepath=os.path.join(params.model_dir, params.checkpoint_name_format),
-        verbose=1
-    )
+    if params.horovod:
+        import horovod.tensorflow.keras as hvd
+        if hvd.rank() == 0:
+            yield tf.keras.callbacks.ModelCheckpoint(
+                filepath=os.path.join(params.model_dir, params.checkpoint_name_format),
+                verbose=1, save_weights_only=True
+            )
+    else:
+        yield tf.keras.callbacks.ModelCheckpoint(
+            filepath=os.path.join(params.model_dir, params.checkpoint_name_format),
+            verbose=1, save_weights_only=True
+        )
 
     if params.log_tensorboard:
         yield tf.keras.callbacks.TensorBoard(
             log_dir=params.log_tensorboard,
             update_freq='batch'
         )
+
+    if params.horovod:
+        import horovod.tensorflow.keras as hvd
+        logging.info("MPI horovod is enabled, hvd.callbacks.BroadcastGlobalVariablesCallback")
+        yield hvd.callbacks.BroadcastGlobalVariablesCallback(0)
