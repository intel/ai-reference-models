diff --git a/keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py b/keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py
index e7e1896..21fba0c 100644
--- a/keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py
+++ b/keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py
@@ -14,6 +14,12 @@
 
 import tensorflow as tf
 from tensorflow import keras
+try:
+    import intel_extension_for_tensorflow as itex
+    keras.layers.GroupNormalization = itex.ops.GroupNormalization
+except:
+    pass
+    
 
 from keras_cv.models.stable_diffusion.__internal__.layers.padded_conv2d import (
     PaddedConv2D,
diff --git a/keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py b/keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py
index 29aeaaa..11d4be6 100644
--- a/keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py
+++ b/keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py
@@ -13,6 +13,13 @@
 # limitations under the License.
 
 from tensorflow import keras
+try:
+    import intel_extension_for_tensorflow as itex
+    keras.layers.GroupNormalization = itex.ops.GroupNormalization
+except:
+    pass
+    
+    
 
 from keras_cv.models.stable_diffusion.__internal__.layers.padded_conv2d import (
     PaddedConv2D,
diff --git a/keras_cv/models/stable_diffusion/decoder.py b/keras_cv/models/stable_diffusion/decoder.py
index fe619d3..ccc4b5b 100644
--- a/keras_cv/models/stable_diffusion/decoder.py
+++ b/keras_cv/models/stable_diffusion/decoder.py
@@ -13,6 +13,12 @@
 # limitations under the License.
 
 from tensorflow import keras
+try:
+    import intel_extension_for_tensorflow as itex
+    keras.layers.GroupNormalization = itex.ops.GroupNormalization
+except:
+    pass
+    
 
 from keras_cv.models.stable_diffusion.__internal__.layers.attention_block import (  # noqa: E501
     AttentionBlock,
diff --git a/keras_cv/models/stable_diffusion/diffusion_model.py b/keras_cv/models/stable_diffusion/diffusion_model.py
index 25b5241..dafdcbc 100644
--- a/keras_cv/models/stable_diffusion/diffusion_model.py
+++ b/keras_cv/models/stable_diffusion/diffusion_model.py
@@ -14,7 +14,14 @@
 
 import tensorflow as tf
 from tensorflow import keras
-
+try:
+    import intel_extension_for_tensorflow as itex
+    keras.layers.GroupNormalization = itex.ops.GroupNormalization
+    keras.layers.LayerNormalization = itex.ops.LayerNormalization
+except:
+    pass
+    
+    
 from keras_cv.models.stable_diffusion.__internal__.layers.padded_conv2d import (
     PaddedConv2D,
 )
@@ -302,6 +309,26 @@ class CrossAttention(keras.layers.Layer):
         self.num_heads = num_heads
         self.head_size = head_size
         self.out_proj = keras.layers.Dense(num_heads * head_size)
+        
+    def naive_scaled_dot_product_attention(self, query, key, value):
+        i_dtype = query.dtype
+        atten_scores = tf.matmul(query, key, transpose_b=True)
+        atten_scores = tf.multiply(atten_scores, tf.cast(self.scale, i_dtype))
+        atten_probs = tf.nn.softmax(atten_scores)
+        # `atten_output` = [B, N, F, H]
+        atten_output = tf.matmul(atten_probs, value)
+        # `atten_output` = [B, F, N, H]
+        atten_output  = tf.transpose(a=atten_output, perm=[0, 2, 1, 3]) 
+        return atten_output
+
+
+    def sdp(self, q, k, v):
+        try:
+            from intel_extension_for_tensorflow.python.ops.multi_head_attention import scaled_dot_product_attention
+            output = scaled_dot_product_attention(q, k, v, use_fast_attention=True, is_training=False)
+        except ImportError:
+            output = self.naive_scaled_dot_product_attention(q, k, v)
+        return output
 
     def call(self, inputs):
         inputs, context = inputs
@@ -316,17 +343,10 @@ class CrossAttention(keras.layers.Layer):
         )
 
         q = tf.transpose(q, (0, 2, 1, 3))  # (bs, num_heads, time, head_size)
-        k = tf.transpose(k, (0, 2, 3, 1))  # (bs, num_heads, head_size, time)
+        k = tf.transpose(k, (0, 2, 1, 3))  # (bs, num_heads, head_size, time)
         v = tf.transpose(v, (0, 2, 1, 3))  # (bs, num_heads, time, head_size)
 
-        score = td_dot(q, k) * self.scale
-        weights = keras.activations.softmax(
-            score
-        )  # (bs, num_heads, time, time)
-        attn = td_dot(weights, v)
-        attn = tf.transpose(
-            attn, (0, 2, 1, 3)
-        )  # (bs, time, num_heads, head_size)
+        attn = self.sdp(q, k, v)
         out = tf.reshape(
             attn, (-1, inputs.shape[1], self.num_heads * self.head_size)
         )
@@ -352,10 +372,11 @@ class GEGLU(keras.layers.Layer):
     def call(self, inputs):
         x = self.dense(inputs)
         x, gate = x[..., : self.output_dim], x[..., self.output_dim :]
-        tanh_res = keras.activations.tanh(
-            gate * 0.7978845608 * (1 + 0.044715 * (gate**2))
-        )
-        return x * 0.5 * gate * (1 + tanh_res)
+        # tanh_res = keras.activations.tanh(
+        #     gate * 0.7978845608 * (1 + 0.044715 * (gate**2))
+        # )
+        # return x * 0.5 * gate * (1 + tanh_res)
+        return x * tf.keras.activations.gelu(gate, approximate=True)
 
 
 def td_dot(a, b):
diff --git a/keras_cv/models/stable_diffusion/image_encoder.py b/keras_cv/models/stable_diffusion/image_encoder.py
index 614b11d..8212389 100644
--- a/keras_cv/models/stable_diffusion/image_encoder.py
+++ b/keras_cv/models/stable_diffusion/image_encoder.py
@@ -13,6 +13,12 @@
 # limitations under the License.
 
 from tensorflow import keras
+try:
+    import intel_extension_for_tensorflow as itex
+    keras.layers.GroupNormalization = itex.ops.GroupNormalization
+except:
+    pass
+
 
 from keras_cv.models.stable_diffusion.__internal__.layers.attention_block import (  # noqa: E501
     AttentionBlock,
diff --git a/keras_cv/models/stable_diffusion/stable_diffusion.py b/keras_cv/models/stable_diffusion/stable_diffusion.py
index 5ef7471..93d619d 100644
--- a/keras_cv/models/stable_diffusion/stable_diffusion.py
+++ b/keras_cv/models/stable_diffusion/stable_diffusion.py
@@ -29,7 +29,8 @@ import math
 import numpy as np
 import tensorflow as tf
 from tensorflow import keras
-
+from keras import backend as K
+import os
 from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer
 from keras_cv.models.stable_diffusion.constants import _ALPHAS_CUMPROD
 from keras_cv.models.stable_diffusion.constants import _UNCONDITIONAL_TOKENS
@@ -51,6 +52,7 @@ class StableDiffusionBase:
         img_height=512,
         img_width=512,
         jit_compile=False,
+        precision="fp32",
     ):
         # UNet requires multiples of 2**7 = 128
         img_height = round(img_height / 128) * 128
@@ -66,6 +68,7 @@ class StableDiffusionBase:
         self._tokenizer = None
 
         self.jit_compile = jit_compile
+        self.to_fp16 = (precision == "fp16")
 
     def text_to_image(
         self,
@@ -76,6 +79,9 @@ class StableDiffusionBase:
         unconditional_guidance_scale=7.5,
         seed=None,
     ):
+        if self.to_fp16:
+            K.set_floatx('float16')
+
         encoded_text = self.encode_text(prompt)
 
         return self.generate_image(
@@ -207,18 +213,21 @@ class StableDiffusionBase:
 
         # Iterative reverse diffusion stage
         timesteps = tf.range(1, 1000, 1000 // num_steps)
+        t_embs_lst = self._get_timesteps_embedding(timesteps, batch_size)
+        contexts = tf.concat((unconditional_context, context), 0)
+
         alphas, alphas_prev = self._get_initial_alphas(timesteps)
         progbar = keras.utils.Progbar(len(timesteps))
         iteration = 0
         for index, timestep in list(enumerate(timesteps))[::-1]:
             latent_prev = latent  # Set aside the previous latent vector
-            t_emb = self._get_timestep_embedding(timestep, batch_size)
-            unconditional_latent = self.diffusion_model.predict_on_batch(
-                [latent, t_emb, unconditional_context]
-            )
-            latent = self.diffusion_model.predict_on_batch(
-                [latent, t_emb, context]
-            )
+            latents = tf.concat((latent, latent), 0)
+            t_embs = t_embs_lst[index]
+
+            pred_latent = self.diffusion_model.predict_on_batch(
+                [latents, t_embs, contexts])
+            unconditional_latent, latent = tf.split(pred_latent, 2)
+
             latent = unconditional_latent + unconditional_guidance_scale * (
                 latent - unconditional_latent
             )
@@ -304,6 +313,23 @@ class StableDiffusionBase:
             self._tokenizer = SimpleTokenizer()
         return self._tokenizer
 
+    def _get_timesteps_embedding(
+        self, timesteps, batch_size, dim=320, max_period=10000
+    ):
+        half = dim // 2
+        freqs = tf.math.exp(
+            -math.log(max_period) * tf.range(0, half, dtype=tf.float32) / half
+        )
+        # timesteps shape: [num_steps]
+        args = tf.cast(tf.reshape(timesteps, [-1, 1]), dtype=tf.float32) * freqs
+        # embeddings shape:(steps, half)
+        embeddings = tf.concat([tf.math.cos(args), tf.math.sin(args)], axis=1)
+        embeddings = tf.expand_dims(embeddings, axis=1)
+        if self.to_fp16:
+            embeddings = tf.cast(embeddings, tf.float16)
+        #  2 is to concatenate the embedding of two forward pass
+        return tf.repeat(embeddings, batch_size * 2, axis=1)
+
     def _get_timestep_embedding(
         self, timestep, batch_size, dim=320, max_period=10000
     ):
@@ -314,6 +340,8 @@ class StableDiffusionBase:
         args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs
         embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)
         embedding = tf.reshape(embedding, [1, -1])
+        if self.to_fp16:
+            embedding = tf.cast(embedding, tf.float16)
         return tf.repeat(embedding, batch_size, axis=0)
 
     def _get_initial_alphas(self, timesteps):
@@ -324,14 +352,25 @@ class StableDiffusionBase:
 
     def _get_initial_diffusion_noise(self, batch_size, seed):
         if seed is not None:
-            return tf.random.stateless_normal(
-                (batch_size, self.img_height // 8, self.img_width // 8, 4),
-                seed=[seed, seed],
-            )
+            if self.to_fp16:
+                return tf.random.stateless_normal(
+                    (batch_size, self.img_height // 8, self.img_width // 8, 4),
+                    seed=[seed, seed], dtype=tf.float16
+                )
+            else:
+                return tf.random.stateless_normal(
+                    (batch_size, self.img_height // 8, self.img_width // 8, 4),
+                    seed=[seed, seed],
+                )
         else:
-            return tf.random.normal(
-                (batch_size, self.img_height // 8, self.img_width // 8, 4)
-            )
+            if self.to_fp16:
+                return tf.random.normal(
+                        (batch_size, self.img_height // 8, self.img_width // 8, 4), dtype=tf.float16
+                    )
+            else:
+                return tf.random.normal(
+                    (batch_size, self.img_height // 8, self.img_width // 8, 4)
+                )
 
     @staticmethod
     def _get_pos_ids():
@@ -390,8 +429,9 @@ class StableDiffusion(StableDiffusionBase):
         img_height=512,
         img_width=512,
         jit_compile=False,
+        precision="fp32",
     ):
-        super().__init__(img_height, img_width, jit_compile)
+        super().__init__(img_height, img_width, jit_compile, precision)
         print(
             "By using this model checkpoint, you acknowledge that its usage is "
             "subject to the terms of the CreativeML Open RAIL-M license at "
@@ -475,8 +515,9 @@ class StableDiffusionV2(StableDiffusionBase):
         img_height=512,
         img_width=512,
         jit_compile=False,
+        precision="fp32",
     ):
-        super().__init__(img_height, img_width, jit_compile)
+        super().__init__(img_height, img_width, jit_compile, precision)
         print(
             "By using this model checkpoint, you acknowledge that its usage is "
             "subject to the terms of the CreativeML Open RAIL++-M license at "
