From a82f7b321dc4f6b2e11441bdbe49d836432d7c98 Mon Sep 17 00:00:00 2001
From: "Wang, Yiting" <yiting.wang@intel.com>
Date: Fri, 3 Nov 2023 00:37:48 -0700
Subject: [PATCH] 1103 patch for bert large training

---
 .../LanguageModeling/BERT/common_flags.py     |  5 +
 .../create_small_dataset_for_benchmark.sh     | 30 ++++++
 .../official/modeling/model_training_utils.py |  5 +-
 .../modeling/model_training_utils_test.py     |  4 +-
 .../BERT/official/nlp/bert_modeling.py        |  9 +-
 .../BERT/official/nlp/bert_models.py          |  5 +
 .../official/nlp/modeling/layers/attention.py | 75 ++++++++++-----
 .../nlp/modeling/layers/transformer.py        | 94 +++++++++++++------
 .../networks/albert_transformer_encoder.py    |  4 +-
 .../nlp/modeling/networks/masked_lm.py        | 14 ++-
 .../modeling/networks/transformer_encoder.py  | 24 +++--
 .../BERT/official/utils/flags/flags_test.py   |  3 +
 .../LanguageModeling/BERT/optimization.py     |  2 +-
 .../LanguageModeling/BERT/run_pretraining.py  | 31 ++++--
 .../LanguageModeling/BERT/run_squad.py        | 60 +++++++++---
 .../BERT/scripts/run_pretraining_adam.sh      |  6 +-
 .../BERT/scripts/run_pretraining_lamb.sh      |  6 +-
 .../scripts/run_pretraining_lamb_phase1.sh    | 20 ++--
 .../scripts/run_pretraining_lamb_phase2.sh    | 28 +++---
 .../BERT/scripts/run_squad.sh                 | 22 ++++-
 20 files changed, 327 insertions(+), 120 deletions(-)
 create mode 100644 TensorFlow2/LanguageModeling/BERT/data/create_small_dataset_for_benchmark.sh

diff --git a/TensorFlow2/LanguageModeling/BERT/common_flags.py b/TensorFlow2/LanguageModeling/BERT/common_flags.py
index 0c471089..b7c4780e 100644
--- a/TensorFlow2/LanguageModeling/BERT/common_flags.py
+++ b/TensorFlow2/LanguageModeling/BERT/common_flags.py
@@ -75,6 +75,9 @@ def define_common_bert_flags():
   flags.DEFINE_boolean(
       'use_fp16', False,
       'Whether to use fp32 or fp16 arithmetic on GPU.')
+  flags.DEFINE_boolean(
+      'use_bf16', False,
+      'Whether to use fp32 or bf16 arithmetic on GPU.')
   flags.DEFINE_string("optimizer_type", "adam",
     "Optimizer used for training - LAMB or ADAM")
   flags.DEFINE_integer(
@@ -106,6 +109,8 @@ def define_common_bert_flags():
 def use_float16():
   return flags_core.get_tf_dtype(flags.FLAGS) == tf.float16

+def use_bfloat16():
+  return flags_core.get_tf_dtype(flags.FLAGS) == tf.bfloat16

 def get_loss_scale():
   return flags_core.get_loss_scale(flags.FLAGS, default_for_fp16='dynamic')
diff --git a/TensorFlow2/LanguageModeling/BERT/data/create_small_dataset_for_benchmark.sh b/TensorFlow2/LanguageModeling/BERT/data/create_small_dataset_for_benchmark.sh
new file mode 100644
index 00000000..61ba6762
--- /dev/null
+++ b/TensorFlow2/LanguageModeling/BERT/data/create_small_dataset_for_benchmark.sh
@@ -0,0 +1,30 @@
+#!/bin/bash
+# bash data/create_small_dataset_for_benchmark.sh [output_dir](default: ${PWD}/dataset)
+
+output_dir=${1:-"${PWD}/dataset"}
+echo "output_dir: ${output_dir}"
+if [ ! -d "${output_dir}/training" ]; then
+  mkdir -p ${output_dir}/training
+fi
+if [ ! -d "${output_dir}/test" ]; then
+  mkdir -p ${output_dir}/test
+fi
+
+# prepare pretrained_model
+cp -r data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/ ${output_dir}
+
+# prepare small training dataset
+for i in {0..23}
+do
+    echo "download training dataset: ${i}"
+    cp ./data/tfrecord/lower_case_1_seq_len_512_max_pred_80_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/wikicorpus_en/training/wikicorpus_en_training_${i}.tfrecord ${output_dir}/training
+done
+
+# prepare small test dataset
+for i in {0..23}
+do
+    echo "download test dataset: ${i}"
+    cp ./data/tfrecord/lower_case_1_seq_len_512_max_pred_80_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/wikicorpus_en/test/wikicorpus_en_test_${i}.tfrecord ${output_dir}/test
+done
+
+echo "Create small dataset done."
\ No newline at end of file
diff --git a/TensorFlow2/LanguageModeling/BERT/official/modeling/model_training_utils.py b/TensorFlow2/LanguageModeling/BERT/official/modeling/model_training_utils.py
index f5d080fa..0d568f76 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/modeling/model_training_utils.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/modeling/model_training_utils.py
@@ -269,7 +269,8 @@ def run_customized_training_loop(
           scaled_loss = optimizer.get_scaled_loss(loss)

       if hvd:
-        tape = hvd.DistributedGradientTape(tape, sparse_as_dense=True, compression=Compression.fp16 if use_float16 else Compression.none)
+        # use grouped_allreduce and set num_groups=1
+        tape = hvd.DistributedGradientTape(tape, sparse_as_dense=True, compression=Compression.fp16 if use_float16 else Compression.none, num_groups=1)

       if use_float16:
         scaled_grads = tape.gradient(scaled_loss, training_vars)
@@ -463,7 +464,7 @@ def run_customized_training_loop(

     current_step = optimizer.iterations.numpy()
     checkpoint_name = 'ctl_step_{step}.ckpt'
-    manager = tf.train.CheckpointManager(checkpoint, model_dir, max_to_keep=3)
+    manager = tf.train.CheckpointManager(checkpoint, model_dir, max_to_keep=2)
     FLAGS = params['FLAGS']
     steps_from_save = 0
     start_time = time.time()
diff --git a/TensorFlow2/LanguageModeling/BERT/official/modeling/model_training_utils_test.py b/TensorFlow2/LanguageModeling/BERT/official/modeling/model_training_utils_test.py
index b473aa9e..86e8267b 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/modeling/model_training_utils_test.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/modeling/model_training_utils_test.py
@@ -172,8 +172,8 @@ class ModelTrainingUtilsTest(tf.test.TestCase, parameterized.TestCase):
   @combinations.generate(eager_gpu_strategy_combinations())
   def test_train_eager_mixed_precision(self, distribution):
     model_dir = self.get_temp_dir()
-    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')
-    tf.keras.mixed_precision.experimental.set_policy(policy)
+    policy = tf.keras.mixed_precision.Policy('mixed_float16')
+    tf.keras.mixed_precision.set_global_policy(policy)
     self._model_fn = create_model_fn(
         input_shape=[128], num_classes=3, use_float16=True)
     self.run_training(
diff --git a/TensorFlow2/LanguageModeling/BERT/official/nlp/bert_modeling.py b/TensorFlow2/LanguageModeling/BERT/official/nlp/bert_modeling.py
index 6e4afa5e..3b976666 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/nlp/bert_modeling.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/nlp/bert_modeling.py
@@ -27,6 +27,11 @@ import tensorflow as tf
 from tensorflow.python.util import deprecation
 from official.modeling import tf_utils

+try:
+  import intel_extension_for_tensorflow as itex
+  itex.experimental_ops_override()
+except:
+  pass

 class BertConfig(object):
   """Configuration for `BertModel`."""
@@ -258,7 +263,9 @@ class BertModel(tf.keras.layers.Layer):
     word_embeddings = self.embedding_lookup(input_word_ids)
     embedding_tensor = self.embedding_postprocessor(
         word_embeddings=word_embeddings, token_type_ids=input_type_ids)
-    if self.float_type == tf.float16:
+    if self.float_type == tf.bfloat16:
+      embedding_tensor = tf.cast(embedding_tensor, tf.bfloat16)
+    elif self.float_type == tf.float16:
       embedding_tensor = tf.cast(embedding_tensor, tf.float16)
     attention_mask = None
     if input_mask is not None:
diff --git a/TensorFlow2/LanguageModeling/BERT/official/nlp/bert_models.py b/TensorFlow2/LanguageModeling/BERT/official/nlp/bert_models.py
index 4f1f8801..bd9e7f67 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/nlp/bert_models.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/nlp/bert_models.py
@@ -29,6 +29,11 @@ from official.nlp.modeling.networks import bert_classifier
 from official.nlp.modeling.networks import bert_pretrainer
 from official.nlp.modeling.networks import bert_span_labeler

+try:
+  import intel_extension_for_tensorflow as itex
+  itex.experimental_ops_override()
+except:
+  pass

 def gather_indexes(sequence_tensor, positions):
   """Gathers the vectors at the specific positions.
diff --git a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/layers/attention.py b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/layers/attention.py
index bf7ca5ac..eaf0de64 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/layers/attention.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/layers/attention.py
@@ -24,7 +24,14 @@ import tensorflow as tf

 from official.nlp.modeling.layers import dense_einsum
 from official.nlp.modeling.layers import masked_softmax
-
+IS_ITEX=False
+try:
+  import intel_extension_for_tensorflow as itex
+  itex.experimental_ops_override()
+  from intel_extension_for_tensorflow.python.ops.multi_head_attention import scaled_dot_product_attention
+  IS_ITEX=True
+except:
+  IS_ITEX=False

 # @tf.keras.utils.register_keras_serializable(package="Text")
 class Attention(tf.keras.layers.Layer):
@@ -59,6 +66,7 @@ class Attention(tf.keras.layers.Layer):
   """

   def __init__(self,
+               sequence_length,
                num_heads,
                head_size,
                dropout_rate=0.0,
@@ -71,6 +79,7 @@ class Attention(tf.keras.layers.Layer):
                bias_constraint=None,
                **kwargs):
     super(Attention, self).__init__(**kwargs)
+    self._sequence_length = sequence_length
     self._num_heads = num_heads
     self._head_size = head_size
     self._dropout_rate = dropout_rate
@@ -81,8 +90,8 @@ class Attention(tf.keras.layers.Layer):
     self._kernel_constraint = tf.keras.constraints.get(kernel_constraint)
     self._bias_constraint = tf.keras.constraints.get(bias_constraint)

-    self._query_dense = dense_einsum.DenseEinsum(
-        output_shape=(self._num_heads, self._head_size),
+    self._query_dense = tf.keras.layers.Dense(
+        self._num_heads * self._head_size,
         kernel_initializer=self._kernel_initializer,
         bias_initializer=self._bias_initializer,
         kernel_regularizer=self._kernel_regularizer,
@@ -92,8 +101,8 @@ class Attention(tf.keras.layers.Layer):
         bias_constraint=self._bias_constraint,
         name="query")

-    self._key_dense = dense_einsum.DenseEinsum(
-        output_shape=(self._num_heads, self._head_size),
+    self._key_dense = tf.keras.layers.Dense(
+        self._num_heads * self._head_size,
         kernel_initializer=self._kernel_initializer,
         bias_initializer=self._bias_initializer,
         kernel_regularizer=self._kernel_regularizer,
@@ -103,8 +112,8 @@ class Attention(tf.keras.layers.Layer):
         bias_constraint=self._bias_constraint,
         name="key")

-    self._value_dense = dense_einsum.DenseEinsum(
-        output_shape=(self._num_heads, self._head_size),
+    self._value_dense = tf.keras.layers.Dense(
+        self._num_heads * self._head_size,
         kernel_initializer=self._kernel_initializer,
         bias_initializer=self._bias_initializer,
         kernel_regularizer=self._kernel_regularizer,
@@ -156,13 +165,17 @@ class Attention(tf.keras.layers.Layer):
     #   N = `num_attention_heads`
     #   H = `size_per_head`
     # `query_tensor` = [B, F, N ,H]
-    query_tensor = self._query_dense(from_tensor)
+    query_tensor = self._query_dense(tf.reshape(from_tensor, (-1, self._num_heads * self._head_size)))
+    query_tensor = tf.reshape(query_tensor, (-1, self._sequence_length, self._num_heads, self._head_size))

     # `key_tensor` = [B, T, N, H]
-    key_tensor = self._key_dense(to_tensor)
+    key_tensor = self._key_dense(tf.reshape(to_tensor, (-1, self._num_heads * self._head_size)))
+    key_tensor = tf.reshape(key_tensor, (-1, self._sequence_length, self._num_heads, self._head_size))

     # `value_tensor` = [B, T, N, H]
-    value_tensor = self._value_dense(to_tensor)
+    value_tensor = self._value_dense(tf.reshape(to_tensor, (-1, self._num_heads * self._head_size)))
+    value_tensor = tf.reshape(value_tensor, (-1, self._sequence_length, self._num_heads, self._head_size))
+

     # Take the dot product between "query" and "key" to get the raw
     # attention scores.
@@ -174,22 +187,40 @@ class Attention(tf.keras.layers.Layer):
     query_tensor = tf.transpose(query_tensor, [0, 2, 1, 3])
     # `key_tensor` = [B, N, T, H]
     key_tensor = tf.transpose(key_tensor, [0, 2, 1, 3])
-    # `attention_scores` = [B, N, F, T]
-    attention_scores = tf.matmul(query_tensor, key_tensor, transpose_b=True)
+    # `key_tensor` = [B, N, T, H]
+    value_tensor = tf.transpose(value_tensor, [0, 2, 1, 3])

-    attention_scores = tf.multiply(attention_scores,
-                                   1.0 / math.sqrt(float(self._head_size)))
+    if(IS_ITEX):
+      if attention_mask is not None:
+        attention_mask = tf.expand_dims(attention_mask, axis=[1])

-    # Normalize the attention scores to probabilities.
-    # `attention_probs` = [B, N, F, T]
-    attention_probs = self._masked_softmax([attention_scores, attention_mask])
+        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
+        # masked positions, this operation will create a tensor which is 0.0 for
+        # positions we want to attend and -10000.0 for masked positions.
+        adder = (1.0 - tf.cast(attention_mask, query_tensor.dtype)) * -10000.0

-    # This is actually dropping out entire tokens to attend to, which might
-    # seem a bit unusual, but is taken from the original Transformer paper.
-    attention_probs = self._dropout(attention_probs)
+      context_layer = scaled_dot_product_attention(query_tensor, key_tensor, value_tensor, adder, self._dropout_rate, use_fast_attention=True, use_stateless_randomuniform=False)
+    else:  # default attention
+      # `attention_scores` = [B, N, F, T]
+      attention_scores = tf.matmul(query_tensor, key_tensor, transpose_b=True)

-    # `context_layer` = [B, F, N, H]
-    return tf.einsum("BNFT,BTNH->BFNH", attention_probs, value_tensor)
+      attention_scores = tf.multiply(attention_scores,
+                                    1.0 / math.sqrt(float(self._head_size)))
+
+      # Normalize the attention scores to probabilities.
+      # `attention_probs` = [B, N, F, T]
+      attention_probs = self._masked_softmax([attention_scores, attention_mask])
+
+      # This is actually dropping out entire tokens to attend to, which might
+      # seem a bit unusual, but is taken from the original Transformer paper.
+      attention_probs = self._dropout(attention_probs)
+
+      # `context_layer` = [B, F, N, H]
+      # return tf.einsum("BNFT,BTNH->BFNH", attention_probs, value_tensor)
+      context_layer = tf.matmul(attention_probs, value_tensor)
+      context_layer = tf.transpose(context_layer, [0, 2, 1, 3])
+
+    return tf.reshape(context_layer, (-1, self._num_heads * self._head_size))


 # @tf.keras.utils.register_keras_serializable(package="Text")
diff --git a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/layers/transformer.py b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/layers/transformer.py
index 76847be0..6ddb2ad5 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/layers/transformer.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/layers/transformer.py
@@ -20,6 +20,15 @@ from __future__ import division
 from __future__ import print_function

 import tensorflow as tf
+IS_ITEX=False
+try:
+  import intel_extension_for_tensorflow as itex
+  itex.experimental_ops_override()
+  from intel_extension_for_tensorflow.python.ops.activations import gelu as gelu_itex_impl
+  from intel_extension_for_tensorflow.python.ops.mlp import FusedDenseBiasAddGelu
+  IS_ITEX=True
+except:
+  IS_ITEX=False

 from official.nlp.modeling.layers import attention
 from official.nlp.modeling.layers import dense_einsum
@@ -100,6 +109,7 @@ class Transformer(tf.keras.layers.Layer):
     self._attention_head_size = int(hidden_size // self._num_heads)

     self._attention_layer = attention.Attention(
+        sequence_length=sequence_length,
         num_heads=self._num_heads,
         head_size=self._attention_head_size,
         dropout_rate=self._attention_dropout_rate,
@@ -111,9 +121,8 @@ class Transformer(tf.keras.layers.Layer):
         kernel_constraint=self._kernel_constraint,
         bias_constraint=self._bias_constraint,
         name="self_attention")
-    self._attention_output_dense = dense_einsum.DenseEinsum(
-        output_shape=hidden_size,
-        num_summed_dimensions=2,
+    self._attention_output_dense = tf.keras.layers.Dense(
+        hidden_size,
         kernel_initializer=self._kernel_initializer,
         bias_initializer=self._bias_initializer,
         kernel_regularizer=self._kernel_regularizer,
@@ -123,14 +132,33 @@ class Transformer(tf.keras.layers.Layer):
         bias_constraint=self._bias_constraint,
         name="self_attention_output")
     self._attention_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)
-    self._attention_layer_norm = (
-        tf.keras.layers.LayerNormalization(
-            name="self_attention_layer_norm",
-            axis=-1,
-            epsilon=1e-12,
-            dtype=tf.float32))
-    self._intermediate_dense = dense_einsum.DenseEinsum(
-        output_shape=self._intermediate_size,
+    if self.dtype == tf.float16:
+      self._attention_layer_norm = (
+          tf.keras.layers.LayerNormalization(
+              name="self_attention_layer_norm",
+              axis=-1,
+              epsilon=1e-12, dtype=tf.float32))
+    else:
+      self._attention_layer_norm = (
+          tf.keras.layers.LayerNormalization(
+              name="self_attention_layer_norm",
+              axis=-1,
+              epsilon=1e-12))
+    if(IS_ITEX):
+      self._intermediate_dense = FusedDenseBiasAddGelu(
+        self._intermediate_size,
+        kernel_initializer=self._kernel_initializer,
+        bias_initializer=self._bias_initializer,
+        kernel_regularizer=self._kernel_regularizer,
+        bias_regularizer=self._bias_regularizer,
+        activity_regularizer=self._activity_regularizer,
+        kernel_constraint=self._kernel_constraint,
+        bias_constraint=self._bias_constraint,
+        name="intermediate")
+      self._intermediate_activation_layer = gelu_itex_impl
+    else:
+      self._intermediate_dense = tf.keras.layers.Dense(
+        self._intermediate_size,
         activation=None,
         kernel_initializer=self._kernel_initializer,
         bias_initializer=self._bias_initializer,
@@ -140,10 +168,10 @@ class Transformer(tf.keras.layers.Layer):
         kernel_constraint=self._kernel_constraint,
         bias_constraint=self._bias_constraint,
         name="intermediate")
-    self._intermediate_activation_layer = tf.keras.layers.Activation(
+      self._intermediate_activation_layer = tf.keras.layers.Activation(
         self._intermediate_activation)
-    self._output_dense = dense_einsum.DenseEinsum(
-        output_shape=hidden_size,
+    self._output_dense = tf.keras.layers.Dense(
+        hidden_size,
         kernel_initializer=self._kernel_initializer,
         bias_initializer=self._bias_initializer,
         kernel_regularizer=self._kernel_regularizer,
@@ -153,8 +181,12 @@ class Transformer(tf.keras.layers.Layer):
         bias_constraint=self._bias_constraint,
         name="output")
     self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)
-    self._output_layer_norm = tf.keras.layers.LayerNormalization(
-        name="output_layer_norm", axis=-1, epsilon=1e-12, dtype=tf.float32)
+    if self.dtype == tf.float16:
+      self._output_layer_norm = tf.keras.layers.LayerNormalization(
+          name="output_layer_norm", axis=-1, epsilon=1e-12, dtype=tf.float32)
+    else:
+      self._output_layer_norm = tf.keras.layers.LayerNormalization(
+          name="output_layer_norm", axis=-1, epsilon=1e-12)

     super(Transformer, self).build(input_shape)

@@ -207,24 +239,30 @@ class Transformer(tf.keras.layers.Layer):
     if self.dtype == tf.float16:
       input_tensor = tf.cast(input_tensor, tf.float32)
       attention_output = tf.cast(attention_output, tf.float32)
-    attention_output = self._attention_layer_norm(input_tensor +
-                                                  attention_output)
+    input_tensor_shape = input_tensor.shape
+    batch_size, sequence_length, hidden_size = input_tensor_shape
+    attention_output = self._attention_layer_norm(tf.reshape(input_tensor, (-1, hidden_size)) +
+                                                  tf.reshape(attention_output, (-1, hidden_size)))
     intermediate_output = self._intermediate_dense(attention_output)
-    if self.dtype == tf.float16:
-      # Casts to float32 so that activation is done in float32.
-      intermediate_output = tf.cast(intermediate_output, tf.float32)
-      intermediate_output = self._intermediate_activation_layer(
-          intermediate_output)
-      intermediate_output = tf.cast(intermediate_output, tf.float16)
-    else:
-      intermediate_output = self._intermediate_activation_layer(
-          intermediate_output)
+    intermediate_output = tf.reshape(intermediate_output, [-1, self._intermediate_size])
+    if(not IS_ITEX):
+      if self.dtype == tf.float16:
+        # Casts to float32 so that activation is done in float32.
+        intermediate_output = tf.cast(intermediate_output, tf.float32)
+        intermediate_output = self._intermediate_activation_layer(
+            intermediate_output)
+        intermediate_output = tf.cast(intermediate_output, tf.float16)
+      else:
+        intermediate_output = self._intermediate_activation_layer(
+            intermediate_output)
     layer_output = self._output_dense(intermediate_output)
     layer_output = self._output_dropout(layer_output)
     # Use float32 in keras layer norm for numeric stability
     if self.dtype == tf.float16:
       layer_output = tf.cast(layer_output, tf.float32)
-    layer_output = self._output_layer_norm(layer_output + attention_output)
+    layer_output = self._output_layer_norm(tf.reshape(layer_output, (-1, hidden_size)) +
+            tf.reshape(attention_output, (-1, hidden_size)))
+    layer_output = tf.reshape(layer_output, (-1 , sequence_length, hidden_size))
     if self.dtype == tf.float16:
       layer_output = tf.cast(layer_output, tf.float16)

diff --git a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/albert_transformer_encoder.py b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/albert_transformer_encoder.py
index e18f198a..897f1f9e 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/albert_transformer_encoder.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/albert_transformer_encoder.py
@@ -158,7 +158,9 @@ class AlbertTransformerEncoder(tf.keras.Model):
           name='embedding_projection')(
               embeddings)

-    if float_dtype == 'float16':
+    if float_dtype == 'bfloat16':
+      embeddings = tf.cast(embeddings, tf.bfloat16)
+    elif float_dtype == 'float16':
       embeddings = tf.cast(embeddings, tf.float16)

     data = embeddings
diff --git a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/masked_lm.py b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/masked_lm.py
index 15ec13a3..4af29cb5 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/masked_lm.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/masked_lm.py
@@ -22,7 +22,11 @@ from __future__ import print_function
 import tensorflow as tf

 from official.modeling import tf_utils
-
+try:
+  import intel_extension_for_tensorflow as itex
+  itex.experimental_ops_override()
+except:
+  pass

 @tf.keras.utils.register_keras_serializable(package='Text')
 class MaskedLM(tf.keras.Model):
@@ -70,7 +74,13 @@ class MaskedLM(tf.keras.Model):
             activation=activation,
             kernel_initializer=initializer,
             name='cls/predictions/transform/dense')(masked_lm_input))
-    lm_data = tf.keras.layers.LayerNormalization(
+    if float_type == tf.float16:
+      lm_data = tf.keras.layers.LayerNormalization(
+        axis=-1, epsilon=1e-12, name='cls/predictions/transform/LayerNorm')(
+            tf.cast(lm_data, tf.float32))
+      lm_data = tf.cast(lm_data, tf.float16)
+    else:
+      lm_data = tf.keras.layers.LayerNormalization(
         axis=-1, epsilon=1e-12, name='cls/predictions/transform/LayerNorm')(
             lm_data)
     lm_data = tf.keras.layers.Lambda(
diff --git a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/transformer_encoder.py b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/transformer_encoder.py
index 6b8d3f77..b10ee381 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/transformer_encoder.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/nlp/modeling/networks/transformer_encoder.py
@@ -24,6 +24,11 @@ import tensorflow as tf
 from official.modeling import activations
 from official.nlp.modeling import layers

+try:
+  import intel_extension_for_tensorflow as itex
+  itex.experimental_ops_override()
+except:
+  pass

 @tf.keras.utils.register_keras_serializable(package='Text')
 class TransformerEncoder(tf.keras.Model):
@@ -129,18 +134,25 @@ class TransformerEncoder(tf.keras.Model):

     embeddings = tf.keras.layers.Add()(
         [word_embeddings, position_embeddings, type_embeddings])
-    embeddings = (
+    if(float_dtype == 'float16'):
+      embeddings = (
         tf.keras.layers.LayerNormalization(
             name='embeddings/layer_norm',
             axis=-1,
             epsilon=1e-12,
             dtype=tf.float32)(embeddings))
-    embeddings = (
-        tf.keras.layers.Dropout(rate=dropout_rate,
-                                dtype=tf.float32)(embeddings))
-
-    if float_dtype == 'float16':
+      embeddings = (
+          tf.keras.layers.Dropout(rate=dropout_rate,
+                                  dtype=tf.float32)(embeddings))
       embeddings = tf.cast(embeddings, tf.float16)
+    else:
+      embeddings = (
+          tf.keras.layers.LayerNormalization(
+              name='embeddings/layer_norm',
+              axis=-1,
+              epsilon=1e-12)(tf.reshape(embeddings,(-1, hidden_size))))
+      embeddings = (
+          tf.keras.layers.Dropout(rate=dropout_rate)(tf.reshape(embeddings, (-1 , sequence_length, hidden_size))))

     data = embeddings
     attention_mask = layers.SelfAttentionMask()([data, mask])
diff --git a/TensorFlow2/LanguageModeling/BERT/official/utils/flags/flags_test.py b/TensorFlow2/LanguageModeling/BERT/official/utils/flags/flags_test.py
index e11a1642..0ecb5921 100644
--- a/TensorFlow2/LanguageModeling/BERT/official/utils/flags/flags_test.py
+++ b/TensorFlow2/LanguageModeling/BERT/official/utils/flags/flags_test.py
@@ -90,6 +90,9 @@ class BaseTester(unittest.TestCase):
     self.assertEqual(flags_core.get_loss_scale(flags.FLAGS,
                                                default_for_fp16=2), 2)

+    flags_core.parse_flags([__file__, "--dtype", "bf16"])
+    self.assertEqual(flags_core.get_tf_dtype(flags.FLAGS), tf.bfloat16)
+
     flags_core.parse_flags(
         [__file__, "--dtype", "fp16", "--loss_scale", "5"])
     self.assertEqual(flags_core.get_loss_scale(flags.FLAGS,
diff --git a/TensorFlow2/LanguageModeling/BERT/optimization.py b/TensorFlow2/LanguageModeling/BERT/optimization.py
index e2b75b08..e90ac768 100644
--- a/TensorFlow2/LanguageModeling/BERT/optimization.py
+++ b/TensorFlow2/LanguageModeling/BERT/optimization.py
@@ -108,7 +108,7 @@ def create_optimizer(init_lr, num_train_steps, num_warmup_steps, optimizer_type=
   return optimizer


-class AdamWeightDecay(tf.keras.optimizers.Adam):
+class AdamWeightDecay(tf.keras.optimizers.legacy.Adam):
   """Adam enables L2 weight decay and clip_by_global_norm on gradients.
   Just adding the square of the weights to the loss function is *not* the
   correct way of using L2 regularization/weight decay with Adam, since that will
diff --git a/TensorFlow2/LanguageModeling/BERT/run_pretraining.py b/TensorFlow2/LanguageModeling/BERT/run_pretraining.py
index a55757b9..15ba78b5 100644
--- a/TensorFlow2/LanguageModeling/BERT/run_pretraining.py
+++ b/TensorFlow2/LanguageModeling/BERT/run_pretraining.py
@@ -32,7 +32,7 @@ import model_saving_utils
 from official.modeling import model_training_utils
 from official.nlp import bert_modeling as modeling
 import optimization
-import gpu_affinity
+# import gpu_affinity
 import dllogger_class
 from official.utils.misc import distribution_utils
 from official.utils.misc import keras_utils
@@ -109,8 +109,15 @@ def run_customized_training(strategy,

   def _get_pretrain_model():
     """Gets a pretraining model."""
-    pretrain_model, core_model = bert_models.pretrain_model(
-        bert_config, max_seq_length, max_predictions_per_seq, float_type=tf.float16 if FLAGS.use_fp16 else tf.float32)
+    if FLAGS.use_bf16:
+      pretrain_model, core_model = bert_models.pretrain_model(
+          bert_config, max_seq_length, max_predictions_per_seq, float_type=tf.bfloat16)
+    elif FLAGS.use_fp16:
+      pretrain_model, core_model = bert_models.pretrain_model(
+          bert_config, max_seq_length, max_predictions_per_seq, float_type=tf.float16)
+    else:
+      pretrain_model, core_model = bert_models.pretrain_model(
+          bert_config, max_seq_length, max_predictions_per_seq, float_type=tf.float32)
     pretrain_model.optimizer = optimization.create_optimizer(
         initial_lr, steps_per_epoch * epochs, warmup_steps, FLAGS.optimizer_type)
     if FLAGS.use_fp16:
@@ -176,7 +183,7 @@ def main(_):
   if not FLAGS.model_dir:
     FLAGS.model_dir = '/tmp/bert20/'

-  gpus = tf.config.experimental.list_physical_devices('GPU')
+  gpus = tf.config.experimental.list_physical_devices('XPU')
   for gpu in gpus:
     tf.config.experimental.set_memory_growth(gpu, True)

@@ -193,12 +200,16 @@ def main(_):

     hvd.init()
     if gpus:
-      tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')
-      gpu_affinity.set_affinity(hvd.local_rank())
-
-  if FLAGS.use_fp16:
-    policy = tf.keras.mixed_precision.experimental.Policy("mixed_float16")
-    tf.keras.mixed_precision.experimental.set_policy(policy)
+      tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'XPU')
+      # TODO(Maozhou)
+      # gpu_affinity.set_affinity(hvd.local_rank())
+
+  if FLAGS.use_bf16:
+    policy = tf.keras.mixed_precision.Policy("mixed_bfloat16")
+    tf.keras.mixed_precision.set_global_policy(policy)
+  elif FLAGS.use_fp16:
+    policy = tf.keras.mixed_precision.Policy("mixed_float16")
+    tf.keras.mixed_precision.set_global_policy(policy)

   run_bert_pretrain(strategy)

diff --git a/TensorFlow2/LanguageModeling/BERT/run_squad.py b/TensorFlow2/LanguageModeling/BERT/run_squad.py
index 87f5f019..a78d130b 100644
--- a/TensorFlow2/LanguageModeling/BERT/run_squad.py
+++ b/TensorFlow2/LanguageModeling/BERT/run_squad.py
@@ -46,7 +46,7 @@ import squad_lib as squad_lib_wp
 # sentence-piece tokenizer based squad_lib
 import squad_lib_sp
 import tokenization
-import gpu_affinity
+# import gpu_affinity
 import tf_trt
 from official.utils.misc import distribution_utils
 from official.utils.misc import keras_utils
@@ -208,8 +208,15 @@ def predict_squad_customized(strategy, input_meta_data, bert_config,

   else:
     with distribution_utils.get_strategy_scope(strategy):
-      squad_model, _ = bert_models.squad_model(
-          bert_config, input_meta_data['max_seq_length'], float_type=tf.float16 if FLAGS.use_fp16 else tf.float32)
+      if FLAGS.use_bf16:
+        squad_model, _ = bert_models.squad_model(
+          bert_config, input_meta_data['max_seq_length'], float_type=tf.bfloat16)
+      elif FLAGS.use_fp16:
+        squad_model, _ = bert_models.squad_model(
+          bert_config, input_meta_data['max_seq_length'], float_type=tf.float16)
+      else:
+        squad_model, _ = bert_models.squad_model(
+          bert_config, input_meta_data['max_seq_length'], float_type=tf.float32)

     if FLAGS.init_checkpoint:
       checkpoint = tf.train.Checkpoint(model=squad_model)
@@ -278,7 +285,10 @@ def predict_squad_customized(strategy, input_meta_data, bert_config,
   logging.info("Summary Inference Statistics")
   logging.info("Batch size = %d", FLAGS.predict_batch_size)
   logging.info("Sequence Length = %d", input_meta_data['max_seq_length'])
-  logging.info("Precision = %s", "fp16" if FLAGS.use_fp16 else "fp32")
+  if FLAGS.use_bf16:
+    logging.info("Precision = bf16")
+  else:
+    logging.info("Precision = %s", "fp16" if FLAGS.use_fp16 else "fp32")
   logging.info("Total Inference Time = %0.2f for Sentences = %d", eval_time_elapsed,
     num_steps * FLAGS.predict_batch_size)

@@ -326,7 +336,11 @@ def train_squad(strategy,

   use_float16 = common_flags.use_float16()
   if use_float16:
-    tf.keras.mixed_precision.experimental.set_policy('mixed_float16')
+    tf.keras.mixed_precision.set_global_policy('mixed_float16')
+
+  use_bfloat16 = common_flags.use_bfloat16()
+  if use_bfloat16:
+    tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')

   bert_config = MODEL_CLASSES[FLAGS.model_type][0].from_json_file(
       FLAGS.bert_config_file)
@@ -351,10 +365,23 @@ def train_squad(strategy,

   def _get_squad_model():
     """Get Squad model and optimizer."""
-    squad_model, core_model = bert_models.squad_model(
+    if FLAGS.use_bf16:
+      squad_model, core_model = bert_models.squad_model(
+        bert_config,
+        max_seq_length,
+        float_type=tf.bfloat16,
+        hub_module_url=FLAGS.hub_module_url)
+    elif FLAGS.use_fp16:
+      squad_model, core_model = bert_models.squad_model(
         bert_config,
         max_seq_length,
-        float_type=tf.float16 if FLAGS.use_fp16 else tf.float32,
+        float_type=tf.float16,
+        hub_module_url=FLAGS.hub_module_url)
+    else:
+      squad_model, core_model = bert_models.squad_model(
+        bert_config,
+        max_seq_length,
+        float_type=tf.float32,
         hub_module_url=FLAGS.hub_module_url)
     learning_rate = FLAGS.learning_rate * hvd.size() if FLAGS.use_horovod else FLAGS.learning_rate
     squad_model.optimizer = optimization.create_optimizer(
@@ -591,7 +618,7 @@ dynamic_batching {{
     "max_batch_size": max_batch_size,
     "seq_length": input_meta_data['max_seq_length'],
     "dynamic_batching": batching_str,
-    "gpu_list": ", ".join([x.name.split(":")[-1] for x in tf.config.list_physical_devices('GPU')]),
+    "gpu_list": ", ".join([x.name.split(":")[-1] for x in tf.config.list_physical_devices('XPU')]),
     "engine_count": FLAGS.triton_engine_count
   }

@@ -612,7 +639,7 @@ def main(_):
     export_squad(FLAGS.model_export_path, input_meta_data)
     return

-  gpus = tf.config.experimental.list_physical_devices('GPU')
+  gpus = tf.config.experimental.list_physical_devices('XPU')
   for gpu in gpus:
     tf.config.experimental.set_memory_growth(gpu, True)

@@ -627,12 +654,15 @@ def main(_):

     hvd.init()
     if gpus:
-      tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')
-      gpu_affinity.set_affinity(hvd.local_rank())
-
-  if FLAGS.use_fp16:
-    policy = tf.keras.mixed_precision.experimental.Policy("mixed_float16")
-    tf.keras.mixed_precision.experimental.set_policy(policy)
+      tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'XPU')
+      # gpu_affinity.set_affinity(hvd.local_rank())
+
+  if FLAGS.use_bf16:
+    policy = tf.keras.mixed_precision.Policy("mixed_bfloat16")
+    tf.keras.mixed_precision.set_global_policy(policy)
+  elif FLAGS.use_fp16:
+    policy = tf.keras.mixed_precision.Policy("mixed_float16")
+    tf.keras.mixed_precision.set_global_policy(policy)

   os.makedirs(FLAGS.model_dir, exist_ok=True)
   dllogging = dllogger_class.dllogger_class(FLAGS.dllog_path)
diff --git a/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_adam.sh b/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_adam.sh
index df886b92..fc5d57ab 100755
--- a/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_adam.sh
+++ b/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_adam.sh
@@ -29,7 +29,7 @@ num_accumulation_steps=${9:-1}
 seq_len=${10:-512}
 max_pred_per_seq=${11:-80}

-DATA_DIR=data/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus
+DATA_DIR=data/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/wikicorpus_en

 if [ "$bert_model" = "large" ] ; then
     export BERT_CONFIG=data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/bert_config.json
@@ -40,6 +40,8 @@ fi
 PREC=""
 if [ "$precision" = "fp16" ] ; then
    PREC="--use_fp16"
+elif [ "$precision" = "bf16" ] ; then
+   PREC="--use_bf16"
 elif [ "$precision" = "fp32" ] || [ "$precision" = "tf32" ] ; then
    PREC=""
 else
@@ -57,7 +59,7 @@ printf -v TAG "tf_bert_pretraining_adam_%s_%s_gbs%d" "$bert_model" "$precision"
 DATESTAMP=`date +'%y%m%d%H%M%S'`

 #Edit to save logs & checkpoints in a different directory
-RESULTS_DIR=${RESULTS_DIR:-/results/${TAG}_${DATESTAMP}}
+RESULTS_DIR=${RESULTS_DIR:-./results/${TAG}_${DATESTAMP}}
 LOGFILE=$RESULTS_DIR/$TAG.$DATESTAMP.log
 mkdir -m 777 -p $RESULTS_DIR
 printf "Saving checkpoints to %s\n" "$RESULTS_DIR"
diff --git a/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb.sh b/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb.sh
index 29b0b264..a0072342 100755
--- a/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb.sh
+++ b/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb.sh
@@ -22,7 +22,7 @@ train_batch_size_phase2=${2:-10}
 eval_batch_size=${3:-8}
 learning_rate_phase1=${4:-"7.5e-4"}
 learning_rate_phase2=${5:-"5e-4"}
-precision=${6:-"fp16"}
+precision=${6:-"fp16"}  # fp16 bf16
 use_xla=${7:-"true"}
 num_gpus=${8:-8}
 warmup_steps_phase1=${9:-"2133"}
@@ -33,7 +33,7 @@ num_accumulation_steps_phase1=${13:-128}
 num_accumulation_steps_phase2=${14:-384}
 bert_model=${15:-"large"}

-DATA_DIR=data
+DATA_DIR=${16:-./data}
 export DATA_DIR=$DATA_DIR

 GBS1=$(expr $train_batch_size_phase1 \* $num_gpus \* $num_accumulation_steps_phase1)
@@ -42,7 +42,7 @@ printf -v TAG "tf_bert_pretraining_lamb_%s_%s_gbs1%d_gbs2%d" "$bert_model" "$pre
 DATESTAMP=`date +'%y%m%d%H%M%S'`

 #Edit to save logs & checkpoints in a different directory
-RESULTS_DIR=${RESULTS_DIR:-/results/${TAG}_${DATESTAMP}}
+RESULTS_DIR=${17:-./results/${TAG}}
 LOGFILE=$RESULTS_DIR/$TAG.$DATESTAMP.log
 mkdir -m 777 -p $RESULTS_DIR
 printf "Saving checkpoints to %s\n" "$RESULTS_DIR"
diff --git a/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb_phase1.sh b/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb_phase1.sh
index 2fcd54e5..1bb5d5a8 100755
--- a/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb_phase1.sh
+++ b/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb_phase1.sh
@@ -33,19 +33,21 @@ num_accumulation_steps_phase1=${13:-128}
 num_accumulation_steps_phase2=${14:-384}
 bert_model=${15:-"large"}

-DATA_DIR=${DATA_DIR:-data}
+DATA_DIR=${DATA_DIR:-./data}
 #Edit to save logs & checkpoints in a different directory
-RESULTS_DIR=${RESULTS_DIR:-/results}
+RESULTS_DIR=${RESULTS_DIR:-./results}

 if [ "$bert_model" = "large" ] ; then
-    export BERT_CONFIG=data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/bert_config.json
+    export BERT_CONFIG=$DATA_DIR/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/bert_config.json
 else
-    export BERT_CONFIG=data/download/google_pretrained_weights/uncased_L-12_H-768_A-12/bert_config.json
+    export BERT_CONFIG=$DATA_DIR/download/google_pretrained_weights/uncased_L-12_H-768_A-12/bert_config.json
 fi

 PREC=""
 if [ "$precision" = "fp16" ] ; then
    PREC="--use_fp16"
+elif [ "$precision" = "bf16" ]; then
+   PREC="--use_bf16"
 elif [ "$precision" = "fp32" ] || [ "$precision" = "tf32" ] ; then
    PREC=""
 else
@@ -60,21 +62,21 @@ fi

 mpi=""
 if [ $num_gpus -gt 1 ] ; then
-   mpi="mpiexec --allow-run-as-root -np $num_gpus"
+   mpi="mpiexec -np $num_gpus -l"
    horovod="--use_horovod"
 fi

 #PHASE 1

-train_steps_phase1=$(expr $train_steps \* 9 \/ 10) #Phase 1 is 10% of training
+train_steps_phase1=$(expr $train_steps \* 9 \/ 10) #Phase 1 is 90% of training
 gbs_phase1=$(expr $train_batch_size_phase1 \* $num_accumulation_steps_phase1)
 seq_len=128
 max_pred_per_seq=20
 RESULTS_DIR_PHASE1=${RESULTS_DIR}/phase_1
 mkdir -m 777 -p $RESULTS_DIR_PHASE1

-INPUT_FILES="$DATA_DIR/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus/training/*"
-EVAL_FILES="$DATA_DIR/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus/test"
+INPUT_FILES="$DATA_DIR/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/wikicorpus_en/training/*"
+EVAL_FILES="$DATA_DIR/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/wikicorpus_en/test"

 #Check if all necessary files are available before training
 for DIR_or_file in $DATA_DIR $RESULTS_DIR_PHASE1 $BERT_CONFIG; do
@@ -84,7 +86,7 @@ for DIR_or_file in $DATA_DIR $RESULTS_DIR_PHASE1 $BERT_CONFIG; do
   fi
 done

- $mpi python /workspace/bert_tf2/run_pretraining.py \
+ $mpi python run_pretraining.py \
      --input_files=$INPUT_FILES \
      --model_dir=$RESULTS_DIR_PHASE1 \
      --bert_config_file=$BERT_CONFIG \
diff --git a/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb_phase2.sh b/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb_phase2.sh
index db279129..a16683bf 100755
--- a/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb_phase2.sh
+++ b/TensorFlow2/LanguageModeling/BERT/scripts/run_pretraining_lamb_phase2.sh
@@ -33,14 +33,15 @@ num_accumulation_steps_phase1=${13:-128}
 num_accumulation_steps_phase2=${14:-384}
 bert_model=${15:-"large"}

-DATA_DIR=${DATA_DIR:-data}
+DATA_DIR=${16:-./data}
 #Edit to save logs & checkpoints in a different directory
-RESULTS_DIR=${RESULTS_DIR:-/results}
+RESULTS_DIR=${17:-./results}

 if [ "$bert_model" = "large" ] ; then
-    export BERT_CONFIG=data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/bert_config.json
+   #  export BERT_CONFIG=$DATA_DIR/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/bert_config.json
+    export BERT_CONFIG=$DATA_DIR/uncased_L-24_H-1024_A-16/bert_config.json
 else
-    export BERT_CONFIG=data/download/google_pretrained_weights/uncased_L-12_H-768_A-12/bert_config.json
+    export BERT_CONFIG=$DATA_DIR/download/google_pretrained_weights/uncased_L-12_H-768_A-12/bert_config.json
 fi

 echo "Container nvidia build = " $NVIDIA_BUILD_ID
@@ -48,6 +49,8 @@ echo "Container nvidia build = " $NVIDIA_BUILD_ID
 PREC=""
 if [ "$precision" = "fp16" ] ; then
    PREC="--use_fp16"
+elif [ "$precision" = "bf16" ]; then
+   PREC="--use_bf16"
 elif [ "$precision" = "fp32" ] || [ "$precision" = "tf32" ] ; then
    PREC=""
 else
@@ -62,13 +65,13 @@ fi

 mpi=""
 if [ $num_gpus -gt 1 ] ; then
-   mpi="mpiexec --allow-run-as-root -np $num_gpus"
+   mpi="mpiexec -np $num_gpus -l"
    horovod="--use_horovod"
 fi

 #PHASE 1 Config

-train_steps_phase1=$(expr $train_steps \* 9 \/ 10) #Phase 1 is 10% of training
+train_steps_phase1=$(expr $train_steps \* 9 \/ 10) #Phase 1 is 90% of training
 gbs_phase1=$(expr $train_batch_size_phase1 \* $num_accumulation_steps_phase1)
 PHASE1_CKPT=${RESULTS_DIR}/phase_1/pretrained/bert_model.ckpt-1

@@ -80,15 +83,16 @@ train_steps_phase2=$(expr $train_steps \* 1 \/ 10) #Phase 2 is 10% of training
 gbs_phase2=$(expr $train_batch_size_phase2 \* $num_accumulation_steps_phase2)
 train_steps_phase2=$(expr $train_steps_phase2 \* $gbs_phase1 \/ $gbs_phase2) # Adjust for batch size

-RESULTS_DIR_PHASE2=${RESULTS_DIR}/phase_2
+RESULTS_DIR_PHASE2=${RESULTS_DIR}
 mkdir -m 777 -p $RESULTS_DIR_PHASE2

-INPUT_FILES="$DATA_DIR/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus/training/*"
-EVAL_FILES="$DATA_DIR/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus/test"
+# INPUT_FILES="$DATA_DIR/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/wikicorpus_en/training/*"
+# EVAL_FILES="$DATA_DIR/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/wikicorpus_en/test"
+INPUT_FILES="$DATA_DIR/training/*"
+EVAL_FILES="$DATA_DIR/test"

-$mpi python /workspace/bert_tf2/run_pretraining.py \
+$mpi python run_pretraining.py \
     --input_files=$INPUT_FILES \
-    --init_checkpoint=$PHASE1_CKPT \
     --model_dir=$RESULTS_DIR_PHASE2 \
     --bert_config_file=$BERT_CONFIG \
     --train_batch_size=$train_batch_size_phase2 \
@@ -103,3 +107,5 @@ $mpi python /workspace/bert_tf2/run_pretraining.py \
     --optimizer_type=LAMB \
     $horovod $PREC

+   #  --init_checkpoint=$PHASE1_CKPT \
+
diff --git a/TensorFlow2/LanguageModeling/BERT/scripts/run_squad.sh b/TensorFlow2/LanguageModeling/BERT/scripts/run_squad.sh
index 47cce05a..6e20b81f 100755
--- a/TensorFlow2/LanguageModeling/BERT/scripts/run_squad.sh
+++ b/TensorFlow2/LanguageModeling/BERT/scripts/run_squad.sh
@@ -25,6 +25,9 @@ use_xla=${5:-"true"}
 bert_model=${6:-"large"}
 squad_version=${7:-"1.1"}
 epochs=${8:-"2"}
+use_mytrain=${9:-"false"}
+pretrain_path=${10:-"data/results/tf_bert_pretraining_lamb_large_bf16_gbs1_3840_gbs2_1920/phase_2/pretrained/bert_model.ckpt-1"}
+DATA_DIR=${11:-data}

 if [ $num_gpu -gt 1 ] ; then
     mpi_command="mpirun -np $num_gpu \
@@ -41,6 +44,9 @@ fi
 if [ "$precision" = "fp16" ] ; then
     echo "fp16 activated!"
     use_fp16="--use_fp16"
+elif [ "$precision" = "bf16" ]; then
+    echo "bf16 activated!"
+    use_fp16="--use_bf16"
 else
     use_fp16=""
 fi
@@ -53,20 +59,26 @@ else
 fi

 if [ "$bert_model" = "large" ] ; then
-    export BERT_BASE_DIR=data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16
+    export BERT_BASE_DIR=$DATA_DIR/download/google_pretrained_weights/uncased_L-24_H-1024_A-16
 else
-    export BERT_BASE_DIR=data/download/google_pretrained_weights/uncased_L-12_H-768_A-12
+    export BERT_BASE_DIR=$DATA_DIR/download/google_pretrained_weights/uncased_L-12_H-768_A-12
+fi
+
+if [ "$use_mytrain" = "false" ] ; then
+    export PRETRAIN_FILE=$BERT_BASE_DIR/bert_model.ckpt
+else
+    export PRETRAIN_FILE=$pretrain_path
 fi

 export SQUAD_VERSION=v$squad_version
-export SQUAD_DIR=data/download/squad/$SQUAD_VERSION
+export SQUAD_DIR=$DATA_DIR/download/squad/$SQUAD_VERSION

 export GBS=$(expr $batch_size \* $num_gpu)
 printf -v TAG "tf_bert_finetuning_squad_%s_%s_gbs%d" "$bert_model" "$precision" $GBS
 DATESTAMP=`date +'%y%m%d%H%M%S'`

 #Edit to save logs & checkpoints in a different directory
-RESULTS_DIR=/results/${TAG}_${DATESTAMP}
+RESULTS_DIR=${12:-./results/${TAG}}
 LOGFILE=$RESULTS_DIR/$TAG.$DATESTAMP.log
 mkdir -m 777 -p $RESULTS_DIR
 printf "Saving checkpoints to %s\n" "$RESULTS_DIR"
@@ -80,7 +92,7 @@ $mpi_command python run_squad.py \
   --predict_file=${SQUAD_DIR}/dev-${SQUAD_VERSION}.json \
   --vocab_file=${BERT_BASE_DIR}/vocab.txt \
   --bert_config_file=$BERT_BASE_DIR/bert_config.json \
-  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
+  --init_checkpoint=$PRETRAIN_FILE \
   --train_batch_size=$batch_size \
   --learning_rate=$learning_rate \
   --num_train_epochs=$epochs \
--
2.34.1