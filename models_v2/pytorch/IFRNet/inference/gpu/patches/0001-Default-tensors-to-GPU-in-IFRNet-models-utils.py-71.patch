From 1010dc728a07c74e2650588bfea32d39cb91ee66 Mon Sep 17 00:00:00 2001
From: Tony Ye <tony.ye@intel.com>
Date: Tue, 12 Dec 2023 07:25:22 -0800
Subject: [PATCH] Default tensors to GPU in IFRNet/models/utils.py (#71)

The new tensors created in utils.py are defaulting to CPU. This makes
the concatenation op in warp() being executed on CPU, which causes high
CPU utilization and low performance and even worse with multi-processes.

Co-authored-by: Phoong, Stanley Cheong Kwan <stanley.cheong.kwan.phoong@intel.com>
Co-authored-by: varistar <vasily.aristarkhov@intel.com>
---
 samples/pytorch/IFRNet/models/loss.py  | 18 ++++++++++--------
 samples/pytorch/IFRNet/models/utils.py |  5 +++++
 2 files changed, 15 insertions(+), 8 deletions(-)

diff --git a/samples/pytorch/IFRNet/models/loss.py b/samples/pytorch/IFRNet/models/loss.py
index fca04ae..b84da76 100644
--- a/samples/pytorch/IFRNet/models/loss.py
+++ b/samples/pytorch/IFRNet/models/loss.py
@@ -3,17 +3,19 @@ import torch.nn as nn
 import torch.nn.functional as F
 import numpy as np
 
-device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
-
+device = torch.device(
+        'xpu' if hasattr(torch, 'xpu') and torch.xpu.is_available()
+        else 'cuda' if hasattr(torch, 'cuda') and torch.cuda.is_available()
+        else 'cpu')
 
 class Ternary(nn.Module):
     def __init__(self, patch_size=7):
         super(Ternary, self).__init__()
         self.patch_size = patch_size
         out_channels = patch_size * patch_size
-        self.w = np.eye(out_channels).reshape((patch_size, patch_size, 1, out_channels))
-        self.w = np.transpose(self.w, (3, 2, 0, 1))
-        self.w = torch.tensor(self.w).float().to(device)
+        self.w = torch.eye(out_channels, device=device).view(patch_size, patch_size, 1, out_channels)
+        self.w = self.w.permute(3, 2, 0, 1)
+        self.w = self.w.float().to(device)
 
     def transform(self, tensor):
         tensor_ = tensor.mean(dim=1, keepdim=True)
@@ -44,9 +46,9 @@ class Geometry(nn.Module):
         super(Geometry, self).__init__()
         self.patch_size = patch_size
         out_channels = patch_size * patch_size
-        self.w = np.eye(out_channels).reshape((patch_size, patch_size, 1, out_channels))
-        self.w = np.transpose(self.w, (3, 2, 0, 1))
-        self.w = torch.tensor(self.w).float().to(device)
+        self.w = torch.eye(out_channels, device=device).view(patch_size, patch_size, 1, out_channels)
+        self.w = self.w.permute(3, 2, 0, 1)
+        self.w = self.w.float().to(device)
 
     def transform(self, tensor):
         b, c, h, w = tensor.size()
diff --git a/samples/pytorch/IFRNet/models/utils.py b/samples/pytorch/IFRNet/models/utils.py
index 6feb5f2..9cc0418 100644
--- a/samples/pytorch/IFRNet/models/utils.py
+++ b/samples/pytorch/IFRNet/models/utils.py
@@ -10,6 +10,11 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
+device = torch.device(
+        'xpu' if hasattr(torch, 'xpu') and torch.xpu.is_available()
+        else 'cuda' if hasattr(torch, 'cuda') and torch.cuda.is_available()
+        else 'cpu')
+torch.set_default_device(device)
 
 def warp(img, flow):
     B, _, H, W = flow.shape
-- 
2.34.1

