diff --git a/examples/legacy/question-answering/run_squad.py b/examples/legacy/question-answering/run_squad.py
index fc9411e95..cdc3cdbeb 100644
--- a/examples/legacy/question-answering/run_squad.py
+++ b/examples/legacy/question-answering/run_squad.py
@@ -22,6 +22,9 @@ import logging
 import os
 import random
 import timeit
+import time
+import sys
+import threading
 
 import numpy as np
 import torch
@@ -48,7 +51,29 @@ from transformers.data.metrics.squad_metrics import (
 from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor
 from transformers.trainer_utils import is_main_process
 
-
+def trace_handler(prof):
+    print(prof.key_averages().table(
+        sort_by="self_cpu_time_total", row_limit=10))
+    import datetime
+    now = datetime.datetime.now()
+    log_path = os.path.join(os.getcwd(), "bert_profiling_{}_step_{}.json".format(now.strftime("%Y%m%d%H%M%S"), str(prof.step_num)))
+    prof.export_chrome_trace(log_path)
+profile_ctx = torch.profiler.profile(
+        activities=[
+            torch.profiler.ProfilerActivity.CPU,
+        ],
+        schedule=torch.profiler.schedule(
+            wait=0,
+            warmup=20,
+            active=20,
+            repeat=1),
+        on_trace_ready=trace_handler,
+        record_shapes=True,
+        profile_memory=True,
+        with_stack=True,
+        with_flops=True,
+        with_modules=True
+    )
 try:
     from torch.utils.tensorboard import SummaryWriter
 except ImportError:
@@ -265,6 +290,217 @@ def train(args, train_dataset, model, tokenizer):
 
     return global_step, tr_loss / global_step
 
+def wrap_model(model, args, eval_dataloader):
+    for it, batch in enumerate(eval_dataloader):
+        break
+    torch_compile_inputs = {
+        "input_ids": batch[0],
+        "attention_mask": batch[1],
+        "token_type_ids": batch[2],
+    }
+    model.eval()
+    if args.ipex:
+        ipex.nn.utils._model_convert.replace_dropout_with_identity(model)
+    else:
+        torch._C._jit_set_texpr_fuser_enabled(False)
+    dumpy_tensor = torch.ones((args.eval_batch_size, 384), dtype=torch.long) \
+                    if not args.use_multi_stream_module \
+                    else torch.ones((args.eval_batch_size//args.num_streams, 384), dtype=torch.long)
+    jit_inputs = (dumpy_tensor, dumpy_tensor, dumpy_tensor)
+    print(args)
+    if args.int8 and args.ipex:
+        from intel_extension_for_pytorch.quantization import prepare, convert
+        from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig
+        qconfig = QConfig(activation=MinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.quint8), weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric))
+        prepared_model = prepare(model, qconfig, example_inputs=jit_inputs, inplace=False)
+        prepared_model.load_qconf_summary(qconf_summary = args.int8_config)
+
+        # convert model to trace model.
+        if args.int8_fp32:
+            model = convert(prepared_model)
+            model = torch.jit.trace(model, jit_inputs, strict=False)
+        elif args.int8_bf16:
+            with torch.cpu.amp.autocast():
+                model = convert(prepared_model)
+                model = torch.jit.trace(model, jit_inputs, strict=False)
+        model = torch.jit.freeze(model)
+        input = {
+                 "input_ids": dumpy_tensor,
+                 "attention_mask": dumpy_tensor,
+                 "token_type_ids": dumpy_tensor,
+         }
+        # enable fusion path work(need to run two interation).
+        with torch.no_grad():
+            y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor) 
+            #dumpy_tensor = torch.ones((128, 384), dtype=torch.long)
+            #y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            #dumpy_tensor = torch.ones((81, 384), dtype=torch.long)
+            #y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            #print("############################################")
+    elif args.bf16:
+        if args.ipex:
+            model = ipex.optimize(model, dtype=torch.bfloat16)
+        else:
+            torch._C._jit_set_autocast_mode(False)
+            model = model.to(torch.bfloat16)
+        with torch.cpu.amp.autocast(),torch.no_grad():
+            if args.use_jit:
+                model = torch.jit.trace(model, jit_inputs, strict=False)
+                model = torch.jit.freeze(model)
+                #model = torch.jit._recursive.wrap_cpp_module(torch._C._freeze_module(model._c, preserveParameters=True))
+                # print(model.graph)
+    elif args.fp16_cpu:
+        if args.ipex:
+            model = ipex.optimize(model, dtype=torch.half, conv_bn_folding=False, auto_kernel_selection=True, weights_prepack=True)
+        else:
+            torch._C._jit_set_autocast_mode(False)
+            model = model.to(torch.half)
+        with torch.cpu.amp.autocast(enabled=True, dtype=torch.half):
+            if args.use_jit:
+                model = torch.jit.trace(model, jit_inputs, strict=False)
+                model = torch.jit.freeze(model)
+    elif args.bf32:
+        ipex.set_fp32_math_mode(mode=ipex.FP32MathMode.BF32, device="cpu")
+        model = ipex.optimize(model, dtype=torch.float32, level="O1", auto_kernel_selection=True)
+        with torch.no_grad():
+             model = torch.jit.trace(model, jit_inputs, strict=False)
+        model = torch.jit.freeze(model)
+    elif args.fp8:
+        model= prepare_fp8(model)
+        if os.path.exists(args.fp8_config):
+            model.load_state_dict(torch.load(args.fp8_config))
+    elif args.use_jit: # fp32
+        model = ipex.optimize(model.eval(), dtype=torch.float32)# auto_kernel_selection=True)
+        with torch.no_grad():
+            model = torch.jit.trace(model, jit_inputs, strict=False)
+            model = torch.jit.freeze(model)
+    # torch.compile() path
+    if args.inductor:
+        from torch._inductor import config as inductor_config
+        inductor_config.cpp_wrapper = True
+        if args.int8:
+            from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e
+            import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq
+            from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer
+            from torch._export import capture_pre_autograd_graph
+            print('[Info] Running torch.compile() for INT8 quantization')
+            print('[Info] Running int8_bf16 is: {}'.format(args.int8_bf16))
+            with torch.no_grad():
+                dynamic_shapes = None
+                if args.eval_batch_size != 1:
+                    dynamic_shapes = {
+                        "input_ids": {0: torch.export.Dim("dim", max=1024 * 1024)},
+                        "attention_mask": {0: torch.export.Dim("dim", max=1024 * 1024)},
+                        "token_type_ids": {0: torch.export.Dim("dim", max=1024 * 1024)}
+                    }                    
+                exported_model = capture_pre_autograd_graph(
+                    model,
+                    (),
+                    kwargs=torch_compile_inputs,
+                    dynamic_shapes=dynamic_shapes,
+                )
+                quantizer = X86InductorQuantizer()
+                quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())
+                prepared_model = prepare_pt2e(exported_model, quantizer)
+                prepared_model(**torch_compile_inputs)
+                converted_model = convert_pt2e(prepared_model)
+                torch.ao.quantization.move_exported_model_to_eval(converted_model)
+                with torch.cpu.amp.autocast(enabled=args.int8_bf16):
+                    if args.ipex:
+                        model = torch.compile(converted_model, backend="ipex")
+                    else:
+                        model = torch.compile(converted_model, dynamic=(args.eval_batch_size != 1))
+                    model(**torch_compile_inputs)
+                    model(**torch_compile_inputs)
+        else:
+            with torch.no_grad(), torch.cpu.amp.autocast(enabled=args.bf16 or args.fp16_cpu, dtype=torch.half if args.fp16_cpu else torch.bfloat16):
+                if args.ipex:
+                    print('[Info] Running torch.compile() with IPEX backend')
+                    model = torch.compile(model, backend="ipex")
+                else:
+                    print('[Info] Running torch.compile() with default backend')
+                    model = torch.compile(model)
+                # warmup run before threading to compile the model
+                model(**torch_compile_inputs)
+                model(**torch_compile_inputs)
+
+    if args.use_multi_stream_module:
+        print("Use multi stream module on numa node:{0}, num of streams:{1}, batch per stream:{2}".format(args.instance_number, args.num_streams, args.eval_batch_size//args.num_streams))
+        input_hint_object = {"input_ids": 0, "attention_mask": 0, "token_type_ids": 0}
+        multi_stream_input_hint = ipex.cpu.runtime.MultiStreamModuleHint(**input_hint_object)
+        multi_stream_output_hint = ipex.cpu.runtime.MultiStreamModuleHint((0, 0))
+        cpu_pool = ipex.cpu.runtime.CPUPool(node_id=args.instance_number)
+        model = ipex.cpu.runtime.MultiStreamModule(model,
+                                                num_streams=args.num_streams,
+                                                cpu_pool=cpu_pool,
+                                                input_split_hint = multi_stream_input_hint,
+                                                output_concat_hint = multi_stream_output_hint)
+    return model 
+
+def benchmark_evaluate(args, model, eval_dataloader):
+    steps_per_epoch = len(eval_dataloader)
+    total_steps = (args.perf_run_iters + args.perf_begin_iter)
+    test_epoches = int(total_steps / steps_per_epoch)
+    print('Evaluating BERT: Steps per Epoch {} total Steps {}'.format(steps_per_epoch, total_steps))
+    total_time = 0
+    i = 0
+    timeBuff = []
+    #with torch.profiler.profile(
+    #        activities=[
+    #            torch.profiler.ProfilerActivity.CPU],
+
+    #        schedule=torch.profiler.schedule(
+    #            wait=1,
+    #            warmup=9,
+    #            active=5),
+    #        #on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/bert_bf16'),#trace_handler
+    #        on_trace_ready=trace_handler#torch.profiler.tensorboard_trace_handler('./log/bert_bf16')
+    #        # used when outputting for tensorboard
+    #        ) as prof:
+
+    with tqdm(total=total_steps, desc="Evaluating") as pbar:
+        if args.profile:
+            prof = profile_ctx.__enter__()
+        for epoch in range(test_epoches + 1):
+            for it, batch in enumerate(eval_dataloader):
+                if epoch * steps_per_epoch + it >= total_steps:
+                    throughput = args.eval_batch_size * args.perf_run_iters / total_time
+                    timeBuff = np.asarray(timeBuff)
+                    p99 = np.percentile(timeBuff, 99)
+                    print('P99 Latency {:.2f} ms'.format(p99*1000))
+                    print("Throughput: {:.3f} sentence/s".format(throughput))     
+                    break
+                import contextlib
+                maybe_autocast = torch.cpu.amp.autocast(enabled=args.bf16 or args.int8_bf16 or args.fp16_cpu, dtype=torch.half if args.fp16_cpu else torch.bfloat16) if args.inductor else contextlib.nullcontext()
+                with torch.no_grad(), maybe_autocast:
+                    inputs = {
+                        "input_ids": batch[0],
+                        "attention_mask": batch[1],
+                        "token_type_ids": batch[2],
+                    }
+                    #print("---------------**inputs is:{}".format(**inputs))
+                    time_start = time.time()
+                    #print("inputs type is: {}".format(type(inputs)))
+                    #print("inputs is: {}".format(inputs))
+
+                    outputs = model(**inputs)          
+
+                    # print("outputs type is: {}".format(type(outputs)))
+                    # print("outputs len is: {}".format(len(outputs)))
+                    # print("output[0] size is: {}".format(outputs[0].size()))
+                    # print("output[1] size is: {}".format(outputs[1].size()))
+                    # print("outputs is: {}".format(outputs))
+                    if args.profile:
+                        prof.step()
+                    time_end = time.time()
+                    if epoch * steps_per_epoch + it > args.perf_begin_iter:
+                        total_time +=(time_end - time_start)
+                        timeBuff.append(time_end - time_start)
+                    pbar.update(1)
+        if args.profile:
+            profile_ctx.__exit__(None, None, None)
+
 
 def evaluate(args, model, tokenizer, prefix=""):
     dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)
@@ -278,6 +514,8 @@ def evaluate(args, model, tokenizer, prefix=""):
     eval_sampler = SequentialSampler(dataset)
     eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)
 
+    model = wrap_model(model, args, eval_dataloader)
+
     # multi-gpu evaluate
     if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):
         model = torch.nn.DataParallel(model)
@@ -286,12 +524,63 @@ def evaluate(args, model, tokenizer, prefix=""):
     logger.info("***** Running evaluation {} *****".format(prefix))
     logger.info("  Num examples = %d", len(dataset))
     logger.info("  Batch size = %d", args.eval_batch_size)
+    if args.do_calibration:
+        if args.fp8:
+            with fp8_autocast(enabled=False, calibrating=True, fp8_recipe=DelayedScaling(fp8_format=Format.E4M3, amax_history_len=1024)):
+                for step, batch in enumerate(eval_dataloader):
+                    print("calibration step: {}".format(step))
+                    batch = {
+                        "input_ids": batch[0],
+                        "attention_mask": batch[1],
+                        "token_type_ids": batch[2],
+                    }
+
+                    _ = model(**batch)
+                    if step == args.calibration_iters -1:
+                        torch.save(model.state_dict(), args.fp8_config)
+                        print("calibration finished, FP8 quantization info saved on file {}".format(args.fp8_config))
+                        exit()
+        else:
+            import intel_extension_for_pytorch as ipex
+            from intel_extension_for_pytorch.quantization import prepare
+            from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig
+            qconfig = QConfig(activation=MinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.quint8), weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric))
+            dumpy_tensor = torch.ones((args.eval_batch_size, 384), dtype=torch.long)
+            jit_inputs=(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            ipex.nn.utils._model_convert.replace_dropout_with_identity(model)
+            prepared_model = prepare(model, qconfig, example_inputs=jit_inputs, inplace=False)
+            for step, batch in enumerate(eval_dataloader):
+                print("calibration step: {}".format(step))
+                batch = {
+                    "input_ids": batch[0],
+                    "attention_mask": batch[1],
+                    "token_type_ids": batch[2],
+                }
+                prepared_model(**batch)
+                if step == args.calibration_iters -1:
+                    print("calibration finished, quantization info saved on file {}".format(args.int8_config))
+                    prepared_model.save_qconf_summary(qconf_summary = args.int8_config)
+                    exit()
+    if args.benchmark:
+        if args.use_share_weight:
+            threads = []
+            num_instances = args.total_cores // args.cores_per_instance
+            for i in range(0, num_instances):
+               t = threading.Thread(target=benchmark_evaluate, args=(args, model, eval_dataloader))
+               threads.append(t)
+               t.start()
+            for t in threads:
+                t.join()
+        else:
+            benchmark_evaluate(args, model, eval_dataloader)        
+        exit()
 
     all_results = []
     start_time = timeit.default_timer()
 
     for batch in tqdm(eval_dataloader, desc="Evaluating"):
-        model.eval()
+        if not (args.inductor and args.int8):
+            model.eval()
         batch = tuple(t.to(args.device) for t in batch)
 
         with torch.no_grad():
@@ -314,13 +603,20 @@ def evaluate(args, model, tokenizer, prefix=""):
                     inputs.update(
                         {"langs": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}
                     )
-            outputs = model(**inputs)
+            if args.inductor:
+                with torch.cpu.amp.autocast(enabled=args.bf16 or args.int8_bf16 or args.fp16_cpu, dtype=torch.half if args.fp16_cpu else torch.bfloat16):
+                    outputs = model(**inputs)
+            elif args.fp8:
+                with fp8_autocast(enabled=True, calibrating=False, fp8_recipe=DelayedScaling(fp8_format=Format.E4M3, amax_history_len=1024)):
+                    outputs = model(**inputs)
+            else:
+                outputs = model(**inputs)
 
         for i, feature_index in enumerate(feature_indices):
             eval_feature = features[feature_index.item()]
             unique_id = int(eval_feature.unique_id)
 
-            output = [to_list(output[i]) for output in outputs.to_tuple()]
+            output = [to_list(output[i]) for output in outputs]
 
             # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other "simpler"
             # models only use two.
@@ -671,7 +967,68 @@ def main():
     parser.add_argument("--server_port", type=str, default="", help="Can be used for distant debugging.")
 
     parser.add_argument("--threads", type=int, default=1, help="multiple threads for converting example to features")
+    parser.add_argument(
+        "--fp16_cpu",
+        action="store_true",
+        help="Whether to use fp16 16-bit (mixed) precision  instead of 32-bit on cpu")
+    parser.add_argument(
+        "--bf16",
+        action="store_true",
+        help="Whether to use 16-bit (mixed) precision  instead of 32-bit")
+    parser.add_argument("--perf_begin_iter", type=int, default=15,
+                        help="Number iterations to warm up")
+    parser.add_argument("--perf_run_iters", type=int, default=100,
+                        help="Number iterations to collection performance data begin from perf_begin_iter")
+    parser.add_argument("--iter_num", type=int, default=40,
+                        help="Number iterations to collect time")
+    parser.add_argument("--benchmark", action='store_true',
+                        help="Bench the model speed")
+    parser.add_argument("--bf32", action='store_true', help="For enabling IPEX bf32")
+    parser.add_argument("--use_jit", action='store_true', help="For jit trace")
+    parser.add_argument('--int8', dest='int8', action='store_true',
+                        help='use llga int8 in pytorch jit model')
+    parser.add_argument('--int8_fp32', dest='int8_fp32', action='store_true',
+                        help='use int8 fp32 mix precision')
+    parser.add_argument('--int8_bf16', dest='int8_bf16', action='store_true',
+                        help='use int8 bf16 mix precision')
+    parser.add_argument("--int8_config", type=str, default="config.json", 
+                        help="quantization config file for int8 mode")
+    parser.add_argument('--fp8', dest='fp8', action='store_true',
+                        help='use FP8')
+    parser.add_argument("--fp8_config", type=str, default="fp8_state_dict.pt",
+                        help="FP8 state file for FP8 mode")
+    parser.add_argument("--do_calibration", action='store_true',
+                        help="Enable calibration process")
+    parser.add_argument("--calibration_iters", type=int, default=100,
+                        help="Number iterations to do calibration")
+    parser.add_argument("--use_share_weight", action='store_true',
+                        help="Enable share weight mode")
+    parser.add_argument("--cores_per_instance", type=int, default=4,
+                        help="Number iterations to collect time")
+    parser.add_argument("--total_cores", type=int, default=28,
+                        help="Total cores used for this process, used for share_weight mode")
+    parser.add_argument('--use_multi_stream_module', dest='use_multi_stream_module', action='store_true',
+                        help='Whether use multi stream module for throughput mode')
+    parser.add_argument("--num_streams", type=int, default=1,
+                        help="The number of stream to use, used for multi stream module")
+    parser.add_argument("--instance_number", type=int, default=0,
+                        help="The socket to run multi stream module, used for multi stream module with multi sockets")
+    parser.add_argument("--ipex", action='store_true', default=False,
+                        help='use intel pytorch extension')
+    parser.add_argument('--inductor', action='store_true', default=False,
+                        help='using torch.compile() inductor backend')
+    parser.add_argument('--profile', action='store_true', default=False,
+                        help='profile')
     args = parser.parse_args()
+    if args.ipex:
+        import intel_extension_for_pytorch as ipex
+        from intel_extension_for_pytorch.quantization.fp8 import (
+            fp8_autocast,
+            DelayedScaling,
+            Format,
+            prepare_fp8,
+        )
+        global ipex, fp8_autocast, DelayedScaling, Format, prepare_fp8
 
     if args.doc_stride >= args.max_seq_length - args.max_query_length:
         logger.warning(
@@ -742,11 +1099,13 @@ def main():
     args.model_type = args.model_type.lower()
     config = AutoConfig.from_pretrained(
         args.config_name if args.config_name else args.model_name_or_path,
+        return_dict=False,
         cache_dir=args.cache_dir if args.cache_dir else None,
     )
     tokenizer = AutoTokenizer.from_pretrained(
         args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,
         do_lower_case=args.do_lower_case,
+        return_dict=False,
         cache_dir=args.cache_dir if args.cache_dir else None,
         use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling
     )
@@ -824,7 +1183,7 @@ def main():
         for checkpoint in checkpoints:
             # Reload the model
             global_step = checkpoint.split("-")[-1] if len(checkpoints) > 1 else ""
-            model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)  # , force_download=True)
+            model = AutoModelForQuestionAnswering.from_pretrained(checkpoint, config=config)  # , force_download=True)
             model.to(args.device)
 
             # Evaluate
diff --git a/examples/pytorch/image-classification/run_image_classification.py b/examples/pytorch/image-classification/run_image_classification.py
index e2ce9f1c5..e0666475c 100644
--- a/examples/pytorch/image-classification/run_image_classification.py
+++ b/examples/pytorch/image-classification/run_image_classification.py
@@ -230,6 +230,7 @@ def main():
             cache_dir=model_args.cache_dir,
             task="image-classification",
             use_auth_token=True if model_args.use_auth_token else None,
+            revision="014711311cec8b5959350c373878a3311caeb764",
         )
     else:
         data_files = {}
@@ -275,6 +276,7 @@ def main():
         id2label=id2label,
         finetuning_task="image-classification",
         cache_dir=model_args.cache_dir,
+        return_dict = False,
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
diff --git a/examples/pytorch/question-answering/run_qa.py b/examples/pytorch/question-answering/run_qa.py
index 9dc3b2c81..0c21b2dd9 100755
--- a/examples/pytorch/question-answering/run_qa.py
+++ b/examples/pytorch/question-answering/run_qa.py
@@ -322,6 +322,7 @@ def main():
         model_args.config_name if model_args.config_name else model_args.model_name_or_path,
         cache_dir=model_args.cache_dir,
         revision=model_args.model_revision,
+        return_dict=False,
         use_auth_token=True if model_args.use_auth_token else None,
     )
     tokenizer = AutoTokenizer.from_pretrained(
diff --git a/examples/pytorch/text-classification/run_glue.py b/examples/pytorch/text-classification/run_glue.py
index c14107d89..fdc7d03f6 100755
--- a/examples/pytorch/text-classification/run_glue.py
+++ b/examples/pytorch/text-classification/run_glue.py
@@ -360,6 +360,7 @@ def main():
         num_labels=num_labels,
         finetuning_task=data_args.task_name,
         cache_dir=model_args.cache_dir,
+        return_dict = False,
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
@@ -487,7 +488,12 @@ def main():
     if data_args.task_name is not None:
         metric = evaluate.load("glue", data_args.task_name)
     else:
-        metric = evaluate.load("accuracy")
+        #metric = evaluate.load("accuracy")
+        curpath = os.path.abspath(os.path.dirname(__file__))
+        curpath  = curpath.replace("/transformers/examples/pytorch/text-classification", '')
+        accuracy_path = os.path.join( curpath, "accuracy.py")
+        metric =  datasets.load_metric(accuracy_path)
+        #evaluate.load(accuracy_path)
 
     # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a
     # predictions and label_ids field) and has to return a dictionary string to float.
@@ -575,6 +581,7 @@ def main():
 
             trainer.log_metrics("eval", metrics)
             trainer.save_metrics("eval", combined if task is not None and "mnli" in task else metrics)
+        exit(0)
 
     if training_args.do_predict:
         logger.info("*** Predict ***")
diff --git a/src/transformers/activations.py b/src/transformers/activations.py
index 587dc2e59..b4e331e28 100644
--- a/src/transformers/activations.py
+++ b/src/transformers/activations.py
@@ -53,8 +53,7 @@ class NewGELUActivation(nn.Module):
     """
 
     def forward(self, input: Tensor) -> Tensor:
-        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
-
+        return nn.functional.gelu(input, approximate='tanh') 
 
 class GELUActivation(nn.Module):
     """
diff --git a/src/transformers/generation/utils.py b/src/transformers/generation/utils.py
index ae12ae293..e6a08d367 100644
--- a/src/transformers/generation/utils.py
+++ b/src/transformers/generation/utils.py
@@ -16,6 +16,8 @@
 
 import copy
 import inspect
+import re
+import time
 import warnings
 from dataclasses import dataclass
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union
@@ -700,6 +702,9 @@ class GenerationMixin:
 
     def _extract_past_from_model_output(self, outputs: ModelOutput, standardize_cache_format: bool = False):
         past_key_values = None
+        # To use torch.jit.trace, the output is no longer a Dict. outputs[1] corresponds to "past_key_values"
+        if self.jit == True:
+            past_key_values = outputs[1]
         if "past_key_values" in outputs:
             past_key_values = outputs.past_key_values
         elif "mems" in outputs:
@@ -1208,6 +1213,11 @@ class GenerationMixin:
 
         # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call
         self._validate_model_class()
+        self.jit = kwargs.pop("jit", False)
+        self.quantized_model_path = kwargs.pop("quantized_model_path", None)
+        self.ipex_int8 = kwargs.pop("ipex_int8", False)
+        self.tp_number = kwargs.pop("TP_number", 1)
+        self.token_latency = kwargs.pop("token_latency", None)
 
         # priority: `generation_config` argument > `model.generation_config` (the default generation config)
         if generation_config is None:
@@ -2186,6 +2196,7 @@ class GenerationMixin:
         ["It might be possible to get a better understanding of the nature of the problem, but it's not"]
         ```"""
         # init values
+        latency_list = []
         logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
         stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
         if max_length is not None:
@@ -2231,6 +2242,7 @@ class GenerationMixin:
 
         this_peer_finished = False  # used by synced_gpus only
         while True:
+            tic = time.time()
             if synced_gpus:
                 # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.
                 # The following logic allows an early break if all peers finished generating their sequence
@@ -2243,19 +2255,95 @@ class GenerationMixin:
 
             # prepare model inputs
             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
-
-            # forward pass to get next token
-            outputs = self(
-                **model_inputs,
-                return_dict=True,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
-            )
-
-            if synced_gpus and this_peer_finished:
-                continue  # don't waste resources running the code we don't need
-
-            next_token_logits = outputs.logits[:, -1, :]
+            if re.search("GPTJ", self.config.architectures[0]) or re.search("llama", self.config.architectures[0], re.IGNORECASE) or re.search("bloom", self.config.architectures[0], re.IGNORECASE) or re.search("chatglm", self.config.architectures[0], re.IGNORECASE):
+                if self.jit == False:
+                    outputs = self(
+                        **model_inputs,
+                        return_dict=True,
+                        output_attentions=output_attentions,
+                        output_hidden_states=output_hidden_states,
+                        )
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs.logits[:, -1, :]
+                else:
+                    first_token = False
+                    input_bs = input_ids.size()[0]
+                    if model_inputs["past_key_values"] is None:
+                        first_token = True
+                    if first_token:
+                        seq_len = input_ids.size()[1]
+                        if re.search("GPTJ", self.config.architectures[0]):
+                            # beam_idx_tmp=torch.zeros(int(input_bs), dtype=torch.int)
+                            # model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), beam_idx_tmp) for i in range(self.config.n_layer)])
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)])) for i in range(self.config.n_layer)])
+                            model_inputs["position_ids"] = model_inputs["position_ids"][:1,:]
+                        elif re.search("llama", self.config.architectures[0], re.IGNORECASE):
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)]), torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)])) for i in range(self.config.num_hidden_layers)])
+                            model_inputs["position_ids"] = model_inputs["position_ids"][:1,:]
+                        elif re.search("bloom", self.config.architectures[0], re.IGNORECASE):
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.hidden_size/self.config.n_head)]), torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.hidden_size/self.config.n_head)])) for i in range(self.config.n_layer)])
+                        elif re.search("chatglm", self.config.architectures[0], re.IGNORECASE):
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)]), torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)])) for i in range(self.config.num_layers)])
+                            model_inputs["position_ids"] = model_inputs["position_ids"][:1,:]
+                        model_inputs["input_ids"] = model_inputs["input_ids"][:1,:]
+                        model_inputs["attention_mask"] = torch.cat([torch.zeros(1, 1), model_inputs["attention_mask"]], dim=-1)
+                    else:
+                        model_inputs["attention_mask"] = torch.cat([torch.zeros(input_bs, 1), model_inputs["attention_mask"]], dim=-1)
+                    model_inputs.pop("use_cache", None)
+                    model_inputs.pop("token_type_ids", None)
+
+                    if not hasattr(self, "trace_graph") and self.jit and self.ipex_int8:
+                        print("load_int8_model")
+                        self_jit = torch.jit.load(self.quantized_model_path)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+                    if not hasattr(self,"trace_graph") and self.jit and not self.ipex_int8:
+                        if hasattr(self, "forward"):
+                            sig = inspect.signature(self.forward)
+                        else:
+                            sig = inspect.signature(self.call)
+                        example_inputs = tuple(model_inputs[key] for key in sig.parameters
+                            if model_inputs.get(key, None) is not None and not isinstance(model_inputs.get(key, None), bool))
+                        self_jit = torch.jit.trace(self, example_inputs, strict=False)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+                    outputs = self.trace_graph(**model_inputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    if first_token:
+                        outputs = list(outputs)
+                        outputs[0] = outputs[0].expand(input_bs, -1, -1)
+                        past_key_values = []
+                        for key, value in outputs[1]:
+                            key_dim = key.dim()
+                            value_dim = value.dim()
+                            key = key.expand(input_bs, -1, -1, -1).contiguous()
+                            value = value.expand(input_bs, -1, -1, -1).contiguous()
+                            if key_dim == 3:
+                                key = key.view(key.size(1) * key.size(0), key.size(2), key.size(3))
+                            if value_dim == 3:
+                                value = value.view(value.size(1) * value.size(0), value.size(2), value.size(3))
+                            past_key_values.append(tuple([key, value]))
+                        outputs[1] = tuple(past_key_values)
+                        outputs = tuple(outputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs[0][:, -1, :]
+            else:
+                outputs = self(
+                    **model_inputs,
+                    return_dict=True,
+                    output_attentions=output_attentions,
+                    output_hidden_states=output_hidden_states,
+                    )
+                if synced_gpus and this_peer_finished:
+                    cur_len = cur_len + 1
+                    continue  # don't waste resources running the code we don't need
+                next_token_logits = outputs.logits[:, -1, :]
 
             # pre-process distribution
             next_tokens_scores = logits_processor(input_ids, next_token_logits)
@@ -2302,6 +2390,7 @@ class GenerationMixin:
                 )
 
             # stop when each sentence is finished, or if we exceed the maximum length
+            latency_list.append(time.time() - tic)
             if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):
                 if not synced_gpus:
                     break
@@ -2313,7 +2402,7 @@ class GenerationMixin:
 
         if return_dict_in_generate:
             if self.config.is_encoder_decoder:
-                return GreedySearchEncoderDecoderOutput(
+                output_result = GreedySearchEncoderDecoderOutput(
                     sequences=input_ids,
                     scores=scores,
                     encoder_attentions=encoder_attentions,
@@ -2323,14 +2412,19 @@ class GenerationMixin:
                     decoder_hidden_states=decoder_hidden_states,
                 )
             else:
-                return GreedySearchDecoderOnlyOutput(
+                output_result = GreedySearchDecoderOnlyOutput(
                     sequences=input_ids,
                     scores=scores,
                     attentions=decoder_attentions,
                     hidden_states=decoder_hidden_states,
                 )
         else:
-            return input_ids
+            output_result = input_ids
+
+        if self.token_latency is not None:
+            return (output_result, latency_list)
+        else:
+            return output_result
 
     def sample(
         self,
@@ -2733,6 +2827,7 @@ class GenerationMixin:
         ['Wie alt bist du?']
         ```"""
         # init values
+        latency_list = []
         logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
         stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
         if max_length is not None:
@@ -2795,6 +2890,7 @@ class GenerationMixin:
 
         this_peer_finished = False  # used by synced_gpus only
         while True:
+            tic = time.time()
             if synced_gpus:
                 # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.
                 # The following logic allows an early break if all peers finished generating their sequence
@@ -2806,19 +2902,134 @@ class GenerationMixin:
                     break
 
             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
-
-            outputs = self(
-                **model_inputs,
-                return_dict=True,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
-            )
-
-            if synced_gpus and this_peer_finished:
-                cur_len = cur_len + 1
-                continue  # don't waste resources running the code we don't need
-
-            next_token_logits = outputs.logits[:, -1, :]
+            if re.search("GPTJ", self.config.architectures[0]) or re.search("llama", self.config.architectures[0], re.IGNORECASE) or re.search("chatglm", self.config.architectures[0], re.IGNORECASE):
+                if self.jit == False:
+                    outputs = self(
+                        **model_inputs,
+                        return_dict=True,
+                        output_attentions=output_attentions,
+                        output_hidden_states=output_hidden_states,
+                        )
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs.logits[:, -1, :]
+                else:
+                    first_token = False
+                    input_bs = input_ids.size()[0]
+                    if model_inputs["past_key_values"] is None:
+                        first_token = True
+                    if first_token:
+                        seq_len = input_ids.size()[1]
+                        if re.search("GPTJ", self.config.architectures[0]):
+                            # beam_idx_tmp=torch.zeros(int(batch_size * num_beams), dtype=torch.int)
+                            # model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), beam_idx_tmp) for i in range(self.config.n_layer)])
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)])) for i in range(self.config.n_layer)])
+                        elif re.search("llama", self.config.architectures[0], re.IGNORECASE):
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)]), torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)])) for i in range(self.config.num_hidden_layers)])
+                        elif re.search("chatglm", self.config.architectures[0], re.IGNORECASE):
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)]), torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)])) for i in range(self.config.num_layers)])
+
+                        model_inputs["attention_mask"] = model_inputs["attention_mask"][:1,:]
+                        model_inputs["input_ids"] = model_inputs["input_ids"][:1,:]
+                        model_inputs["position_ids"] = model_inputs["position_ids"][:1,:]
+                        model_inputs["attention_mask"] = torch.cat([torch.zeros(1, 1), model_inputs["attention_mask"]], dim=-1)
+                    else:
+                        model_inputs["attention_mask"] = torch.cat([torch.zeros(input_bs, 1), model_inputs["attention_mask"]], dim=-1)
+                    model_inputs.pop("use_cache", None)
+                    model_inputs.pop("token_type_ids", None)
+
+                    if not hasattr(self, "trace_graph") and self.jit and self.ipex_int8:
+                        print("load_int8_model")
+                        self_jit = torch.jit.load(self.quantized_model_path)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+                    if not hasattr(self,"trace_graph") and self.jit and not self.ipex_int8:
+                        if hasattr(self, "forward"):
+                            sig = inspect.signature(self.forward)
+                        else:
+                            sig = inspect.signature(self.call)
+                        example_inputs = tuple(model_inputs[key] for key in sig.parameters
+                            if model_inputs.get(key, None) is not None and not isinstance(model_inputs.get(key, None), bool))
+                        self_jit = torch.jit.trace(self, example_inputs, strict=False)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+                    outputs = self.trace_graph(**model_inputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    if first_token:
+                        outputs = list(outputs)
+                        outputs[0] = outputs[0].expand(input_bs, -1, -1)
+                        past_key_values = []
+                        for key, value in outputs[1]:
+                            key_dim = key.dim()
+                            value_dim = value.dim()
+                            key = key.expand(input_bs, -1, -1, -1).contiguous()
+                            value = value.expand(input_bs, -1, -1, -1).contiguous()
+                            if key_dim == 3:
+                                key = key.view(key.size(1) * key.size(0), key.size(2), key.size(3))
+                            if value_dim == 3:
+                                value = value.view(value.size(1) * value.size(0), value.size(2), value.size(3))
+                            past_key_values.append(tuple([key, value]))
+                        outputs[1] = tuple(past_key_values)
+                        outputs = tuple(outputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs[0][:, -1, :]
+            else:
+                if model_inputs["past_key_values"] is None or self.jit == False:
+                    if re.search("T5", self.config.architectures[0]):
+                        first_token = False
+                    else:
+                        first_token = model_inputs["input_ids"].size()[1] != 1
+                    if first_token: 
+                        input_bs = input_ids.size()[0]
+                        seq_len = input_ids.size()[1]
+                        model_inputs["attention_mask"] = model_inputs["attention_mask"][:1,:]
+                        model_inputs["input_ids"] = model_inputs["input_ids"][:1,:]
+                    outputs = self(
+                        **model_inputs,
+                        return_dict=True,
+                        output_attentions=output_attentions,
+                        output_hidden_states=output_hidden_states,
+                    )
+                    if first_token: 
+                        outputs.logits = outputs.logits.expand(input_bs, seq_len, -1)
+                        past_key_values = []
+                        for key, value in outputs["past_key_values"]:
+                            key_dim = key.dim()
+                            value_dim = value.dim()
+                            key = key.expand(input_bs, -1, -1, -1).contiguous()
+                            value = value.expand(input_bs, -1, -1, -1).contiguous()
+                            if key_dim == 3:
+                                key = key.view(key.size(1) * key.size(0), key.size(2), key.size(3))
+                            if value_dim == 3:
+                                value = value.view(value.size(1) * value.size(0), value.size(2), value.size(3))
+                            past_key_values.append(tuple([key, value]))
+                        outputs.past_key_values = tuple(past_key_values)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need               
+                    next_token_logits = outputs.logits[:, -1, :]          
+                else:
+                    if hasattr(self, "forward"):
+                        sig = inspect.signature(self.forward)
+                    else:
+                        sig = inspect.signature(self.call)
+                    example_inputs = tuple(model_inputs[key] for key in sig.parameters
+                        if model_inputs.get(key, None) is not None and not isinstance(model_inputs.get(key, None), bool))
+                    if not hasattr(self,"trace_graph") and self.jit and not self.ipex_int8:
+                        self_jit = torch.jit.trace(self, example_inputs, strict=False)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+
+                    outputs = self.trace_graph(*example_inputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs[0][:, -1, :]               
             # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`
             # cannot be generated both before and after the `nn.functional.log_softmax` operation.
             next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)
@@ -2887,6 +3098,7 @@ class GenerationMixin:
 
             # increase cur_len
             cur_len = cur_len + 1
+            latency_list.append(time.time() - tic)
 
             if beam_scorer.is_done or stopping_criteria(input_ids, scores):
                 if not synced_gpus:
@@ -2910,7 +3122,7 @@ class GenerationMixin:
                 sequence_outputs["sequence_scores"] = None
 
             if self.config.is_encoder_decoder:
-                return BeamSearchEncoderDecoderOutput(
+                output_result = BeamSearchEncoderDecoderOutput(
                     sequences=sequence_outputs["sequences"],
                     sequences_scores=sequence_outputs["sequence_scores"],
                     scores=scores,
@@ -2922,7 +3134,7 @@ class GenerationMixin:
                     decoder_hidden_states=decoder_hidden_states,
                 )
             else:
-                return BeamSearchDecoderOnlyOutput(
+                output_result = BeamSearchDecoderOnlyOutput(
                     sequences=sequence_outputs["sequences"],
                     sequences_scores=sequence_outputs["sequence_scores"],
                     scores=scores,
@@ -2931,7 +3143,9 @@ class GenerationMixin:
                     hidden_states=decoder_hidden_states,
                 )
         else:
-            return sequence_outputs["sequences"]
+            output_result = sequence_outputs["sequences"]
+        # result
+        return (output_result, latency_list)
 
     def beam_sample(
         self,
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index 0df8d7e25..d14c9eede 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -298,8 +298,13 @@ def shard_checkpoint(
     current_block_size = 0
     total_size = 0
 
+    import io
     for key, weight in state_dict.items():
-        weight_size = weight.numel() * dtype_byte_size(weight.dtype)
+        if isinstance(weight, io.BytesIO):
+            # FP8 has extra state with io.BytesIO
+            weight_size = weight.seek(0, io.SEEK_END)
+        else:
+            weight_size = weight.numel() * dtype_byte_size(weight.dtype)
 
         # If this weight is going to tip up over the maximal size, we split.
         if current_block_size + weight_size > max_shard_size:
@@ -1831,7 +1836,10 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                 and is_main_process
                 and reg.fullmatch(filename_no_suffix) is not None
             ):
-                os.remove(full_filename)
+                try:
+                    os.remove(full_filename)
+                except OSError as e:
+                    print(e)
 
         # Save the model
         for shard_file, shard in shards.items():
diff --git a/src/transformers/models/bert/modeling_bert.py b/src/transformers/models/bert/modeling_bert.py
index ee7dcd5e4..5ead9c989 100755
--- a/src/transformers/models/bert/modeling_bert.py
+++ b/src/transformers/models/bert/modeling_bert.py
@@ -346,6 +346,8 @@ class BertSelfAttention(nn.Module):
 
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
         if attention_mask is not None:
+            if attention_mask.dtype != attention_scores.dtype:
+                attention_mask = attention_mask.to(attention_scores.dtype)
             # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
             attention_scores = attention_scores + attention_mask
 
@@ -1063,6 +1065,7 @@ class BertForPreTraining(BertPreTrainedModel):
 
         # Initialize weights and apply final processing
         self.post_init()
+        self.dense_seq_output = config.dense_seq_output
 
     def get_output_embeddings(self):
         return self.cls.predictions.decoder
@@ -1133,12 +1136,24 @@ class BertForPreTraining(BertPreTrainedModel):
         )
 
         sequence_output, pooled_output = outputs[:2]
+        if labels is not None and self.dense_seq_output:
+            batch_size = sequence_output.shape[0]
+            seq_len = sequence_output.shape[1]
+            hidden_dim = sequence_output.shape[2]
+            sequence_flattened = torch.index_select(sequence_output.view(-1,sequence_output.shape[-1]), 0, torch.nonzero(labels.view(-1) != -100, as_tuple=False).squeeze())
+            sequence_output = sequence_flattened
+
         prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)
 
         total_loss = None
         if labels is not None and next_sentence_label is not None:
             loss_fct = CrossEntropyLoss()
-            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
+            if self.dense_seq_output:
+                labels_flat = labels.view(-1)
+                labels_dense = labels_flat[labels_flat != -100]
+                masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels_dense)
+            else:
+                masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
             next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))
             total_loss = masked_lm_loss + next_sentence_loss
 
diff --git a/src/transformers/models/distilbert/modeling_distilbert.py b/src/transformers/models/distilbert/modeling_distilbert.py
index 84db89e0f..cb986d7ea 100755
--- a/src/transformers/models/distilbert/modeling_distilbert.py
+++ b/src/transformers/models/distilbert/modeling_distilbert.py
@@ -216,12 +216,11 @@ class MultiHeadSelfAttention(nn.Module):
         k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)
         v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)
 
-        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)
+
         scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)
+        scores = scores / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)
         mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)
-        scores = scores.masked_fill(
-            mask, torch.tensor(torch.finfo(scores.dtype).min)
-        )  # (bs, n_heads, q_length, k_length)
+        scores = scores.masked_fill(mask, -float("inf"))  # (bs, n_heads, q_length, k_length)
 
         weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)
         weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)
@@ -743,11 +742,11 @@ class DistilBertForSequenceClassification(DistilBertPreTrainedModel):
     )
     def forward(
         self,
+        labels: Optional[torch.LongTensor] = None,
         input_ids: Optional[torch.Tensor] = None,
         attention_mask: Optional[torch.Tensor] = None,
         head_mask: Optional[torch.Tensor] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
-        labels: Optional[torch.LongTensor] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
diff --git a/src/transformers/models/vit/modeling_vit.py b/src/transformers/models/vit/modeling_vit.py
index 474b92f72..094ea6eb0 100644
--- a/src/transformers/models/vit/modeling_vit.py
+++ b/src/transformers/models/vit/modeling_vit.py
@@ -129,14 +129,14 @@ class ViTEmbeddings(nn.Module):
             embeddings = embeddings * (1.0 - mask) + mask_tokens * mask
 
         # add the [CLS] token to the embedded patch tokens
-        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
+        cls_tokens = self.cls_token.to(embeddings.dtype).expand(batch_size, -1, -1)
         embeddings = torch.cat((cls_tokens, embeddings), dim=1)
 
         # add positional encoding to each token
         if interpolate_pos_encoding:
             embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)
         else:
-            embeddings = embeddings + self.position_embeddings
+            embeddings = embeddings + self.position_embeddings.to(embeddings.dtype)
 
         embeddings = self.dropout(embeddings)
 
@@ -779,8 +779,8 @@ class ViTForImageClassification(ViTPreTrainedModel):
     def forward(
         self,
         pixel_values: Optional[torch.Tensor] = None,
-        head_mask: Optional[torch.Tensor] = None,
         labels: Optional[torch.Tensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         interpolate_pos_encoding: Optional[bool] = None,
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index cf71499b0..0ff7236bd 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -158,6 +158,29 @@ from .utils import (
 )
 from .utils.generic import ContextManagers
 
+def trace_handler(prof):
+    print(prof.key_averages().table(
+        sort_by="self_cpu_time_total", row_limit=10))
+    import datetime
+    now = datetime.datetime.now()
+    log_path = os.path.join(os.getcwd(), "vit_profiling_{}_step_{}.json".format(now.strftime("%Y%m%d%H%M%S"), str(prof.step_num)))
+    prof.export_chrome_trace(log_path)
+profile_ctx = torch.profiler.profile(
+        activities=[
+            torch.profiler.ProfilerActivity.CPU,
+        ],
+        schedule=torch.profiler.schedule(
+            wait=0,
+            warmup=20,
+            active=20,
+            repeat=1),
+        on_trace_ready=trace_handler,
+        record_shapes=True,
+        profile_memory=True,
+        with_stack=True,
+        with_flops=True,
+        with_modules=True
+    )
 
 _is_native_cpu_amp_available = is_torch_greater_or_equal_than_1_10
 
@@ -336,7 +359,7 @@ class Trainer:
         self.hp_name = None
         self.deepspeed = None
         self.is_in_train = False
-
+        self.fp16_scaler = None
         # memory metrics - must set up as early as possible
         self._memory_tracker = TrainerMemoryTracker(self.args.skip_memory_metrics)
         self._memory_tracker.start()
@@ -606,7 +629,7 @@ class Trainer:
                         "but SageMaker Model Parallelism < 1.10 does not support FP16 in trainer."
                     )
 
-        if args.fp16 or args.bf16:
+        if args.fp16 or args.bf16 or args.fp16_cpu:
             if args.half_precision_backend == "auto":
                 if args.device == torch.device("cpu"):
                     if args.fp16:
@@ -621,7 +644,7 @@ class Trainer:
             logger.info(f"Using {args.half_precision_backend} half precision backend")
 
         self.do_grad_scaling = False
-        if (args.fp16 or args.bf16) and not (args.deepspeed or is_sagemaker_mp_enabled()):
+        if (args.fp16 or args.bf16 or args.fp16_cpu) and not (args.deepspeed or is_sagemaker_mp_enabled()):
             # deepspeed and SageMaker Model Parallel manage their own half precision
             if args.half_precision_backend == "cuda_amp":
                 self.use_cuda_amp = True
@@ -645,7 +668,7 @@ class Trainer:
                         self.scaler = torch.cuda.amp.GradScaler()
             elif args.half_precision_backend == "cpu_amp":
                 self.use_cpu_amp = True
-                self.amp_dtype = torch.bfloat16
+                self.amp_dtype = torch.bfloat16 if not args.fp16_cpu else torch.half
             else:
                 if not is_apex_available():
                     raise ImportError(
@@ -695,7 +718,7 @@ class Trainer:
         self._memory_tracker.stop_and_update_metrics()
 
         # torch.compile
-        if args.torch_compile and not is_torch_compile_available():
+        if args.inductor and not is_torch_compile_available():
             raise RuntimeError("Using torch.compile requires PyTorch 2.0 or higher.")
 
     def add_callback(self, callback):
@@ -1323,15 +1346,45 @@ class Trainer:
         return model
 
     def torch_jit_model_eval(self, model, dataloader, training=False):
+        print("[INFO] torch_jit_model_eval")
         if not training:
             if dataloader is None:
                 logger.warning("failed to use PyTorch jit mode due to current dataloader is none.")
                 return model
             example_batch = next(iter(dataloader))
             example_batch = self._prepare_inputs(example_batch)
+            int8_inputs=[]
+            if (self.args.int8 or self.args.do_calibration) and self.args.use_ipex:
+                import intel_extension_for_pytorch as ipex 
+                from intel_extension_for_pytorch.quantization import prepare, convert
+                from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig
+                qconfig = QConfig(activation=MinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.quint8), weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric))
+                #qconfig = ipex.quantization.default_static_qconfig
+                if self.args.smooth_quant:
+                    qconfig = ipex.quantization.get_smooth_quant_static_qconfig()
+                ipex.nn.utils._model_convert.replace_dropout_with_identity(model)
+                for key,value in example_batch.items():
+                    int8_inputs.append(value)
+                int8_inputs=tuple(int8_inputs)
+                prepared_model = prepare(model, qconfig, example_inputs=int8_inputs, inplace=False)
+                if self.args.do_calibration:
+                    for step, inputs in enumerate(dataloader):
+                        print("calibration step: {}".format(step))
+                        prepared_model(**inputs)
+                        if step == self.args.calibration_iters -1:
+                            prepared_model.save_qconf_summary(qconf_summary = self.args.int8_config)
+                            exit()
+                else:
+                    prepared_model.load_qconf_summary(qconf_summary = self.args.int8_config)
             try:
+                if self.args.int8:
+                    model = prepared_model
                 jit_model = model.eval()
                 with ContextManagers([self.autocast_smart_context_manager(cache_enabled=False), torch.no_grad()]):
+                    if self.args.int8 and self.args.use_ipex:
+                        jit_model = convert(jit_model)
+                        if self.args.smooth_quant:
+                            jit_model(*int8_inputs)
                     if version.parse(version.parse(torch.__version__).base_version) >= version.parse("1.14.0"):
                         if isinstance(example_batch, dict):
                             jit_model = torch.jit.trace(jit_model, example_kwarg_inputs=example_batch, strict=False)
@@ -1361,6 +1414,7 @@ class Trainer:
         return model
 
     def ipex_optimize_model(self, model, training=False, dtype=torch.float32):
+        print("[INFO] ipex_optimize_model")
         if not is_ipex_available():
             raise ImportError(
                 "Using IPEX but IPEX is not installed or IPEX's version does not match current PyTorch, please refer"
@@ -1372,22 +1426,40 @@ class Trainer:
         if not training:
             model.eval()
             dtype = torch.bfloat16 if not self.is_in_train and self.args.bf16_full_eval else dtype
+            if self.args.bf32:
+                ipex.set_fp32_math_mode(mode=ipex.FP32MathMode.BF32, device="cpu")
             # conv_bn_folding is disabled as it fails in symbolic tracing, resulting in ipex warnings
-            model = ipex.optimize(model, dtype=dtype, level="O1", conv_bn_folding=False, inplace=not self.is_in_train)
+            ipex._C.disable_jit_concat_linear()
+            model = ipex.optimize(model, dtype=dtype, level="O1", conv_bn_folding=False, inplace=not self.is_in_train, auto_kernel_selection=True if self.args.bf32 else False)
         else:
             if not model.training:
                 model.train()
-            model, self.optimizer = ipex.optimize(
-                model, dtype=dtype, optimizer=self.optimizer, inplace=True, level="O1"
-            )
+
+            if self.args.bf32:
+                ipex.set_fp32_math_mode(mode=ipex.FP32MathMode.BF32, device="cpu")
+                model, self.optimizer = ipex.optimize(model, dtype=torch.float32, optimizer=self.optimizer, auto_kernel_selection=True, inplace=True)
+            elif self.args.fp16_cpu:
+                self.fp16_scaler = torch.cpu.amp.GradScaler()
+                model, self.optimizer = ipex.optimize(model, optimizer=self.optimizer, dtype=torch.half, level='O0', weights_prepack=True, inplace=True)
+            else:
+                model, self.optimizer = ipex.optimize(
+                    model, dtype=dtype, optimizer=self.optimizer, inplace=True, level="O1"
+                )
 
         return model
 
     def _wrap_model(self, model, training=True, dataloader=None):
-        if self.args.use_ipex:
-            dtype = torch.bfloat16 if self.use_cpu_amp else torch.float32
+        if self.args.use_ipex and not self.args.int8 and not self.args.do_calibration:
+            dtype=torch.float32
+            if self.args.bf16:
+                dtype=torch.bfloat16
+            elif self.args.fp16_cpu:
+                dtype=torch.float16
+            elif self.args.bf32:
+                dtype=torch.float32
             model = self.ipex_optimize_model(model, training, dtype=dtype)
-
+        if (self.args.int8 or self.args.do_calibration) and self.args.use_ipex and not self.args.inductor:
+            self.args.jit_mode_eval = True
         if is_sagemaker_mp_enabled():
             # Wrapping the base model twice in a DistributedModel will raise an error.
             if isinstance(self.model_wrapped, smp.model.DistributedModel):
@@ -1415,6 +1487,74 @@ class Trainer:
             model = self.torch_jit_model_eval(model, dataloader, training)
             self.jit_compilation_time = round(time.time() - start_time, 4)
 
+        if self.args.inductor:
+            from torch._inductor import config as inductor_config
+            inductor_config.cpp_wrapper = True
+            example_batch = next(iter(dataloader))
+            if 'pixel_values' in example_batch:
+                if self.args.fp16_cpu:
+                    example_batch['pixel_values'] = example_batch['pixel_values'].to(torch.half)
+                elif self.args.bf16 or self.args.int8_bf16:
+                    example_batch['pixel_values'] = example_batch['pixel_values'].to(torch.bfloat16)
+            example_batch = self._prepare_inputs(example_batch)
+            if self.args.int8:
+                from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e
+                import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq
+                from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer
+                from torch._export import capture_pre_autograd_graph
+                print('[Info] Running torch.compile() INT8 quantization')
+                with torch.no_grad():
+                    exported_model = capture_pre_autograd_graph(
+                        model,
+                        (),
+                        kwargs=example_batch,
+                    )
+                    quantizer = X86InductorQuantizer()
+                    quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())
+                    prepared_model = prepare_pt2e(exported_model, quantizer)
+                    prepared_model(**example_batch)
+                    converted_model = convert_pt2e(prepared_model)
+                    torch.ao.quantization.move_exported_model_to_eval(converted_model)
+                    with torch.cpu.amp.autocast(enabled=(self.args.bf16 or self.args.int8_bf16)):
+                        if self.args.use_ipex:
+                            print('[Info] Running torch.compile() with IPEX backend')
+                            model = torch.compile(converted_model, backend="ipex")
+                        else:
+                            print('[Info] Running torch.compile() with default backend')
+                            model = torch.compile(converted_model)
+                        y = model(**example_batch)
+                        y = model(**example_batch)
+            elif self.args.bf16:
+                with torch.no_grad(), torch.cpu.amp.autocast(dtype=torch.bfloat16):
+                    if self.args.use_ipex:
+                        print('[Info] Running torch.compile() Bfloat16 with IPEX backend')
+                        model = torch.compile(model, backend="ipex")
+                    else:
+                        print('[Info] Running torch.compile() Bfloat16 with default backend')
+                        model = torch.compile(model)
+                    y = model(**example_batch)
+                    y = model(**example_batch)
+            elif self.args.fp16_cpu:
+                with torch.no_grad(), torch.cpu.amp.autocast(dtype=torch.half):
+                    if self.args.use_ipex:
+                        print('[Info] Running torch.compile() float16 with IPEX backend')
+                        model = torch.compile(model, backend="ipex")
+                    else:
+                        print('[Info] Running torch.compile() float16 with default backend')
+                        model = torch.compile(model)
+                    y = model(**example_batch)
+                    y = model(**example_batch)
+            else:
+                with torch.no_grad():
+                    if self.args.use_ipex:
+                        print('[Info] Running torch.compile() Float32 with IPEX backend')
+                        model = torch.compile(model, backend="ipex")
+                    else:
+                        print('[Info] Running torch.compile() Float32 with default backend')
+                        model = torch.compile(model)
+                    y = model(**example_batch)
+                    y = model(**example_batch)
+
         # Note: in torch.distributed mode, there's no point in wrapping the model
         # inside a DistributedDataParallel as we'll be under `no_grad` anyways.
         if not training:
@@ -1575,8 +1715,9 @@ class Trainer:
 
         # torch.compile() needs to be called after wrapping the model with FSDP or DDP
         # to ensure that it accounts for the graph breaks required by those wrappers
-        if self.args.torch_compile:
-            model = torch.compile(model, backend=self.args.torch_compile_backend, mode=self.args.torch_compile_mode)
+        if self.args.inductor:
+            with torch.cpu.amp.autocast(enabled=self.args.bf16 or self.args.fp16_cpu, dtype=torch.half if self.args.fp16_cpu else torch.bfloat16):
+                model = torch.compile(model, backend=self.args.torch_compile_backend, mode=self.args.torch_compile_mode)
 
         return model
 
@@ -1993,7 +2134,11 @@ class Trainer:
                         scale_after = self.scaler.get_scale()
                         optimizer_was_run = scale_before <= scale_after
                     else:
-                        self.optimizer.step()
+                        if self.args.fp16_cpu:
+                            self.fp16_scaler.step(self.optimizer)
+                            self.fp16_scaler.update()
+                        else:
+                            self.optimizer.step()
 
                     if optimizer_was_run and not self.deepspeed:
                         self.lr_scheduler.step()
@@ -2714,7 +2859,10 @@ class Trainer:
             # loss gets scaled under gradient_accumulation_steps in deepspeed
             loss = self.deepspeed.backward(loss)
         else:
-            loss.backward()
+            if self.args.fp16_cpu:
+                self.fp16_scaler.scale(loss).backward()
+            else:
+                loss.backward()
 
         return loss.detach()
 
@@ -3085,7 +3233,65 @@ class Trainer:
         self._memory_tracker.stop_and_update_metrics(output.metrics)
 
         return PredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=output.metrics)
-
+   
+    def benchmark_evaluate(self, model, dataloader):
+        steps_per_epoch = len(dataloader)
+        total_steps = (self.args.perf_run_iters + self.args.perf_begin_iter)
+        test_epoches = int(total_steps / steps_per_epoch)
+        print('Evaluating: Steps per Epoch {} total Steps {}'.format(steps_per_epoch, total_steps))
+        i = 0;
+        timeBuff = []
+        import time
+        # with torch.profiler.profile(
+        #   activities=[
+        #      torch.profiler.ProfilerActivity.CPU],
+        #      schedule=torch.profiler.schedule(
+        #      wait=1,
+        #      warmup=9,
+        #      active=5),
+        #   on_trace_ready=trace_handler
+        # ) as prof:
+        with tqdm(total=total_steps, desc="Evaluating") as pbar:
+            if self.args.profile:
+                prof = profile_ctx.__enter__()
+            for epoch in range(test_epoches + 1):
+                for it, batch in enumerate(dataloader):
+                    if 'pixel_values' in batch:
+                        if self.args.fp16_cpu:
+                            batch['pixel_values'] = batch['pixel_values'].to(torch.half)
+                        elif self.args.bf16 or self.args.int8_bf16:
+                            batch['pixel_values'] = batch['pixel_values'].to(torch.bfloat16)
+                    if epoch * steps_per_epoch + it >= total_steps:
+                        timeBuff = np.asarray(timeBuff)
+                        totalTime = np.sum(timeBuff)
+                        p50 = np.percentile(timeBuff, 50) # return 50th percentile, e.g median.
+                        p99 = np.percentile(timeBuff, 99)
+                        print("#############################")
+                        print("#############################")
+                        print('P50 Latency {:.2f} ms'.format(p50*1000))
+                        print('P99 Latency {:.2f} ms'.format(p99*1000))
+                        print('Throughput: {:.2f} sentences/s'.format(self.args.per_device_eval_batch_size*self.args.perf_run_iters/totalTime))
+                        print("#############################")
+                        break
+                    with torch.no_grad():
+                        if (self.args.bf16 or self.args.int8_bf16 or self.args.fp16_cpu) and self.args.inductor:
+                            with torch.cpu.amp.autocast(dtype=torch.half if self.args.fp16_cpu else torch.bfloat16):
+                                start = time.time()
+                                outputs = model(**batch)
+                                #prof.step()
+                                end = time.time()
+                        else:
+                            start = time.time()
+                            outputs = model(**batch)
+                            #prof.step()
+                            end = time.time()
+                        if epoch * steps_per_epoch + it > self.args.perf_begin_iter:
+                            timeBuff.append(end-start)
+                        pbar.update(1)
+                        if self.args.profile:
+                            prof.step()
+            if self.args.profile:
+                profile_ctx.__exit__(None, None, None)
     def evaluation_loop(
         self,
         dataloader: DataLoader,
@@ -3133,7 +3339,12 @@ class Trainer:
             logger.info("  Num examples: Unknown")
         logger.info(f"  Batch size = {batch_size}")
 
-        model.eval()
+        # For PT2 Quantization model, we've called move_exported_model_to_eval on the model
+        # and the training attribute of the model is set to True after the call.
+        # Calling .eval() again here will change the training attribute to False, causing re-compilation
+        # which is not the expected behavior.
+        if not (self.args.inductor and self.args.int8):
+            model.eval()
 
         self.callback_handler.eval_dataloader = dataloader
         # Do this before wrapping.
@@ -3158,6 +3369,20 @@ class Trainer:
         all_labels = None
         all_inputs = None
         # Will be useful when we have an iterable dataset so don't know its length.
+        if args.benchmark:
+            if self.args.use_share_weight:
+                threads = []
+                import threading
+                num_instances = self.args.total_cores // self.args.cores_per_instance
+                for i in range(0, num_instances):
+                     t = threading.Thread(target=self.benchmark_evaluate, args=(model, dataloader))
+                     threads.append(t)
+                     t.start()
+                for t in threads:
+                    t.join()
+            else:
+                self.benchmark_evaluate(model, dataloader)
+            exit()
 
         observed_num_examples = 0
         # Main evaluation loop
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 088eb06b7..887cfec22 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -777,10 +777,121 @@ class TrainingArguments:
             )
         },
     )
+    bf32: bool = field(
+        default=False,
+        metadata={
+            "help": (
+                "Whether to use bf32 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA"
+                " architecture or using CPU (no_cuda). This is an experimental API and it may change."
+            )
+        },
+    )
+
+    int8: bool = field(
+        default=False,
+        metadata={
+            "help": (
+                "Whether to use int8 (mixed) precision instead of 32-bit"
+            )
+        },
+    )
+    int8_fp32: bool = field(
+        default=False,
+        metadata={
+            "help": (
+                "Whether to use int8_fp32 (mixed) precision instead of 32-bit"
+            )
+        },
+    )
+    int8_bf16: bool = field(
+        default=False,
+        metadata={
+            "help": (
+                "Whether to use int8_bf16 (mixed) precision instead of 32-bit"
+            )
+        },
+    )
+    use_share_weight: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable weight sharing for real time mode inference"
+        },
+    )
+    total_cores: int = field(
+        default=56,
+        metadata={
+            "help": "Total cores one socket for use_share_weight"
+        },
+    )
+    cores_per_instance: int = field(
+        default=4,
+        metadata={
+            "help": "cores per instance for use_share_weight"
+        },
+    )
+    do_calibration: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable calibration process for ipex int8"
+        },
+    )
+    calibration_iters: int = field(
+        default=200,
+        metadata={
+            "help": "The iterations for calibration"
+        },
+    )
+    smooth_quant: bool = field(
+        default=False,
+        metadata={
+            "help": (
+                "Whether to use smoothQuant for int8 (mixed) precision"
+            )
+        },
+    )
+
+    int8_config:str = field(
+        default="",
+        metadata={
+            "help": "The calibration result for int8 config"
+        }
+    )
+    auto_kernel_selection: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable mkldnn for ipex fp32"
+        },
+    )
+    benchmark: bool = field(
+        default=False,
+        metadata={
+            "help": "doing the customized benchmark process, getting P50, P99, THP"
+        },
+    )
+    perf_run_iters: int = field(
+        default=100,
+        metadata={
+            "help": "The iterations number for benchmark"
+        },
+    )
+    perf_begin_iter: int = field(
+        default=10,
+        metadata={
+            "help": "The iteration to start the benchmark iterations"
+        },
+    )
     fp16: bool = field(
         default=False,
         metadata={"help": "Whether to use fp16 (mixed) precision instead of 32-bit"},
     )
+    fp16_cpu: bool = field(
+        default=False,
+        metadata={"help": "Whether to use fp16 (mixed) precision instead of 32-bit on cpu using IPEX"},
+    )
+    inductor: bool = field(
+        default=False,
+        metadata={"help": "Whether to use torch.compile() inductor backend"},
+    )
     fp16_opt_level: str = field(
         default="O1",
         metadata={
@@ -963,7 +1074,9 @@ class TrainingArguments:
     label_smoothing_factor: float = field(
         default=0.0, metadata={"help": "The label smoothing epsilon to apply (zero means no label smoothing)."}
     )
-
+    profile: bool = field(
+        default=False, metadata={"help": "enable profile"}
+    )
     default_optim = "adamw_hf"
     # XXX: enable when pytorch==2.0.1 comes out - we want to give it time to get all the bugs sorted out
     # if is_torch_available() and version.parse(version.parse(torch.__version__).base_version) >= version.parse("2.1.0"):
@@ -1253,17 +1366,17 @@ class TrainingArguments:
             if version.parse(version.parse(torch.__version__).base_version) == version.parse("2.0.0") and self.fp16:
                 raise ValueError("--optim adamw_torch_fused with --fp16 requires PyTorch>2.0")
 
-        if (
-            self.framework == "pt"
-            and is_torch_available()
-            and (self.device.type != "cuda")
-            and (get_xla_device_type(self.device) != "GPU")
-            and (self.fp16 or self.fp16_full_eval)
-        ):
-            raise ValueError(
-                "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation"
-                " (`--fp16_full_eval`) can only be used on CUDA devices."
-            )
+        #if (
+        #    self.framework == "pt"
+        #    and is_torch_available()
+        #    and (self.device.type != "cuda")
+        #    and (get_xla_device_type(self.device) != "GPU")
+        #    and (self.fp16 or self.fp16_full_eval)
+        #):
+        #    raise ValueError(
+        #        "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation"
+        #        " (`--fp16_full_eval`) can only be used on CUDA devices."
+        #    )
 
         if (
             self.framework == "pt"
