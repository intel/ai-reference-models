# Copyright (c) 2023 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

ARG BASE_IMAGE="intel/intel-optimized-pytorch"
ARG BASE_TAG="centos-pt2.3-py310"

FROM ${BASE_IMAGE}:${BASE_TAG} AS intel-extension-for-pytorch

SHELL ["/bin/bash", "-c"]

ENV LANG C.UTF-8

WORKDIR /workspace/pytorch-llama-inference

RUN apt-get update && \
    apt-get install --no-install-recommends -y \
    gcc \
    cmake \
    build-essential \
    ca-certificates \
    git \
    wget \
    make \
    cmake \
    curl \
    autoconf \ 
    bzip2 \
    tar \
    unzip \
    bc

RUN git clone https://github.com/jemalloc/jemalloc.git && \
    cd jemalloc && \
    git checkout c8209150f9d219a137412b06431c9d52839c7272 && \
    ./autogen.sh && \
    ./configure --prefix=/workspace/lib/jemalloc && \
    make && \ 
    make install

RUN wget https://github.com/gperftools/gperftools/releases/download/gperftools-2.7.90/gperftools-2.7.90.tar.gz && \
    tar -xzf gperftools-2.7.90.tar.gz && \
    cd gperftools-2.7.90 && \
    ./configure --prefix=/workspace/lib/tcmalloc && \
    make && \
    make install

RUN pip install packaging \
        intel-openmp \
        datasets \
        sentencepiece \
        psutil \
        accelerate

RUN pip install -U "huggingface_hub[cli]"

ENV LD_PRELOAD="/workspace/lib/jemalloc/lib/libjemalloc.so":"/workspace/lib/tcmalloc/lib/libtcmalloc.so":"/usr/local/lib/libiomp5.so":$LD_PRELOAD
#ENV MALLOC_CONF="oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000"

COPY models_v2/pytorch/llama/inference/cpu .
COPY models_v2/common/enable_ipex_for_transformers.diff .

RUN wget https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/prompt.json \
    -O prompt.json

RUN git clone https://github.com/huggingface/transformers.git && \
    cd transformers && \
    git checkout v4.28.1 && \
    git apply ../enable_ipex_for_transformers.diff && \
    pip install -e ./

RUN apt-get update && \
    apt-get install --no-install-recommends --fix-missing -y \
    numactl \
    libegl1-mesa 

RUN python -m pip install --no-cache-dir --upgrade pip Pillow==10.3.0 \
        jinja2==3.1.4 \
        setuptools==65.5.1

RUN rm -rf gperftools-2.7.90* jemalloc 

COPY LICENSE licenses/LICENSE
COPY third_party licenses/third_party
