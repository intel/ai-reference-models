BERT-Large Pretraining fp32:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-pretraining
  cmd: /bin/bash models_v2/pretraining.sh -a 64 -e 2
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm
    OUTPUT_DIR: /output/tf-bert-large-pretraining/fp32
    PRECISION: fp32
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
    OMP_NUM_THREADS: '64'
    BATCH_SIZE: '32'
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm
    dst: /tf_dataset/dataset/bert_large_wwm
  - src: $PWD/output/tf-bert-large-pretraining/fp32
    dst: /output/tf-bert-large-pretraining/fp32
BERT-Large Pretraining bf16:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-pretraining
  cmd: /bin/bash models_v2/pretraining.sh -a 64 -e 2
  env:
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
    OMP_NUM_THREADS: '64'
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm
    OUTPUT_DIR: /output/tf-bert-large-pretraining/bf16
    PRECISION: bfloat16
    BATCH_SIZE: '128'
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm
    dst: /tf_dataset/dataset/bert_large_wwm
  - src: $PWD/output/tf-bert-large-pretraining/bf16
    dst: /output/tf-bert-large-pretraining/bf16
