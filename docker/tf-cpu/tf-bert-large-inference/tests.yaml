BERT-Large Realtime Inference fp32:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/inference_realtime.sh
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/realtime_fp32
    BATCH_SIZE: '1'
    PRECISION: fp32
    OMP_NUM_THREADS: '64'
    CORES_PER_INSTANCE: '64'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/realtime_fp32
    dst: /output/tf-bert-large-inference/realtime_fp32
BERT-Large Weightsharing Inference fp32:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/inference_realtime_weight_sharing.sh
  env:
    TF_ONEDNN_USE_SYSTEM_ALLOCATOR: '1'
    OMP_NUM_THREADS: '4'
    CORES_PER_INSTANCE: '4'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/weightsharing_fp32
    BATCH_SIZE: '1'
    PRECISION: fp32
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/weightsharing_fp32
    dst: /output/tf-bert-large-inference/weightsharing_fp32
BERT-Large Throughput fp32:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/inference_throughput.sh
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/throughput_fp32
    BATCH_SIZE: '128'
    PRECISION: fp32
    OMP_NUM_THREADS: '64'
    CORES_PER_INSTANCE: '64'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/throughput_fp32
    dst: /output/tf-bert-large-inference/throughput_fp32
BERT-Large Accuracy fp32:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/accuracy.sh --verbose
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/accuracy_fp32
    BATCH_SIZE: '56'
    PRECISION: fp32
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/accuracy_fp32
    dst: /output/tf-bert-large-inference/accuracy_fp32
BERT-Large Accuracy bf16:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/accuracy.sh --verbose
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/accuracy_bf16
    BATCH_SIZE: '56'
    PRECISION: bfloat16
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/accuracy_bf16
    dst: /output/tf-bert-large-inference/accuracy_bf16
BERT-Large Realtime Inference bf16:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/inference_realtime.sh
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/realtime_bf16
    BATCH_SIZE: '1'
    PRECISION: bfloat16
    LD_PRELOAD: /workspace/tcmalloc/install/lib/libtcmalloc.so.4
    OMP_NUM_THREADS: '4'
    CORES_PER_INSTANCE: '4'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/realtime_bf16
    dst: /output/tf-bert-large-inference/realtime_bf16
BERT-Large Weightsharing Inference bf16:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/inference_realtime_weight_sharing.sh
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/weightsharing_bf16
    BATCH_SIZE: '1'
    PRECISION: bfloat16
    LD_PRELOAD: /workspace/tcmalloc/install/lib/libtcmalloc.so.4
    TF_ONEDNN_USE_SYSTEM_ALLOCATOR: '1'
    OMP_NUM_THREADS: '64'
    CORES_PER_INSTANCE: '64'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/weightsharing_bf16
    dst: /output/tf-bert-large-inference/weightsharing_bf16
BERT-Large Throughput bf16:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/inference_throughput.sh
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/throughput_bf16
    BATCH_SIZE: '128'
    PRECISION: bfloat16
    OMP_NUM_THREADS: '64'
    CORES_PER_INSTANCE: '64'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/throughput_bf16
    dst: /output/tf-bert-large-inference/throughput_bf16
BERT-Large Accuracy int8:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/accuracy.sh
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/accuracy_int8
    BATCH_SIZE: '56'
    PRECISION: int8
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/accuracy_int8
    dst: /output/tf-bert-large-inference/accuracy_int8
BERT-Large Realtime Inference int8:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/inference_realtime.sh
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/realtime_int8
    BATCH_SIZE: '1'
    PRECISION: int8
    LD_PRELOAD: /workspace/tcmalloc/install/lib/libtcmalloc.so.4
    OMP_NUM_THREADS: '4'
    CORES_PER_INSTANCE: '4'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/realtime_int8
    dst: /output/tf-bert-large-inference/realtime_int8
BERT-Large Weightsharing Inference int8:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/inference_realtime_weight_sharing.sh
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/weightsharing_int8
    BATCH_SIZE: '1'
    PRECISION: int8
    LD_PRELOAD: /workspace/tcmalloc/install/lib/libtcmalloc.so.4
    TF_ONEDNN_USE_SYSTEM_ALLOCATOR: '1'
    OMP_NUM_THREADS: '64'
    CORES_PER_INSTANCE: '64'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/weightsharing_int8
    dst: /output/tf-bert-large-inference/weightsharing_int8
BERT-Large Throughput int8:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-language-modeling-tf-bert-large-inference
  cmd: /bin/bash quickstart/inference_throughput.sh
  env:
    DATASET_DIR: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    OUTPUT_DIR: /output/tf-bert-large-inference/throughput_int8
    BATCH_SIZE: '16'
    PRECISION: int8
    OMP_NUM_THREADS: '4'
    CORES_PER_INSTANCE: '4'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
    dst: /tf_dataset/dataset/bert_large_wwm/wwm_uncased_L-24_H-1024_A-16
  - src: $PWD/output/tf-bert-large-inference/throughput_int8
    dst: /output/tf-bert-large-inference/throughput_int8
