{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Intel® AI Reference Models Jupyter Notebook\n",
    "\n",
    "This Jupyter notebook helps you choose and run a comparison between two models from the Intel® AI Reference Models repo using Intel® Optimizations for TensorFlow*, Intel® extension for TensorFlow, Intel® Optimizations for PyTorch and Intel® extension for PyTorch. When you run the notebook, it installs required package dependencies, displays information about your platform, lets you choose the two models to compare, runs those models, and finally displays a performance comparison chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_1'></a>\n",
    "# Step 1: Display Platform Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required dependencies for the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install matplotlib ipykernel psutil pandas cxxfilt gitpython\n",
    "!pip install gcg\n",
    "!python3 -m pip install gitpython\n",
    "!pip install prettytable\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a framework from the dropdown option below the next cell and install the required framework packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.environ['no_proxy'] = \"\"\n",
    "os.environ['NO_PROXY'] =\"\"\n",
    "\n",
    "# Create a dropdown widget without a default value\n",
    "framework_dropdown = widgets.Dropdown(\n",
    "    options=['PyTorch', 'TensorFlow'],\n",
    "    value=None,\n",
    "    description='Choose Framework:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Display the dropdown widget\n",
    "display(framework_dropdown)\n",
    "\n",
    "def run_command(command):\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    return stdout.decode(), stderr.decode(), process.returncode\n",
    "\n",
    "def install_packages(selected_framework):\n",
    "    if selected_framework == 'PyTorch':\n",
    "        # Install the latest version.\n",
    "        # Installation 2a. pypi.org\n",
    "        subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"torch\", \"--upgrade\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"])\n",
    "        subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"torchvision\", \"--upgrade\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"])\n",
    "        subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"torchaudio\", \"--upgrade\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"])\n",
    "        subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"intel-extension-for-pytorch\", \"--upgrade\"])\n",
    "        subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"oneccl_bind_pt\", \"--upgrade\", \"--extra-index-url\", \"https://pytorch-extension.intel.com/release-whl/stable/cpu/us/\"])\n",
    "\n",
    "        print(\"PyTorch packages installed.\")\n",
    "        os.environ['Framework'] = 'pytorch'\n",
    "        os.environ['yaml_file'] = 'profiling/ai_reference_models_pytorch.yaml'\n",
    "\n",
    "        # Install dependencies for pytorch:\n",
    "        print(\"Install PyTorch dependencies\")\n",
    "        current_directory = os.getcwd()\n",
    "\n",
    "        if os.path.isdir(\"jemalloc\"):\n",
    "            import shutil\n",
    "            shutil.rmtree(\"jemalloc\")\n",
    "            os.environ.pop(\"LD_PRELOAD\", None)\n",
    "            os.environ.pop(\"MALLOC_CONF\", None)\n",
    "        !git clone https://github.com/jemalloc/jemalloc.git\n",
    "        os.chdir(\"jemalloc\")\n",
    "        !git checkout c8209150f9d219a137412b06431c9d52839c7272\n",
    "        !./autogen.sh\n",
    "        !./configure --prefix={current_directory}/\n",
    "        !make\n",
    "        !make install\n",
    "        os.chdir(\"..\")\n",
    "\n",
    "        !pip install packaging intel-openmp --target f\"{current_directory}\"\n",
    "\n",
    "        if os.path.isdir(\"gperftools-2.7.90\"):\n",
    "            shutil.rmtree(\"gperftools-2.7.90\")\n",
    "        !wget https://github.com/gperftools/gperftools/releases/download/gperftools-2.7.90/gperftools-2.7.90.tar.gz\n",
    "        !tar -xzf gperftools-2.7.90.tar.gz\n",
    "        os.chdir(\"gperftools-2.7.90\")\n",
    "        !./configure --prefix={current_directory}/tcmalloc\n",
    "        !make\n",
    "        !make install\n",
    "        os.chdir(\"..\")\n",
    "\n",
    "        # print(f\"{current_directory}/jemalloc/lib/libjemalloc.so:{current_directory}/tcmalloc/lib/libtcmalloc.so:/usr/local/lib/libiomp5.so\")\n",
    "        # os.environ[\"LD_PRELOAD\"] = f\"{current_directory}/jemalloc/lib/libjemalloc.so:{current_directory}/tcmalloc/lib/libtcmalloc.so:/usr/local/lib/libiomp5.so\"\n",
    "        os.environ[\"LD_PRELOAD\"] = f\"{current_directory}/jemalloc/lib/libjemalloc.so:{current_directory}/tcmalloc/lib/libtcmalloc.so\"\n",
    "        os.environ[\"MALLOC_CONF\"] = \"oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000\"\n",
    "        os.environ[\"DNNL_MAX_CPU_ISA\"] = \"AVX512_CORE_AMX\"\n",
    "        print(\"Finish building PyTorch\")\n",
    "    elif selected_framework == 'TensorFlow':\n",
    "        subprocess.run([\"pip\", \"install\", \"intel-tensorflow\"])\n",
    "        # subprocess.run([\"pip\", \"install\", \"tensorflow\"])\n",
    "        subprocess.run([\"pip\", \"install\", \"--upgrade\", \"intel-extension-for-tensorflow[cpu]\"])\n",
    "        print(\"TensorFlow packages installed.\")\n",
    "        os.environ['Framework'] = 'tensorflow'\n",
    "        os.environ['yaml_file'] = 'profiling/ai_reference_models_tensorflow.yaml'\n",
    "        print(\"Finish building TF\")\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid framework selected.\")\n",
    "\n",
    "# Add an event handler to the dropdown to trigger package installation\n",
    "def on_dropdown_change(change):\n",
    "    install_packages(change.new)\n",
    "\n",
    "framework_dropdown.observe(on_dropdown_change, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Set the path for the AI reference models, assumed to be the current working directory, and the path for the utility functions directory inside the cloned Intel® AI Reference Models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# If default path does not work, change AIReferenceRoot path according to your environment\n",
    "os.environ['NotebookRoot'] = os.getcwd()\n",
    "os.environ['AIReferenceRoot'] = os.environ['NotebookRoot'] + os.sep + \"..\"\n",
    "os.environ['ProfileUtilsRoot'] = os.environ['AIReferenceRoot'] + os.sep + \"notebooks/profiling/\"\n",
    "print(\"Path for the AI Reference Models root is: \", os.environ['AIReferenceRoot'])\n",
    "print(\"Path for utility functions directory is: \", os.environ['ProfileUtilsRoot'])\n",
    "\n",
    "# Check for mandatory python scripts after AIReferenceRoot and ProfileUtilsRoot are assigned\n",
    "import os\n",
    "current_path = os.getcwd()\n",
    "benchmark_path = os.environ['AIReferenceRoot'] + os.sep + \"benchmarks/launch_benchmark.py\"\n",
    "if os.path.exists(benchmark_path) == True:\n",
    "    print(benchmark_path)\n",
    "else:\n",
    "    print(benchmark_path)\n",
    "    print(\"ERROR! Can't find benchmark/launch_benchmark.py script!\")\n",
    "\n",
    "profile_utils_path = os.environ['ProfileUtilsRoot'] + \"profile_utils.py\"\n",
    "if os.path.exists(profile_utils_path) == True:\n",
    "    print(profile_utils_path)\n",
    "else:\n",
    "    print(\"ERROR! Can't find profile_utils.py script!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the first model\n",
    "\n",
    "This notebook helps you compare the performance of two models listed in the supported models. Select the first model to run and compare.  (If the environment variable MODEL_1_INDEX is set, we'll use that instead of prompting for input.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Display all the supported models and select first  model to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "notebook_root = os.environ['NotebookRoot']\n",
    "%cd $notebook_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the list of models is displayed, select one by entering the model index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from profiling.profile_utils import AIReferenceConfigFile\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "config = AIReferenceConfigFile(os.environ['yaml_file'])\n",
    "sections = config.read_supported_section()\n",
    "\n",
    "# Create a table with headers\n",
    "models_table = PrettyTable([\"Index\", \"Model Name\", \"Framework\", \"Mode\"])\n",
    "\n",
    "# Iterate through the sections and add rows to the table\n",
    "for index, section in enumerate(sections):\n",
    "    model_name = section['model-name']\n",
    "    framework = section['framework']\n",
    "    mode = section['mode']\n",
    "\n",
    "    # Add a row to the table\n",
    "    models_table.add_row([index, model_name, framework, mode])\n",
    "\n",
    "print(\"Supported Models: \")\n",
    "print(models_table)\n",
    "\n",
    "# use the \"MODEL_1_INDEX\" environment variable value if it exists.\n",
    "import os\n",
    "env_model_1_index=os.environ.get('MODEL_1_INDEX', '')\n",
    "if env_model_1_index != '':\n",
    "    model_1_index= int(env_model_1_index)\n",
    "else:\n",
    "    ## USER INPUT\n",
    "    model_1_index= int(input('Input a index number of a model: '))\n",
    "\n",
    "# List out the selected model name\n",
    "if model_1_index >= len(sections):\n",
    "    print(\"ERROR! Input a model_index value between 0 and \", len(sections))\n",
    "else:\n",
    "    model_1_name=sections[model_1_index]['name']\n",
    "\n",
    "    # Prints out model name\n",
    "    print(\"First model is: \", model_1_name.split()[0] )\n",
    "\n",
    "    selected_section = sections[model_1_index]\n",
    "    model_name_1 = selected_section['model-name']\n",
    "\n",
    "    # Check if the selected model has a precision section\n",
    "    if model_name_1 in selected_section:\n",
    "        print(f\"Available Precisions for {model_name_1}:\")\n",
    "\n",
    "        # Create a table with headers\n",
    "        precision_table = PrettyTable([\"Index\", \"Precision\"])\n",
    "        precisions = selected_section[model_name_1]\n",
    "\n",
    "        # Iterate through the available precisions\n",
    "        for index, precision_info in enumerate(selected_section[model_name_1]):\n",
    "            precision = precision_info['precision']\n",
    "\n",
    "            # Add a row to the table\n",
    "            precision_table.add_row([index, precision])\n",
    "\n",
    "        print(precision_table)\n",
    "\n",
    "        # Prompt the user to select a precision\n",
    "        precision_index = int(input('Select an index number for the precision: '))\n",
    "        selected_precision = precisions[precision_index]['precision']\n",
    "        print(\"Selected Precision for\", model_1_name, selected_precision)\n",
    "\n",
    "        # Print a table for available scripts for selected precision\n",
    "        script_section = precisions[precision_index]\n",
    "        script_name = script_section['script']\n",
    "\n",
    "        print(f\"Available Scripts for {selected_precision}:\")\n",
    "\n",
    "        # Create a table with headers\n",
    "        script_table = PrettyTable([\"Index\", \"Script\"])\n",
    "\n",
    "        # Iterate through the available scripts\n",
    "        for index, script in enumerate(script_name):\n",
    "            script_table.add_row([index, script])\n",
    "\n",
    "        print(script_table)\n",
    "\n",
    "        # Prompt the user to select a script\n",
    "        script_index = int(input('Input a index number for the script: '))\n",
    "        selected_row = script_table._rows[script_index]\n",
    "        selected_script = selected_row[1]\n",
    "        print(\"Selected script for\", model_1_name, selected_script)\n",
    "\n",
    "        # Prints out model name\n",
    "        print(\"First model with selected precision and script is: \", model_1_name, selected_precision, selected_script)\n",
    "        os.environ['PRECISION']=selected_precision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Setup\n",
    "The following cell checks for additional environment variables that needs to be specified per model specification. \n",
    "\n",
    "Then checks for additional files, libraries, and configurations that needs to be installed per model specification. Commands are installed in a chronological dependency order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "ai_reference_root = os.environ.get('AIReferenceRoot')\n",
    "%cd $ai_reference_root\n",
    "\n",
    "for export in selected_section['exports']:\n",
    "    env_name = export.split(' ')[0]\n",
    "    env_value = export.split(' ')[1]\n",
    "    if env_value == '-i':\n",
    "        env_value = input(f'Please provide the path for the following environment variable {env_name}: ', )\n",
    "    os.environ[env_name] = os.path.expandvars(env_value)\n",
    "\n",
    "    print(\"Assigned environment variable \", env_name, \" : \", os.environ[env_name])\n",
    "\n",
    "for cmd in selected_section['additional-commands']:\n",
    "    subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Get the required dataset for the selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell checks the dataset path. If the data-location is already specified, then the notebook will use the dataset path mentioned in ai_reference_models.ini file. If the data-location is not specified in the ai_reference_models.ini, the data downloading option instructions are shown. You must manually download the dataset using these instructions before you can proceed to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the parameters from config\n",
    "notebook_reference_root = os.environ.get('NotebookRoot')\n",
    "%cd $notebook_reference_root\n",
    "config = AIReferenceConfigFile(os.environ['yaml_file'])\n",
    "config_data = config.read_config(model_1_name)\n",
    "data_download_path=''\n",
    "\n",
    "if config_data['data-download'] != '': #and config.data_location == '':\n",
    "    print(\"\\nFollow these instructions to get the data : \")\n",
    "    if config_data['data-download'] != '':\n",
    "        val = config_data['data-download']\n",
    "    # use the \"DATA_DOWNLOAD_PATH\" environment variable value if it exists.\n",
    "    env_data_download_path=os.environ.get('DATA_DOWNLOAD_PATH', '')\n",
    "    if env_data_download_path != '':\n",
    "        data_download_path= env_data_download_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path for dataset directory for the first model:\n",
    "\n",
    "DATASET_DIR': the path where the dataset exists and is downloaded.\n",
    "\n",
    "**ACTION : You need to input the path where the dataset for the first model exists or where you have downloaded it in your system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = input('Input the path where the dataset exists for the first model:')\n",
    "\n",
    "os.environ['DATASET_DIR'] = dataset_path\n",
    "\n",
    "print(\"Data location path:\", os.environ['DATASET_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Prepare pre-trained model for the selected model\n",
    "\n",
    "This step checks if the pre-trained model for the selected model exists in the pre-trained directory path. If the pre-trained directory does not exist, then it downloads the pre-trained model for the selected precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "config = AIReferenceConfigFile(os.environ['yaml_file'])\n",
    "\n",
    "# Get the parameters from config\n",
    "configvals=config.read_config(model_1_name)\n",
    "\n",
    "# Iterate through the 'resnet50v1_5' list to find the matching 'precision' and get the 'wget' value\n",
    "for item in configvals[model_1_name.split()[0]]:\n",
    "    if item['precision'] == selected_precision:\n",
    "        wget_value = item['wget']\n",
    "        break  # You can break the loop once you find the match\n",
    "\n",
    "# Get the pre-trained model file\n",
    "pretrain_model_path = config.download_pretrained_model(wget=wget_value, current_path=current_path)\n",
    "pretrain_model_path = config.uncompress_file(pretrain_model_path, current_path=current_path)\n",
    "if pretrain_model_path:\n",
    "    print('Downloaded the model in:', pretrain_model_path)\n",
    "else:\n",
    "    print('Failed to download the pretrained model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the environment variable for pre-trained model for the first model:\n",
    "\n",
    "'PRETRAINED_MODEL': the path where is the pretrained_model exists and is downloaded.\n",
    "\n",
    "NOTE: You can change the value of 'PRETRAINED_MODEL' by changing its assignment in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"PRETRAINED_MODEL\"]=pretrain_model_path\n",
    "print(\"Pretrain_model_path:\", os.environ[\"PRETRAINED_MODEL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Set the log directory or output directory\n",
    "\n",
    "'OUTPUT_DIR': the output directory path where model output logs are collected.\n",
    "\n",
    "The default directory name for the output logs is \\\"logs\\\" in the current working directory. You can change this directory name by replacing the value assigned to log_directory in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Set the log directory/output directory to store the logs\n",
    "log_directory=os.environ['NotebookRoot'] + os.sep + \"logs\"\n",
    "print(log_directory)\n",
    "\n",
    "#Set output-dir directory\n",
    "if log_directory !='':\n",
    "    configvals['output-dir'] = log_directory\n",
    "\n",
    "os.environ['OUTPUT_DIR']=log_directory\n",
    "print(\"Output directory path is:\", os.environ['OUTPUT_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5:  Run the first Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**if BATCH_SIZE is specified per model specification, set it for optimal batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "if selected_section['set-batch-size']['cores']:\n",
    "    import subprocess\n",
    "\n",
    "    # Run the shell command to get the number of cores\n",
    "    output = subprocess.check_output(\"lscpu | grep 'Core(s) per socket' | awk '{print $4}'\", shell=True)\n",
    "\n",
    "    # Decode the byte string output into a regular Python string\n",
    "    cores = int(output.decode(\"utf-8\").strip())\n",
    "\n",
    "    cores = eval(selected_section['set-batch-size']['expr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL: You can change the batch size from the model's default. For online_inference,  set batch_size value to be 1.** \n",
    "\n",
    "**Run the next cell if you want to change the batch-size, other skip running the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "batch_size= input('Set the value for batch size that you want to run: ')\n",
    "os.environ[\"BATCH_SIZE\"]=batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the first model using the quickstart script configured in the ai_reference_models.ini file, and save output logs to the selected log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "config = AIReferenceConfigFile(os.environ['yaml_file'])\n",
    "\n",
    "# Get the parameters from config\n",
    "config_data = config.read_config(model_1_name)\n",
    "print(config_data)\n",
    "print(config_data.get('ai-type'))\n",
    "\n",
    "#print(config_dict.get('--mode:'))\n",
    "\n",
    "# Split the model_name into individual parts\n",
    "parts = model_1_name.split()\n",
    "\n",
    "# Join the parts using hyphens as the separator\n",
    "log_name = '-'.join(parts)\n",
    "log_name = log_name + \".log\"\n",
    "\n",
    "ai_reference_root = os.environ.get('AIReferenceRoot')\n",
    "%cd $ai_reference_root\n",
    "run_workload = (\"quickstart/\" + config_data.get('ai-type') + \"/\" + config_data.get('framework') + \"/\"+ config_data.get('model-name') + \"/\"\n",
    "                + config_data.get('mode') + \"/\" + config_data.get('device') +\"/\" + selected_script )\n",
    "print(log_directory)\n",
    "print(log_name)\n",
    "print(run_workload)\n",
    "\n",
    "import subprocess\n",
    "print(selected_precision)\n",
    "print(type(selected_precision))\n",
    "cmd = \"./{} {} {} | tee {}/{}\".format(run_workload, selected_precision, \"ipex-jit\", log_directory, log_name)\n",
    "print(cmd)\n",
    "subprocess.call(cmd, shell=True)\n",
    "\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.6 Get the throughput or accuracy of the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Throughput of first workload:\n",
    "# Split the model_name into individual parts\n",
    "parts_1 = model_1_name.split()\n",
    "\n",
    "# Join the parts using hyphens as the separator\n",
    "log_1_name = '-'.join(parts_1)\n",
    "log_1_name = log_1_name + \".log\"\n",
    "\n",
    "import re, statistics\n",
    "try:\n",
    "    with open(f'{log_directory}/{log_1_name}', 'r') as file:\n",
    "        print(\"DEBUGGING: FILE DIRECTORY:\", f'{log_directory}/{log_1_name}')\n",
    "        file_content = file.read()\n",
    "        throughput_matches = re.findall(r'Throughput[^:]*:\\s*(\\d+\\.\\d+)\\s+fps', file_content)\n",
    "        accuracy_matches = re.findall(r'Accuracy[^:]*:\\s*(\\d+\\.\\d+)', file_content)\n",
    "        if throughput_matches:\n",
    "            throughput_matches = [float(x) for x in throughput_matches]\n",
    "            throughput_value = float(statistics.mean(throughput_matches))\n",
    "            print(f'Throughput: {throughput_value} fps')\n",
    "        else:\n",
    "            print('Throughput not found in the file.')\n",
    "        if accuracy_matches:\n",
    "            accuracy_matches = [float(x) for x in accuracy_matches]\n",
    "            accuracy_value = float(statistics.mean(accuracy_matches))\n",
    "            print(f'Accuracy: {accuracy_value}')\n",
    "        else:\n",
    "            print('Accuracy not found in the file')\n",
    "\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print('File not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Select and run the second model\n",
    "\n",
    "Let's now run a second model and compare its performance with the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Select another model for comparision\n",
    "\n",
    "After the list of models is displayed, select one by entering the model index number. (If the environment variable MODEL_2_INDEX is set, we'll use that instead of prompting for input.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "notebook_root = os.environ['NotebookRoot']\n",
    "%cd $notebook_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Supported Models: \")\n",
    "print(models_table)\n",
    "\n",
    "# use the \"MODEL_2_INDEX\" environment variable value if it exists.\n",
    "import os\n",
    "env_model_2_index=os.environ.get('MODEL_2_INDEX', '')\n",
    "if env_model_2_index != '':\n",
    "    model_2_index= int(env_model_2_index)\n",
    "else:\n",
    "    ## USER INPUT\n",
    "    model_2_index= int(input('Input a index number of a model: '))\n",
    "\n",
    "# List out the selected model name\n",
    "if model_2_index >= len(sections):\n",
    "    print(\"ERROR! Input a model_index value between 0 and \", len(sections))\n",
    "else:\n",
    "    model_2_name=sections[model_2_index]['name']\n",
    "\n",
    "    # Prints out model name\n",
    "    print(\"Second model is: \", model_2_name.split()[0] )\n",
    "\n",
    "    selected_section = sections[model_2_index]\n",
    "    model_name_2 = selected_section['model-name']\n",
    "\n",
    "    # Check if the selected model has a precision section\n",
    "    if model_name_2 in selected_section:\n",
    "        print(f\"Available Precisions for {model_name_2}:\")\n",
    "\n",
    "        # Create a table with headers\n",
    "        precision_table = PrettyTable([\"Index\", \"Precision\"])\n",
    "        precisions = selected_section[model_name_2]\n",
    "\n",
    "        # Iterate through the available precisions\n",
    "        for index, precision_info in enumerate(selected_section[model_name_2]):\n",
    "            precision = precision_info['precision']\n",
    "\n",
    "            # Add a row to the table\n",
    "            precision_table.add_row([index, precision])\n",
    "\n",
    "        print(precision_table)\n",
    "\n",
    "        # Prompt the user to select a precision\n",
    "        precision_index = int(input('Select an index number for the precision: '))\n",
    "        selected_precision_2 = precisions[precision_index]['precision']\n",
    "        print(\"Selected Precision for\", model_2_name, selected_precision_2)\n",
    "\n",
    "        # Print a table for available scripts for selected precision\n",
    "        script_section = precisions[precision_index]\n",
    "        script_name = script_section['script']\n",
    "\n",
    "        print(f\"Available Scripts for {selected_precision}:\")\n",
    "\n",
    "        # Create a table with headers\n",
    "        script_table = PrettyTable([\"Index\", \"Script\"])\n",
    "\n",
    "        # Iterate through the available scripts\n",
    "        for index, script in enumerate(script_name):\n",
    "            script_table.add_row([index, script])\n",
    "\n",
    "        print(script_table)\n",
    "\n",
    "        # Prompt the user to select a script\n",
    "        script_index = int(input('Input a index number for the script: '))\n",
    "        selected_row = script_table._rows[script_index]\n",
    "        selected_script = selected_row[1]\n",
    "        print(\"Selected script for\", model_2_name, selected_script)\n",
    "\n",
    "        # Prints out model name\n",
    "        print(\"Second model with selected precision and script is: \", model_2_name, selected_precision_2, selected_script)\n",
    "        os.environ['PRECISION']=selected_precision_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Setup\n",
    "The following cell checks for additional environment variables that needs to be specified per model specification. \n",
    "\n",
    "Then checks for additional files, libraries, and configurations that needs to be installed per model specification. Commands are installed in a chronological dependency order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "ai_reference_root = os.environ.get('AIReferenceRoot')\n",
    "%cd $ai_reference_root\n",
    "\n",
    "for export in selected_section['exports']:\n",
    "    env_name = export.split(' ')[0]\n",
    "    env_value = export.split(' ')[1]\n",
    "    if env_value == '-i':\n",
    "        env_value = input(f'Please provide the path for the following environment variable {env_name}: ', )\n",
    "    os.environ[env_name] = os.path.expandvars(env_value)\n",
    "\n",
    "    print(\"Assigned environment variable \", env_name, \" : \", os.environ[env_name])\n",
    "\n",
    "for cmd in selected_section['additional-commands']:\n",
    "    subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Get the required dataset for the selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell checks for the second dataset path. If the data-location is already specified, then the notebook will use the dataset path mentioned in ai_reference_models.ini file. If the data-location is not specified in the ai_reference_models.ini, the data downloading option instructions are shown. You must manually download the dataset using these instructions before you can proceed to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Get the parameters from config\n",
    "config = AIReferenceConfigFile(os.environ['yaml_file'])\n",
    "config_data = config.read_config(model_2_name)\n",
    "data_download_path=''\n",
    "\n",
    "if config_data['data-download'] != '': #and config.data_location == '':\n",
    "    print(\"\\nFollow these instructions to get the data : \")\n",
    "    if config_data['data-download'] != '':\n",
    "        val = config_data['data-download']\n",
    "    print(val)\n",
    "    # use the \"DATA_DOWNLOAD_PATH\" environment variable value if it exists.\n",
    "    env_data_download_path=os.environ.get('DATA_DOWNLOAD_PATH', '')\n",
    "    if env_data_download_path != '':\n",
    "        data_download_path= env_data_download_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path for dataset directory for the second model:\n",
    "\n",
    "DATASET_DIR': the path where the dataset exists and is downloaded.\n",
    "\n",
    "**ACTION : You need to input the path where the dataset for the second model exists or where you have downloaded it in your system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = input('Input the path where the dataset exists for the second model:')\n",
    "\n",
    "os.environ['DATASET_DIR'] = dataset_path\n",
    "print(\"Data location path:\", os.environ['DATASET_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Prepare pre-trained model for the selected model\n",
    "\n",
    "This step checks if the pre-trained model for the selected model exists in the pre-trained directory path. If the pre-trained directory does not exist, then it downloads the pre-trained model for the selected precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "config = AIReferenceConfigFile(os.environ['yaml_file'])\n",
    "\n",
    "# Get the parameters from config\n",
    "configvals=config.read_config(model_2_name)\n",
    "\n",
    "# Iterate through the 'resnet50v1_5' list to find the matching 'precision' and get the 'wget' value\n",
    "for item in configvals[model_2_name.split()[0]]:\n",
    "    if item['precision'] == selected_precision_2:\n",
    "        wget_value = item['wget']\n",
    "        break  # You can break the loop once you find the match\n",
    "\n",
    "# Get the pre-trained model file\n",
    "pretrain_model_path = config.download_pretrained_model(wget=wget_value, current_path=current_path)\n",
    "pretrain_model_path = config.uncompress_file(pretrain_model_path, current_path=current_path)\n",
    "if pretrain_model_path:\n",
    "    print('Downloaded the model in:', pretrain_model_path)\n",
    "else:\n",
    "    print('Failed to download the pretrained model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the environment variable for pre-trained model for the second model:\n",
    "\n",
    "'PRETRAINED_MODEL': the path where is the pretrained_model exists and is downloaded.\n",
    "\n",
    "NOTE: You can change the value of 'PRETRAINED_MODEL' by changing its assignment in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"PRETRAINED_MODEL\"]=pretrain_model_path\n",
    "print(\"Pretrain_model_path:\", os.environ[\"PRETRAINED_MODEL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Run the second model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**if BATCH_SIZE is specified per model specification, set it for optimal batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "if selected_section['set-batch-size']['cores']:\n",
    "    import subprocess\n",
    "\n",
    "    # Run the shell command to get the number of cores\n",
    "    output = subprocess.check_output(\"lscpu | grep 'Core(s) per socket' | awk '{print $4}'\", shell=True)\n",
    "\n",
    "    # Decode the byte string output into a regular Python string\n",
    "    cores = int(output.decode(\"utf-8\").strip())\n",
    "\n",
    "    cores = eval(selected_section['set-batch-size']['expr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL: You can change the batch size from the model's default. For online_inference,  set batch_size value to be 1.** \n",
    "\n",
    "**Run the next cell if you want to change the batch-size, other skip running the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "batch_size= input('Set the value for batch size that you want to run: ')\n",
    "os.environ[\"BATCH_SIZE\"]=batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the second model using the quickstart script configured in the ai_reference_models.ini file, and save output logs to the selected log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "config = AIReferenceConfigFile(os.environ['yaml_file'])\n",
    "\n",
    "# Get the parameters from config\n",
    "config_data = config.read_config(model_2_name)\n",
    "\n",
    "# Split the model_name into individual parts\n",
    "parts = model_2_name.split()\n",
    "\n",
    "# Join the parts using hyphens as the separator\n",
    "log_name = '-'.join(parts)\n",
    "log_name = log_name + \".log\"\n",
    "\n",
    "ai_reference_root = os.environ.get('AIReferenceRoot')\n",
    "\n",
    "%cd $ai_reference_root\n",
    "run_workload = (\"quickstart/\" + config_data.get('ai-type') + \"/\" + config_data.get('framework') + \"/\"+ config_data.get('model-name') + \"/\"\n",
    "                + config_data.get('mode') + \"/\" + config_data.get('device') +\"/\" + selected_script )\n",
    "\n",
    "!./$run_workload | tee $log_directory/{log_name}\n",
    "\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5 Get the throughput or accuracy of the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Throughput of second workload:\n",
    "# Split the model_name into individual parts\n",
    "parts_2 = model_2_name.split()\n",
    "\n",
    "# Join the parts using hyphens as the separator\n",
    "log_2_name = '-'.join(parts_2)\n",
    "log_2_name = log_2_name + \".log\"\n",
    "\n",
    "import re, statistics\n",
    "try:\n",
    "    with open(f'{log_directory}/{log_2_name}', 'r') as file:\n",
    "        print(\"DEBUGGING: FILE DIRECTORY:\", f'{log_directory}/{log_2_name}')\n",
    "        file_content = file.read()\n",
    "        throughput_matches = re.findall(r'Throughput[^:]*:\\s*(\\d+\\.\\d+)\\s+fps', file_content)\n",
    "        accuracy_matches = re.findall(r'Accuracy[^:]*:\\s*(\\d+\\.\\d+)', file_content)\n",
    "        if throughput_matches:\n",
    "            throughput_matches = [float(x) for x in throughput_matches]\n",
    "            throughput_value = float(statistics.mean(throughput_matches))\n",
    "            print(f'Throughput: {throughput_value} fps')\n",
    "        else:\n",
    "            print('Throughput not found in the file.')\n",
    "        if accuracy_matches:\n",
    "            accuracy_matches = [float(x) for x in accuracy_matches]\n",
    "            accuracy_value = float(statistics.mean(accuracy_matches))\n",
    "            print(f'Accuracy: {accuracy_value}')\n",
    "        else:\n",
    "            print('Accuracy not found in the file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compare performance results and plot a comparison chart for the two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the results (throughput/accuracy) of the two models for comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Throughput of\", model_1_name, \": \", Throughput_of_workload_1)\n",
    "print(\"Throughput of\", model_2_name, \": \", Throughput_of_workload_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a chart for comparision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Define your data\n",
    "categories = [model_1_name.split(' ')[0] + selected_precision, model_2_name.split(' ')[0] + selected_precision_2]\n",
    "\n",
    "# float array expected for charts.\n",
    "values = [float(Throughput_of_workload_1), float(Throughput_of_workload_2)]\n",
    "\n",
    "\n",
    "# Generate a list of colors for each bar\n",
    "colors = ['blue', 'orange']\n",
    "\n",
    "\n",
    "print(categories, values, colors)\n",
    "\n",
    "ax.bar(x=categories, height=values, color=colors)\n",
    "ax.set_ylabel(\"Throughput\")\n",
    "ax.set_title(\"Throughput by each model\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
