- name: resnet50v1_5 tensorflow inference
  ai-type: image_recognition
  model-name: resnet50v1_5
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  resnet50v1_5:
    - precision: fp32
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://zenodo.org/record/2535873/files/resnet50_v1.pb

    - precision: bfloat32
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://zenodo.org/record/2535873/files/resnet50_v1.pb

    - precision: fp16
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://zenodo.org/record/2535873/files/resnet50_v1.pb

    - precision: int8
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh", "inference_realtime_weightsharing.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_8/bias_resnet50.pb

    - precision: bfloat16
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh", "inference_realtime_weightsharing.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_8/bf16_resnet50_v1.pb


- name: ssd-mobilenet tensorflow inference
  ai-type: object_detection
  model-name: ssd-mobilenet
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: To get the dataset, please refer to the page "https://github.com/IntelAI/models/tree/master/datasets/coco/README.md"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  ssd-mobilenet:
    - precision: fp32
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/ssdmobilenet_fp32_pretrained_model_combinedNMS.pb

    - precision: bfloat16
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/ssdmobilenet_fp32_pretrained_model_combinedNMS.pb

    - precision: bfloat32
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/ssdmobilenet_fp32_pretrained_model_combinedNMS.pb

    - precision: int8
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/ssdmobilenet_int8_pretrained_model_combinedNMS_s8.pb

- name: mobilenet_v1 tensorflow inference
  ai-type: image_recognition
  model-name: mobilenet_v1
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  mobilenet_v1:
    - precision: fp32
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/mobilenet_v1_1.0_224_frozen.pb

    - precision: bfloat16
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/mobilenet_v1_1.0_224_frozen.pb

    - precision: bfloat32
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/mobilenet_v1_1.0_224_frozen.pb

    - precision: int8
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/mobilenetv1_int8_pretrained_model.pb

- name: mobilenet_v2 tensorflow inference
  ai-type: image_recognition
  model-name: mobilenet_v2
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  mobilenet_v1:
    - precision: fp32
      script: ["inference_throughput_multi_instance.sh", "", "", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/mobilenet_v1_1.0_224_frozen.pb

    - precision: bfloat16
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/mobilenet_v1_1.0_224_frozen.pb

    - precision: int8
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/mobilenetv1_int8_pretrained_model.pb

- name: densenet169 tensorflow inference
  ai-type: image_recognition
  model-name: densenet169
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  densenet169:
  - precision: fp32
    script: ["online_inference.sh", "batch_inference.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/densenet169_fp32_pretrained_model.pb

- name: inceptionv3 tensorflow inference
  ai-type: image_recognition
  model-name: inceptionv3
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  inceptionv3:
  - precision: fp32
    script: ["online_inference.sh", "batch_inference.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/inceptionv3_fp32_pretrained_model.pb

- name: resnet101 tensorflow inference
  ai-type: image_recognition
  model-name: resnet101
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  resnet101:
  - precision: fp32
    script: ["online_inference.sh", "batch_inference.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/resnet101_fp32_pretrained_model.pb

  - precision: int8
    script: ["online_inference.sh", "batch_inference.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/intel-neural-compressor/v1_13/resnet-101-inc-int8-inference.pb

- name: vision_transformer tensorflow inference
  ai-type: image_recognition
  model-name: vision_transformer
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  vision_transformer:
  - precision: fp32
    script: ["online_inference.sh", "batch_inference.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_11_0/HF-ViT-Base16-Img224-frozen.pb
  - precision: int8
    script: ["online_inference.sh", "batch_inference.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/3_0/nc_vit_int8_newapi.pb

  - precision: fp16
    script: ["online_inference.sh", "batch_inference.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_11_0/HF-ViT-Base16-Img224-frozen.pb

  - precision: bfloat16
    script: ["online_inference.sh", "batch_inference.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_11_0/HF-ViT-Base16-Img224-frozen.pb

- name: bert_large tensorflow inference
  ai-type: language_modeling
  model-name: bert_large
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
    "CHECKPOINT_DIR -i"
  ]
  set-batch-size:
    cores: false
    expr: ""

  bert_large:
  - precision: fp32
    script: ["inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v2_7_0/fp32_bert_squad.pb

  - precision: fp16
    script: ["inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v2_7_0/fp32_bert_squad.pb

  - precision: bfloat16
    script: ["inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/optimized_bf16_bert.pb

  - precision: int8
    script: ["inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_12_0/bert_itex_int8.pb

- name: distilbert_base tensorflow inference
  ai-type: language_modeling
  model-name: distilbert_base
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
    "IN_GRAPH -i"
  ]
  set-batch-size:
    cores: false
    expr: ""

  distilbert_base:
  - precision: fp32
    script: ["profile.sh", "inference.sh", "inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/distilbert_frozen_graph_fp32_final.pb

  - precision: fp16
    script: ["profile.sh", "inference.sh", "inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/distilbert_frozen_graph_fp32_final.pb

  - precision: bfloat16
    script: ["profile.sh", "inference.sh", "inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/distilbert_frozen_graph_fp32_final.pb

  - precision: bfloat32
    script: ["profile.sh", "inference.sh", "inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/distilbert_frozen_graph_fp32_final.pb

  - precision: int8
    script: ["profile.sh", "inference.sh", "inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/FusedBMM_int8_distilbert_frozen_graph.pb

- name: gpt_j tensorflow inference
  ai-type: language_modeling
  model-name: gpt_j
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
    "CHECKPOINT_DIR -i"
  ]
  set-batch-size:
    cores: false
    expr: ""

  gpt_j:
  - precision: fp32
    script: ["inference.sh", "inference_realtime_multi_instance.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: ""

  - precision: fp16
    script: ["inference.sh", "inference_realtime_multi_instance.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: ""

  - precision: bfloat16
    script: ["inference.sh", "inference_realtime_multi_instance.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: ""

  - precision: int8
    script: ["inference.sh", "inference_realtime_multi_instance.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: ""

- name: 3d_unet_mlperf tensorflow inference
  ai-type: language_modeling
  model-name: 3d_unet_mlperf
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
    "CHECKPOINT_DIR -i"
  ]
  set-batch-size:
    cores: false
    expr: ""

  3d_unet_mlperf:
  - precision: fp32
    script: ["profile.sh", "inference.sh", "inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/3dunet_dynamic_ndhwc.pb

  - precision: bfloat32
    script: ["profile.sh", "inference.sh", "inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/3dunet_dynamic_ndhwc.pb

  - precision: bfloat16
    script: ["profile.sh", "inference.sh", "inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/3dunet_dynamic_ndhwc.pb

  - precision: int8
    script: ["profile.sh", "inference.sh", "inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/3dunet_new_int8_bf16.pb
