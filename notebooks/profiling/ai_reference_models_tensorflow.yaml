- name: resnet50v1_5 tensorflow inference
  ai-type: image_recognition
  model-name: resnet50v1_5
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  resnet50v1_5:
    - precision: fp32
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://zenodo.org/record/2535873/files/resnet50_v1.pb

    - precision: bfloat32
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://zenodo.org/record/2535873/files/resnet50_v1.pb

    - precision: fp16
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh"]
      wget: https://zenodo.org/record/2535873/files/resnet50_v1.pb

    - precision: int8
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh", "inference_realtime_weightsharing.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_8/bias_resnet50.pb

    - precision: bfloat16
      script: ["inference_throughput_multi_instance.sh", "accuracy.sh", "inference_realtime_weightsharing.sh"]
      wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_8/bf16_resnet50_v1.pb

- name: bert_large tensorflow inference
  ai-type: language_modeling
  model-name: bert_large
  mode: inference
  framework: tensorflow
  device: cpu
  data-location:
  data-download: Get the Imagenet dataset by following the instructions at "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
  additional-commands: []
  exports: [
    "CHECKPOINT_DIR -i"
  ]
  set-batch-size:
    cores: false
    expr: ""

  bert_large:
  - precision: fp32
    script: ["inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v2_7_0/fp32_bert_squad.pb

  - precision: fp16
    script: ["inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/v2_7_0/fp32_bert_squad.pb

  - precision: bfloat16
    script: ["inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/optimized_bf16_bert.pb

  - precision: int8
    script: ["inference_realtime_multi_instance.sh", "inference_realtime_weightsharing.sh", "inference_throughput_multi_instance.sh", "accuracy.sh"]
    wget: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_12_0/bert_itex_int8.pb
