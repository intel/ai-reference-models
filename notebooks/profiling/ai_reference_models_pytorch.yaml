- name: resnet50 pytorch inference
  ai-type: image_recognition
  model-name: resnet50
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: Download and extract the ImageNet2012 dataset from http://www.image-net.org/, then move validation images to labeled subfolders, using the valprep.sh shell script"
  additional-commands: []
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  resnet50:
    - precision: fp32
      script: ["inference_throughput.sh", "accuracy.sh"]

    - precision: bf32
      script: ["inference_throughput.sh", "accuracy.sh"]

    - precision: fp16
      script: ["inference_throughput.sh", "accuracy.sh"]

    - precision: int8
      script: ["inference_throughput.sh", "accuracy.sh"]

    - precision: bf16
      script: ["inference_throughput.sh", "accuracy.sh"]

- name: bert_large pytorch inference
  ai-type: language_modeling
  model-name: bert_large
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: "Download and extract the ImageNet2012 dataset from http://www.image-net.org/, then move validation images to labeled subfolders, using the valprep.sh shell script"
  additional-commands:
    [
      "./quickstart/language_modeling/pytorch/bert_large/inference/cpu/setup.sh",
      "mkdir bert_squad_model",
      "mkdir squad1.1",
      "wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -O squad1.1/dev-v1.1.json",
      "wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json -O bert_squad_model/config.json",
      "wget https://cdn.huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin -O bert_squad_model/pytorch_model.bin"
    ]
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  bert_large:
    - precision: fp32
      script: ["run_multi_instance_realtime.sh", "run_multi_instance_throughput.sh", "run_accuracy.sh"]

    - precision: bf16
      script: ["run_multi_instance_realtime.sh", "run_multi_instance_throughput.sh", "run_accuracy.sh"]

- name: distilbert_base pytorch inference
  ai-type: language_modeling
  model-name: distilbert_base
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: Download and extract the ImageNet2012 dataset from http://www.image-net.org/, then move validation images to labeled subfolders, using the valprep.sh shell script"
  additional-commands: [
    "git clone https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english",
    "./quickstart/language_modeling/pytorch/distilbert_base/inference/cpu/setup.sh",
  ]
  exports: [
    "FINETUNED_MODEL -i",
    "HF_DATASETS_ONLINE 0",
    "SEQUENCE_LENGTH 128",
    "CORE_PER_INSTANCE 4"
  ]
  set-batch-size:
    cores: false
    expr: ""

  distilbert_base:
    - precision: fp32
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

    - precision: bf32
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

    - precision: bf16
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

    - precision: int8-fp32
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

    - precision: int8-bf16
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

- name: gptj pytorch inference
  ai-type: language_modeling
  model-name: gptj
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: Download and extract the ImageNet2012 dataset from http://www.image-net.org/, then move validation images to labeled subfolders, using the valprep.sh shell script"
  additional-commands: [
    "./quickstart/language_modeling/pytorch/gptj/inference/cpu/setup.sh",
  ]
  exports: [
    "INPUT_TOKEN 32",
    "OUTPUT_TOKEN 32"
  ]
  set-batch-size:
    cores: false
    expr: ""

  gptj:
    - precision: fp32
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

    - precision: bf32
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

    - precision: bf16
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

    - precision: fp16
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

    - precision: int8-fp32
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

    - precision: int8-bf16
      script: ["run_multi_instance_throughput.sh", "run_multi_instance_realtime.sh", "run_accuracy.sh"]

- name: rnnt pytorch inference
  ai-type: language_modeling
  model-name: rnnt
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: Download and extract the ImageNet2012 dataset from http://www.image-net.org/, then move validation images to labeled subfolders, using the valprep.sh shell script"
  additional-commands: [
    "./quickstart/language_modeling/pytorch/rnnt/inference/cpu/install_dependency_baremetal.sh",
    "export CHECKPOINT_DIR='${PWD}/notebooks/pretrained'",
    "./quickstart/language_modeling/pytorch/rnnt/inference/cpu/download_model.sh",
    "./quickstart/language_modeling/pytorch/rnnt/inference/cpu/setup.sh",
  ]
  exports: [
    "CHECKPOINT_DIR -i"
  ]
  set-batch-size:
    cores: false
    expr: ""

  rnnt:
    - precision: fp32
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: bf32
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: bf16
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: avx-fp32
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

- name: maskrcnn pytorch inference
  ai-type: object_detection
  model-name: maskrcnn
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: Download and extract the ImageNet2012 dataset from http://www.image-net.org/, then move validation images to labeled subfolders, using the valprep.sh shell script"
  additional-commands: []
  exports: [
    "CHECKPOINT_DIR -i",
    "MODE jit"
  ]
  set-batch-size:
    cores: false
    expr: ""
  maskrcnn:
    - precision: fp32
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: bf16
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: avx-fp32
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: bf32
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

- name: ssd-resnet34 pytorch inference
  ai-type: object_detection
  model-name: ssd-resnet34
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: Download and extract the ImageNet2012 dataset from http://www.image-net.org/, then move validation images to labeled subfolders, using the valprep.sh shell script"
  additional-commands: []
  exports: [
    "CHECKPOINT_DIR -i",
  ]
  set-batch-size:
    cores: false
    expr: ""

  ssd-resnet34:
    - precision: fp32
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: avx-fp32
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: int8
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: avx-int8
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: bf16
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

    - precision: bf32
      script: ["inference_realtime.sh", "inference_throughput.sh", "accuracy.sh"]

- name: dlrm pytorch inference
  ai-type: recommendation
  model-name: dlrm
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download:
    Download and extract the Criteo Terabyte Dataset dataset from https://labs.criteo.com/2013/12/download-terabyte-click-logs/,
    then follow the instructions (to provide publicly) in README"
  additional-commands: ["./quickstart/recommendation/pytorch/dlrm/inference/cpu/setup.sh"]
  exports: [
    "DNNL_MAX_CPU_ISA AVX512_CORE_AMX",
    "WEIGHT_PATH -i",
  ]
  set-batch-size:
    cores: false
    expr: ""

  dlrm:
    - precision: fp32
      script: ["inference_performance.sh", "accuracy.sh"]

    - precision: int8
      script: ["inference_performance.sh", "accuracy.sh"]

    - precision: bf16
      script: ["inference_performance.sh", "accuracy.sh"]

    - precision: bf32
      script: ["inference_performance.sh", "accuracy.sh"]

- name: torchrec_dlrm pytorch inference
  ai-type: recommendation
  model-name: torchrec_dlrm
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: "Follow the instructions on ... to download and install the dataset"
  additional-commands: [
    "./quickstart/recommendation/pytorch/torchrec_dlrm/inference/cpu/setup.sh",
    "./quickstart/recommendation/pytorch/torchrec_dlrm/inference/cpu/preprocess_raw_dataset.sh"
    ]
  exports: [
    "DNNL_MAX_CPU_ISA AVX512_CORE_AMX",
    "RAW_DIR -i",
    "TEMP_DIR -i",
    "PREPROCESSED_DIR -i",
    "MULTI_HOT_DIR -i",
    "WEIGHT_DIR -i"
  ]
  set-batch-size:
    cores: false
    expr: ""

  torchrec_dlrm:
    - precision: fp32
      script: ["inference_performance.sh", "accuracy.sh"]

    - precision: int8
      script: ["inference_performance.sh", "accuracy.sh"]

    - precision: bf16
      script: ["inference_performance.sh", "accuracy.sh"]

    - precision: bf32
      script: ["inference_performance.sh", "accuracy.sh"]

    - precision: fp16
      script: ["inference_performance.sh", "accuracy.sh"]

- name: stable_diffusion pytorch inference
  ai-type: diffusion
  model-name: stable_diffusion
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: "Follow the instructions on ... to download and install the dataset"
  additional-commands: [
    "./quickstart/diffusion/pytorch/stable_diffusion/inference/cpu/setup.sh",
    ]
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  stable_diffusion:
    - precision: fp32
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: bf16
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: bf32
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: fp16
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: int8-bf16
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: int8-fp32
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

- name: LCM pytorch inference
  ai-type: diffusion
  model-name: LCM
  mode: inference
  framework: pytorch
  device: cpu
  data-location:
  data-download: "Follow the instructions on ... to download and install the dataset"
  additional-commands: [
    "./quickstart/diffusion/pytorch/LCM/inference/cpu/setup.sh",
    ]
  exports: [
  ]
  set-batch-size:
    cores: false
    expr: ""

  LCM:
    - precision: fp32
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: bf16
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: bf32
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: fp16
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: int8-bf16
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]

    - precision: int8-fp32
      script: ["inference_throughput.sh", "inference_realtime.sh", "accuracy.sh"]
